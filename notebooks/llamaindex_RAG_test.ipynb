{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import json\n",
    "import yaml\n",
    "import time\n",
    "import requests\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "from dotenv import load_dotenv\n",
    "from io import BytesIO\n",
    "import io\n",
    "import zipfile\n",
    "import re\n",
    "from llama_index.core.node_parser import SimpleNodeParser, SentenceWindowNodeParser\n",
    "from llama_index.legacy import VectorStoreIndex,SimpleDirectoryReader,ServiceContext,PromptTemplate\n",
    "from llama_index.core.postprocessor import MetadataReplacementPostProcessor, SentenceTransformerRerank\n",
    "from llama_index.legacy import Document\n",
    "\n",
    "\n",
    "# Adobe PDF Services imports\n",
    "from adobe.pdfservices.operation.auth.service_principal_credentials import ServicePrincipalCredentials\n",
    "from adobe.pdfservices.operation.exception.exceptions import ServiceApiException, ServiceUsageException, SdkException\n",
    "from adobe.pdfservices.operation.io.cloud_asset import CloudAsset\n",
    "from adobe.pdfservices.operation.io.stream_asset import StreamAsset\n",
    "from adobe.pdfservices.operation.pdf_services import PDFServices\n",
    "from adobe.pdfservices.operation.pdf_services_media_type import PDFServicesMediaType\n",
    "from adobe.pdfservices.operation.pdfjobs.jobs.extract_pdf_job import ExtractPDFJob\n",
    "from adobe.pdfservices.operation.pdfjobs.params.extract_pdf.extract_element_type import ExtractElementType\n",
    "from adobe.pdfservices.operation.pdfjobs.params.extract_pdf.extract_pdf_params import ExtractPDFParams\n",
    "from adobe.pdfservices.operation.pdfjobs.result.extract_pdf_result import ExtractPDFResult\n",
    "\n",
    "# Pinecone and Langchain imports\n",
    "from pinecone import Pinecone\n",
    "from pinecone_text.sparse import BM25Encoder\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "hf_key = os.getenv('HUGGINGFACE_API_KEY')\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "dense_embedder_api = os.getenv(\"HF_API_URL\")\n",
    "\n",
    "# Initialize clients\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "\n",
    "# Define model\n",
    "chat_model = \"llama3-8b-8192\"\n",
    "index = pc.Index('hsi-notes')\n",
    "namespace = 'Chapter-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "class ExtractTextTableInfoFromPDF:\n",
    "    def __init__(self, file_path):\n",
    "        try:\n",
    "            file = open(file_path, 'rb')\n",
    "            input_stream = file.read()\n",
    "            file.close()\n",
    "\n",
    "            # Initial setup, create credentials instance\n",
    "            credentials = ServicePrincipalCredentials(\n",
    "                client_id=os.getenv('ADOBE_SERVICES_CLIENT_ID'),\n",
    "                client_secret=os.getenv('ADOBE_SERVICES_CLIENT_SECRET')\n",
    "            )\n",
    "\n",
    "            # Creates a PDF Services instance\n",
    "            pdf_services = PDFServices(credentials=credentials)\n",
    "\n",
    "            # Creates an asset(s) from source file(s) and upload\n",
    "            input_asset = pdf_services.upload(input_stream=input_stream, mime_type=PDFServicesMediaType.PDF)\n",
    "\n",
    "            # Create parameters for the job\n",
    "            extract_pdf_params = ExtractPDFParams(\n",
    "                elements_to_extract=[ExtractElementType.TEXT, ExtractElementType.TABLES],\n",
    "            )\n",
    "\n",
    "            # Creates a new job instance\n",
    "            extract_pdf_job = ExtractPDFJob(input_asset=input_asset, extract_pdf_params=extract_pdf_params)\n",
    "\n",
    "            # Submit the job and gets the job result\n",
    "            location = pdf_services.submit(extract_pdf_job)\n",
    "            pdf_services_response = pdf_services.get_job_result(location, ExtractPDFResult)\n",
    "\n",
    "            # Get content from the resulting asset(s)\n",
    "            result_asset: CloudAsset = pdf_services_response.get_result().get_resource()\n",
    "            stream_asset: StreamAsset = pdf_services.get_content(result_asset)\n",
    "\n",
    "            \n",
    "            zip_bytes = io.BytesIO(stream_asset.get_input_stream())\n",
    "            with zipfile.ZipFile(zip_bytes, 'r') as zip_ref:\n",
    "                # Extract all the contents into memory\n",
    "                self.extracted_data = {name: zip_ref.read(name) for name in zip_ref.namelist()}\n",
    "                \n",
    "        except (ServiceApiException, ServiceUsageException, SdkException) as e:\n",
    "            logging.exception(f'Exception encountered while executing operation: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_table_index_llama(table_str):\n",
    "    class Header(BaseModel):\n",
    "        index: int = Field(description=\"Header of the table, 0 for first row as the header, 1 for first column as the header\")\n",
    "        \n",
    "    parser = JsonOutputParser(pydantic_object=Header)\n",
    "\n",
    "    chat = ChatGroq(temperature=0, model_name=\"llama3-8b-8192\")\n",
    "    template = '''You will assist me in deciding, based on the first 2 entries of a table, whether the first row or the first colum should be the header. \n",
    "            You are to output an int, 0 or 1. Where 0 if the first row is header, and 1 if the first column is the header.\n",
    "            Follow the format instructions carefully.\n",
    "            Table:\n",
    "            {table}\n",
    "            \n",
    "            {format_instructions}\n",
    "            '''\n",
    "    prompt = PromptTemplate(\n",
    "        template=template,\n",
    "        input_variables=[\"table\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "    chain = prompt | chat | parser\n",
    "    return chain.invoke({\"table\": table_str})\n",
    "\n",
    "def clean_values(x):\n",
    "    if isinstance(x, str):\n",
    "        return x.replace('_x000D_', '').strip()\n",
    "    return x\n",
    "\n",
    "def get_table_check_string(df):\n",
    "    table_str = \"\"\n",
    "    for i in range(2):\n",
    "        if i ==1:\n",
    "            table_str += f\"Row {i}: {df.iloc[i].values.tolist()}\"  \n",
    "        else:\n",
    "            table_str += f\"Row {i}: {df.iloc[i].values.tolist()}\\n\"\n",
    "    return table_str\n",
    "\n",
    "def convert_table_to_str(df):\n",
    "    for index, row in df.iterrows():\n",
    "        row_str = \"\"\n",
    "        for col in df.columns:\n",
    "            sentences = re.split(r'(?<=\\.)\\s*', row[col])\n",
    "            row_sentence = \"\"\n",
    "            for i in range(len(sentences)):\n",
    "                row_sentence += sentences[i] + \"\\n\"\n",
    "            row_str += f\"{col}: {row_sentence}, \"\n",
    "        formatted = row_str[:-2]\n",
    "    return formatted\n",
    "    \n",
    "def get_table_meta(elements):\n",
    "    table_file_pages = {}\n",
    "    for el in elements:\n",
    "        # Using get to avoid KeyError and ensure 'filePaths' is not empty\n",
    "        file_paths = el.get('filePaths')\n",
    "        if file_paths:\n",
    "            page = el.get('Page', 'Unknown')  # Provide a default page number if missing\n",
    "            table_file_pages[file_paths[0]] = {\"Page\": page}\n",
    "    return table_file_pages\n",
    "\n",
    "def extract_data(extracted_data):\n",
    "    if 'structuredData.json' in extracted_data:\n",
    "        json_data = json.loads(extracted_data['structuredData.json'])\n",
    "    return json_data\n",
    "    \n",
    "def get_table_pages_and_text_chunks(json_data):\n",
    "    if 'elements' not in json_data:\n",
    "        logging.error(\"Missing 'elements' key in json_data\")\n",
    "        raise ValueError(\"Missing 'elements' key in json_data\")\n",
    "\n",
    "    table_file_pages = {}\n",
    "    page_text = \"\"\n",
    "    start_page = 0\n",
    "    all_chunks = []\n",
    "    separator_list = [\"\\n\\n\", \"\\n\", \". \", \"!\", \"?\", \",\", \" \", \"\", \")\", \"(\"]\n",
    "    \n",
    "    try:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=300,\n",
    "                chunk_overlap=50,\n",
    "                length_function=len,\n",
    "                separators=separator_list)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to initialize text splitter: {e}\")\n",
    "        raise\n",
    "\n",
    "    list_label = \"\"\n",
    "    for i in range(len(json_data['elements'])):\n",
    "        try:\n",
    "            current_page = json_data['elements'][i]['Page']\n",
    "        except KeyError:\n",
    "            logging.warning(f\"Missing 'Page' key in element at index {i}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            if current_page > start_page:\n",
    "                # Update the new page number\n",
    "                start_page = current_page               \n",
    "                # Generate the chunks for the previous page\n",
    "                separated_list = text_splitter.split_text(page_text)\n",
    "                for chunk in separated_list:\n",
    "                    if chunk not in [\". \", \".\"]:  # Simplified condition\n",
    "                        all_chunks.append({'ElementType': 'Text', 'Page': current_page, 'Text': chunk})\n",
    "                # Update the string of text \n",
    "                page_text = \"\"\n",
    "                list_label = \"\"\n",
    "            else:\n",
    "                if 'Text' in json_data['elements'][i]:  # Check if Text is not empty\n",
    "                    if json_data['elements'][i]['Path'].endswith(\"Lbl\") and not json_data['elements'][i]['Path'].startswith(\"//Document/Table\"):\n",
    "                        list_label = json_data['elements'][i]['Text']\n",
    "                    else:\n",
    "                        if list_label:\n",
    "                            page_text += list_label + json_data['elements'][i]['Text']\n",
    "                            list_label = \"\"  # Reset list label to empty string\n",
    "                        else:\n",
    "                            page_text += json_data['elements'][i]['Text'] + \"\\n\"\n",
    "        except KeyError as e:\n",
    "            logging.warning(f\"Key error in json_data['elements'][i] processing at index {i}: {e}\")\n",
    "    \n",
    "    # Process the last page of the text\n",
    "    if page_text:\n",
    "        separated_list = text_splitter.split_text(page_text)\n",
    "        for chunk in separated_list:\n",
    "            if chunk not in [\". \", \".\"]:\n",
    "                all_chunks.append({'ElementType': 'Text', 'Page': start_page + 1, 'Text': chunk})\n",
    " \n",
    "    # Obtaining the table metadata\n",
    "    for i in range(len(json_data['elements'])):\n",
    "        try:\n",
    "            file_paths = json_data['elements'][i].get('filePaths')\n",
    "            if file_paths:\n",
    "                page = json_data['elements'][i].get('Page', 'Unknown')\n",
    "                match = re.search(r'\\d+', file_paths[0])\n",
    "                table_index = match.group(0)\n",
    "                table_file_pages[int(table_index)] = {\"Page\": page}\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing file paths at index {i}: {e}\")\n",
    "    return table_file_pages, all_chunks\n",
    "\n",
    "def get_tables(extractor):\n",
    "    #The literal extraction of the file itself\n",
    "    excel_files = {k: v for k, v in extractor.extracted_data.items() if k.endswith('.xlsx')}\n",
    "    table_dataframes = {}\n",
    "\n",
    "    num_tables =0\n",
    "    for fname, content in excel_files.items():\n",
    "        excel_stream = BytesIO(content)\n",
    "        df = pd.read_excel(excel_stream, header=None)\n",
    "        df = df.applymap(clean_values)\n",
    "        df_str = get_table_check_string(df) \n",
    "        #dic = eval_table_index_llama(df_str)\n",
    "        #header_index = dic['index']\n",
    "        header_index = 1\n",
    "        \n",
    "        # If header_index is non zero\n",
    "        if header_index:\n",
    "            df = pd.read_excel(excel_stream, header=None)\n",
    "            df = df.applymap(clean_values)\n",
    "            df = df.T\n",
    "            # Set the first row as the new header\n",
    "            new_header = df.iloc[0]  # Take the first row for the header\n",
    "            df = df[1:]  # Take the data less the header row\n",
    "            df.columns = new_header  # Set the header row as the df header\n",
    "            # Optionally, reset index if necessary\n",
    "            df.reset_index(drop=True, inplace=True)\n",
    "        else:\n",
    "            df = pd.read_excel(excel_stream, header=0)\n",
    "            \n",
    "        table_str = convert_table_to_str(df)\n",
    "        table_dataframes[num_tables] = table_str\n",
    "        num_tables += 1\n",
    "    return table_dataframes\n",
    "\n",
    "def generate_tables(table_dataframes, table_file_pages):\n",
    "    meta_table_batch = []\n",
    "    table_dfs = []\n",
    "    for table_index, table_str in table_dataframes.items():\n",
    "        dic = {}\n",
    "        dic['ElementType'] = 'Table'\n",
    "        dic['Page'] = table_file_pages[table_index]['Page']\n",
    "        dic['Table'] = table_dataframes[table_index]\n",
    "        table_dfs.append(dic)\n",
    "\n",
    "        meta_table_batch.append(f\"ElementType 'Table', Page {table_file_pages[table_index]['Page']}, {table_dataframes[table_index]}\")\n",
    "    \n",
    "    return table_dfs, meta_table_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[1;32m      5\u001b[0m docs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m      8\u001b[0m     docs\u001b[38;5;241m.\u001b[39mappend(Document(\n\u001b[1;32m      9\u001b[0m         text\u001b[38;5;241m=\u001b[39mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     10\u001b[0m         doc_id\u001b[38;5;241m=\u001b[39mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     11\u001b[0m         extra_info\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[1;32m     12\u001b[0m     ))\n\u001b[1;32m     13\u001b[0m docs[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# What is needed to transform this into something that a node parser can take as input\n",
    "# Documents. Can use this below\n",
    "\n",
    "# docs = []\n",
    "\n",
    "# for i, row in data.iterrows():\n",
    "#     docs.append(Document(\n",
    "#         text=row['context'],\n",
    "#         doc_id=row['id'],\n",
    "#         extra_info={'title': row['title']}\n",
    "#     ))\n",
    "# docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
