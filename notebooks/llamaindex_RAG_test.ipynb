{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import json\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "import time\n",
    "import sys\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "from dotenv import load_dotenv\n",
    "from io import BytesIO\n",
    "import io\n",
    "import zipfile\n",
    "import re\n",
    "\n",
    "# Adobe PDF Services imports\n",
    "from adobe.pdfservices.operation.auth.service_principal_credentials import ServicePrincipalCredentials\n",
    "from adobe.pdfservices.operation.exception.exceptions import ServiceApiException, ServiceUsageException, SdkException\n",
    "from adobe.pdfservices.operation.io.cloud_asset import CloudAsset\n",
    "from adobe.pdfservices.operation.io.stream_asset import StreamAsset\n",
    "from adobe.pdfservices.operation.pdf_services import PDFServices\n",
    "from adobe.pdfservices.operation.pdf_services_media_type import PDFServicesMediaType\n",
    "from adobe.pdfservices.operation.pdfjobs.jobs.extract_pdf_job import ExtractPDFJob\n",
    "from adobe.pdfservices.operation.pdfjobs.params.extract_pdf.extract_element_type import ExtractElementType\n",
    "from adobe.pdfservices.operation.pdfjobs.params.extract_pdf.extract_pdf_params import ExtractPDFParams\n",
    "from adobe.pdfservices.operation.pdfjobs.result.extract_pdf_result import ExtractPDFResult\n",
    "\n",
    "# Pinecone and Langchain imports\n",
    "from pinecone import Pinecone\n",
    "from pinecone_text.sparse import BM25Encoder\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "hf_key = os.getenv('HUGGINGFACE_API_KEY')\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "dense_embedder_api = os.getenv(\"HF_API_URL\")\n",
    "\n",
    "# Initialize clients\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "\n",
    "# Define model\n",
    "chat_model = \"llama3-8b-8192\"\n",
    "index = pc.Index('hsi-notes')\n",
    "namespace = 'Chapter-1'\n",
    "\n",
    "from typing import Optional, Union, TypeAlias\n",
    "\n",
    "# Import other necessary modules\n",
    "from llama_index.legacy import Document\n",
    "from llama_index.legacy import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.legacy.node_parser import SimpleNodeParser, SentenceWindowNodeParser\n",
    "from llama_index.legacy.postprocessor import MetadataReplacementPostProcessor, SentenceTransformerRerank\n",
    "from llama_index.legacy.schema import MetadataMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "class ExtractTextTableInfoFromPDF:\n",
    "    def __init__(self, file_path):\n",
    "        self.unique_id = str(uuid.uuid4())\n",
    "        try:\n",
    "            file = open(file_path, 'rb')\n",
    "            input_stream = file.read()\n",
    "            file.close()\n",
    "\n",
    "            # Initial setup, create credentials instance\n",
    "            credentials = ServicePrincipalCredentials(\n",
    "                client_id=os.getenv('ADOBE_SERVICES_CLIENT_ID'),\n",
    "                client_secret=os.getenv('ADOBE_SERVICES_CLIENT_SECRET')\n",
    "            )\n",
    "\n",
    "            # Creates a PDF Services instance\n",
    "            pdf_services = PDFServices(credentials=credentials)\n",
    "\n",
    "            # Creates an asset(s) from source file(s) and upload\n",
    "            input_asset = pdf_services.upload(input_stream=input_stream, mime_type=PDFServicesMediaType.PDF)\n",
    "\n",
    "            # Create parameters for the job\n",
    "            extract_pdf_params = ExtractPDFParams(\n",
    "                elements_to_extract=[ExtractElementType.TEXT, ExtractElementType.TABLES],\n",
    "            )\n",
    "\n",
    "            # Creates a new job instance\n",
    "            extract_pdf_job = ExtractPDFJob(input_asset=input_asset, extract_pdf_params=extract_pdf_params)\n",
    "\n",
    "            # Submit the job and gets the job result\n",
    "            location = pdf_services.submit(extract_pdf_job)\n",
    "            pdf_services_response = pdf_services.get_job_result(location, ExtractPDFResult)\n",
    "\n",
    "            # Get content from the resulting asset(s)\n",
    "            result_asset: CloudAsset = pdf_services_response.get_result().get_resource()\n",
    "            stream_asset: StreamAsset = pdf_services.get_content(result_asset)\n",
    "            \n",
    "            zip_bytes = io.BytesIO(stream_asset.get_input_stream())\n",
    "            with zipfile.ZipFile(zip_bytes, 'r') as zip_ref:\n",
    "                # Extract all the contents into memory\n",
    "                self.extracted_data = {name: zip_ref.read(name) for name in zip_ref.namelist()}\n",
    "                \n",
    "        except (ServiceApiException, ServiceUsageException, SdkException) as e:\n",
    "            logging.exception(f'Exception encountered while executing operation: {e}')\n",
    "    \n",
    "    # Generates a string containing a directory structure and file name for the output file using unique_id\n",
    "    @staticmethod\n",
    "    def create_output_file_path(unique_id: str) -> str:\n",
    "        os.makedirs(\"../data/Extracted_data\", exist_ok=True)\n",
    "        return f\"../data/Extracted_data/{unique_id}.zip\"\n",
    "\n",
    "    @classmethod\n",
    "    def create_with_unique_id(cls, file_path):\n",
    "        instance = cls(file_path)\n",
    "        return instance, instance.unique_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the extracted data from the extractor\n",
    "def get_extracted_data(extracted_data):\n",
    "    if 'structuredData.json' in extracted_data:\n",
    "        json_data = json.loads(extracted_data['structuredData.json'])\n",
    "    return json_data\n",
    "\n",
    "# Function to initialise a flexible text splitter\n",
    "def initialise_text_splitter(chunk_size, chunk_overlap):\n",
    "    separator_list = [\"\\n\\n\", \"\\n\", \". \", \"!\", \"?\", \",\", \" \", \"\", \")\", \"(\"]\n",
    "    try:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=chunk_overlap,\n",
    "                length_function=len,\n",
    "                separators=separator_list)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to initialize text splitter: {e}\")\n",
    "        return None\n",
    "    return text_splitter\n",
    "\n",
    "# Function to obtain text chunks using the text splitter\n",
    "def get_text_chunks(file_path, json_data, text_splitter):\n",
    "    if 'elements' not in json_data:\n",
    "        logging.error(\"Missing 'elements' key in json_data\")\n",
    "        raise ValueError(\"Missing 'elements' key in json_data\")\n",
    "\n",
    "    if not text_splitter:\n",
    "        logging.error(\"Text splitter not initialised properly. \")\n",
    "        sys.exit()  \n",
    "        \n",
    "    file_name = file_path.split(\"/\")[-1]\n",
    "\n",
    "    # Chunks are split by pages here\n",
    "    page_text = \"\"\n",
    "    start_page = 0\n",
    "    all_chunks = []\n",
    "    \n",
    "    list_label = \"\"\n",
    "    for i in range(len(json_data['elements'])):\n",
    "        try:\n",
    "            current_page = json_data['elements'][i]['Page']\n",
    "        except KeyError:\n",
    "            logging.warning(f\"Missing 'Page' key in element at index {i}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            if current_page > start_page:\n",
    "                # Update the new page number\n",
    "                start_page = current_page               \n",
    "                # Generate the chunks for the previous page\n",
    "                separated_list = text_splitter.split_text(page_text)\n",
    "                for chunk in separated_list:\n",
    "                    if chunk not in [\". \", \".\"]:  # Simplified condition\n",
    "                        all_chunks.append({'ElementType': 'Text', 'file_name': file_name, 'Page': current_page, 'Text': chunk})\n",
    "                # Update the string of text \n",
    "                page_text = \"\"\n",
    "                list_label = \"\"\n",
    "            else:\n",
    "                if 'Text' in json_data['elements'][i]:  # Check if Text is not empty\n",
    "                    if json_data['elements'][i]['Path'].endswith(\"Lbl\") and not json_data['elements'][i]['Path'].startswith(\"//Document/Table\"):\n",
    "                        list_label = json_data['elements'][i]['Text']\n",
    "                    else:\n",
    "                        if list_label:\n",
    "                            page_text += list_label + json_data['elements'][i]['Text']\n",
    "                            list_label = \"\"  # Reset list label to empty string\n",
    "                        else:\n",
    "                            page_text += json_data['elements'][i]['Text'] + \"\\n\"\n",
    "        except KeyError as e:\n",
    "            logging.warning(f\"Key error in json_data['elements'][i] processing at index {i}: {e}\")\n",
    "    \n",
    "    \n",
    "    # Processing the last page of the text\n",
    "    if page_text:\n",
    "        separated_list = text_splitter.split_text(page_text)\n",
    "        for chunk in separated_list:\n",
    "            if chunk not in [\". \", \".\"]:\n",
    "                all_chunks.append({'ElementType': 'Text', 'file_name': file_name, 'Page': current_page, 'Text': chunk})\n",
    "\n",
    "    return all_chunks\n",
    "\n",
    "# Function to derive the nodes from the text chunks\n",
    "def convert_textchunks_to_nodes(text_chunks):\n",
    "    # Conversion of text chunks to Documents\n",
    "    text_documents = [Document(text=chunk['Text'],\n",
    "                            metadata={\n",
    "                                \"file_name\": chunk['file_name'],\n",
    "                                \"page\": chunk['Page']\n",
    "                                },\n",
    "                            excluded_llm_metadata_keys=[\"file_name\"],\n",
    "                            metadata_seperator=\"::\",\n",
    "                            metadata_template=\"{key}=>{value}\",\n",
    "                            text_template=\"Metadata: {metadata_str}\\n-----\\nContent: {content}\")\n",
    "\n",
    "                                for chunk in text_chunks]\n",
    "    print(\n",
    "        \"The LLM sees this: \\n\",\n",
    "        text_documents[0].get_content(metadata_mode=MetadataMode.LLM),\n",
    "    )\n",
    "    print(\"-\"* 80)\n",
    "    print(\n",
    "        \"The Embedding model sees this: \\n\",\n",
    "        text_documents[0].get_content(metadata_mode=MetadataMode.EMBED),\n",
    "    )\n",
    "\n",
    "    # create the sentence window node parser w/ default settings\n",
    "    SW_node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "        window_size=1,\n",
    "        window_metadata_key=\"window\",\n",
    "        original_text_metadata_key=\"original_text\",\n",
    "    )\n",
    "\n",
    "    # Create the nodes\n",
    "    nodes = SW_node_parser.get_nodes_from_documents(text_documents)\n",
    "    return nodes\n",
    "\n",
    "# Function to get the unique tables from all the table elements\n",
    "def extract_unique_tables(table_elements):\n",
    "    tables = set()\n",
    "    for item in table_elements:\n",
    "        match = re.search(r'/Table(\\[\\d+\\])?', item['Path'])\n",
    "        if match:\n",
    "            tables.add('Table' + (match.group(1) if match.group(1) else ''))\n",
    "    \n",
    "    unique_tables = list(tables)\n",
    "    unique_tables[0] += \"/\"\n",
    "\n",
    "    extracted_tables = {}\n",
    "    i=0\n",
    "    for table_name in unique_tables:\n",
    "        table = []\n",
    "        for el in table_elements:\n",
    "            if table_name in el['Path']:\n",
    "                # ADjust this here if u need to extract more information from the table elements\n",
    "                table.append({\"path\": el['Path'], \"text\": el['Text'], \"Page\": el[\"Page\"]})\n",
    "        extracted_tables[i+1] = table\n",
    "        i += 1\n",
    "\n",
    "    return dict(sorted(extracted_tables.items()))\n",
    "\n",
    "# Function to take in table elements from a specific table and convert it to a pandas dataframe\n",
    "def transform_table(table, output_file_path):\n",
    "    # Function to decide if table has 2 indexes\n",
    "    def is_2_index(first_row):\n",
    "        for el in first_row:\n",
    "            if \"TH/\" in el['path']:\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "    \n",
    "    # Function to decide if the table's row is the header   \n",
    "    def is_row_header(first_row):\n",
    "        for el in first_row:\n",
    "            if \"TH\" not in el['path']:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    # Function to get the table dimensions\n",
    "    def get_table_dim(table):\n",
    "        # Regular expressions to find row and column indices\n",
    "        row_regex = re.compile(r'/TR\\[(\\d+)\\]')\n",
    "        col_regex = re.compile(r'/TD\\[(\\d+)\\]')\n",
    "\n",
    "        max_row = 0\n",
    "        max_col = 0\n",
    "\n",
    "        for entry in table:\n",
    "            path = entry['path']  # Extract path from the dictionary\n",
    "            \n",
    "            # Extract row and column indices\n",
    "            row_match = row_regex.search(path)\n",
    "            col_match = col_regex.search(path)\n",
    "            \n",
    "            # Update max row index\n",
    "            if row_match:\n",
    "                row_index = int(row_match.group(1))\n",
    "                max_row = max(max_row, row_index)\n",
    "            \n",
    "            # Update max column index\n",
    "            if col_match:\n",
    "                col_index = int(col_match.group(1))\n",
    "                max_col = max(max_col, col_index)\n",
    "            elif '/TD/' in path and not col_match:  # Check for default first column\n",
    "                max_col = max(max_col, 1)  # Ensure first column is counted\n",
    "        return (max_row, max_col)    \n",
    "\n",
    "    # Function to check if the API could not identify the index of the table\n",
    "    def is_unidentified(table):\n",
    "        table_dim = get_table_dim(table)\n",
    "        min_headers = min(table_dim)\n",
    "        header_count = len([el for el in table if \"TH\" in el['path']])\n",
    "        if header_count < min_headers:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    # Function to transform the unidentified table by determining its index using LLM\n",
    "    def transform_unidentified_table(table):\n",
    "        # Function to get the first 3 rows \n",
    "        def get_first_three_rows(table):\n",
    "                # Regular expression to extract row index\n",
    "                row_regex = re.compile(r'/TR\\[(\\d+)\\]')\n",
    "                \n",
    "                # List to hold the first three rows table\n",
    "                rows = []\n",
    "                \n",
    "                for entry in table:\n",
    "                    path = entry['path']\n",
    "                    # Find row index\n",
    "                    row_match = row_regex.search(path)\n",
    "                    \n",
    "                    if row_match:\n",
    "                        row_index = int(row_match.group(1))\n",
    "                        # Check if the row index is within the first three rows\n",
    "                        if 1 <= row_index <= 3:\n",
    "                            rows.append(entry)\n",
    "                    elif '/TR/' in path:  # Check for paths that imply the first row implicitly\n",
    "                        rows.append(entry)\n",
    "            \n",
    "                table_first_3_rows = [] # This will be a 2D list\n",
    "                table_rows = set([re.search(r'Table(?:\\[\\d+\\])?/TR(?:\\[\\d+\\])?', item['path']).group() for item in table if re.search(r'Table(?:\\[\\d+\\])?/TR(?:\\[\\d+\\])?', item['path'])])\n",
    "                uniq_rows = sorted([row_name.split(\"/\")[-1] for row_name in list(table_rows)[:3]])\n",
    "                uniq_rows[0] += \"/\"\n",
    "\n",
    "\n",
    "                for i in range(len(uniq_rows)):\n",
    "                    row = [el for el in table if uniq_rows[i] in el['path']]\n",
    "                    unique_tds = set()\n",
    "                    for item in row:\n",
    "                                path_parts = item['path'].split('/')\n",
    "                                for part in path_parts:\n",
    "                                    if 'TD' in part:\n",
    "                                        unique_tds.add(part)\n",
    "                    # Convert the set to a list and sort it for consistent output\n",
    "                    unique_tds_list = sorted(list(unique_tds))\n",
    "                            \n",
    "                    sections_of_row = []\n",
    "                    for td in unique_tds_list:\n",
    "                                td_section = \"\"\n",
    "                                for i in range(len(row)):\n",
    "                                    if i ==0:\n",
    "                                        td += \"/\"\n",
    "                                    if td in row[i]['path']:\n",
    "                                        td_section += row[i]['text'].strip()\n",
    "\n",
    "                                sections_of_row.append(td_section)\n",
    "                    \n",
    "                    # print(sections_of_row)\n",
    "                    table_first_3_rows.append(sections_of_row)\n",
    "\n",
    "                # account for case where there are 2 index columns:\n",
    "                min_dim = max(list(map(lambda x: len(x), table_first_3_rows)))\n",
    "                if len(table_first_3_rows[0]) < min_dim:\n",
    "                    table_first_3_rows[0].insert(0, \"empty\")\n",
    "                return table_first_3_rows\n",
    "\n",
    "        # Function to get the first 3 rows of the table into a string to parse to the LLM \n",
    "        def get_table_check_string(table_first_3_rows):\n",
    "            table_str = ''\n",
    "            for i in range(len(table_first_3_rows)):\n",
    "                table_str += f'''Row{i+1}: {\" | \".join(table_first_3_rows[i])}\\n'''\n",
    "            return table_str\n",
    "\n",
    "        # Function to clean the output if it decides to explain its choice.\n",
    "        def clean_llama_output_if_string(input_str):\n",
    "            \"\"\"\n",
    "            Extracts a dictionary-like substring from the given input string.\n",
    "            \n",
    "            Args:\n",
    "            input_str (str): The input string containing the dictionary-like substring.\n",
    "            \n",
    "            Returns:\n",
    "            dict: The extracted dictionary as a Python dictionary.\n",
    "            \"\"\"\n",
    "            # Use a regular expression to find the dictionary-like substring\n",
    "            match = re.search(r'\\{.*?\\}', input_str)\n",
    "            \n",
    "            if match:\n",
    "                dict_str = match.group()\n",
    "                try:\n",
    "                    # Safely evaluate the dictionary string\n",
    "                    extracted_dict = eval(dict_str)\n",
    "                    if isinstance(extracted_dict, dict):\n",
    "                        return extracted_dict\n",
    "                except (SyntaxError, NameError):\n",
    "                    pass\n",
    "            \n",
    "            return None\n",
    "\n",
    "        # Function to decide the indexing of the table\n",
    "        def eval_table_index_llama(table_str):\n",
    "            class Header(BaseModel):\n",
    "                index: int = Field(description=(\n",
    "                    '''Index of the table indicating the type of indexing:\\n\\\n",
    "                        0 - No indexes\\n\\\n",
    "                        1 - Indexed by the first column\\n\\\n",
    "                        2 - Indexed by the first row\\n\\\n",
    "                        3 - Indexed by both the first row and the first column'''))\n",
    "\n",
    "            parser = JsonOutputParser(pydantic_object=Header)\n",
    "\n",
    "            chat = ChatGroq(temperature=0, model_name=\"llama3-8b-8192\")\n",
    "            \n",
    "            template = '''You will assist me in deciding, based on the first 3 entries of a table, whether the first row or the first colum should be the header, or if both the first column and first row are headers\n",
    "        Table:\n",
    "        {table}\n",
    "\n",
    "        Determine the indexing type of the table and output:\n",
    "        - 0 if the table is indexed by the first column.\n",
    "        - 1 if the table is indexed by the first row.\n",
    "        - 2 if the table is indexed by both the first row and the first column.   \n",
    "                    \n",
    "        {format_instructions}'''\n",
    "            prompt = PromptTemplate(\n",
    "                template=template,\n",
    "                input_variables=[\"table\"],\n",
    "                partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "            )\n",
    "                \n",
    "            chain = prompt | chat | parser\n",
    "            return chain.invoke({\"table\": table_str})\n",
    "        \n",
    "        table_first_3_rows = get_first_three_rows(table)\n",
    "        table_str = get_table_check_string(table_first_3_rows)\n",
    "        header_output = eval_table_index_llama(table_str)\n",
    "        if isinstance(header_output, str):\n",
    "            header_output = clean_llama_output_if_string(header_output)\n",
    "        header_index = header_output['index']\n",
    "        # Now need diff cases of how to handle the table if the model outputs, 0,1, or 2\n",
    "            # where 0 is the first col, 1 is second col, 2 is for both cols are headers\n",
    "        pass\n",
    "    \n",
    "    # If the table cannot be identified by the ExtractTable API\n",
    "    if is_unidentified(table):\n",
    "        processed_unidentified_table = transform_unidentified_table(table)\n",
    "        pass\n",
    "    \n",
    "    # only need to look at the first row\n",
    "    first_row = [el for el in table if \"TR/\" in el['path']]\n",
    "    \n",
    "    # If the table has 2 indexes\n",
    "    if is_2_index(first_row):\n",
    "            print(\"This table has 2 indexes\")\n",
    "            # Function to produce table which has 2 indexes\n",
    "            def get_2_index_table(table):\n",
    "                table_rows = set([re.search(r'Table(?:\\[\\d+\\])?/TR(?:\\[\\d+\\])?', item['path']).group() for item in table if re.search(r'Table(?:\\[\\d+\\])?/TR(?:\\[\\d+\\])?', item['path'])])\n",
    "                uniq_rows = sorted([row_name.split(\"/\")[-1] for row_name in list(table_rows)])\n",
    "\n",
    "                # Get indexes from the first row:\n",
    "                row_indexes = [el['text'].strip() for el in table if \"TR/TH\" in el['path']]\n",
    "\n",
    "                data = {}\n",
    "\n",
    "                # Only look at second row onwards\n",
    "                for i in range(1,len(uniq_rows)):\n",
    "                    row_name = uniq_rows[i]\n",
    "                    row = [el for el in table if row_name in el['path']]\n",
    "                    row_key = row[0]['text'].strip()\n",
    "                    \n",
    "                    unique_tds = set()\n",
    "                    for item in row:\n",
    "                        path_parts = item['path'].split('/')\n",
    "                        for part in path_parts:\n",
    "                            if 'TD' in part:\n",
    "                                unique_tds.add(part)\n",
    "                    # Convert the set to a list and sort it for consistent output\n",
    "                    unique_tds_list = sorted(list(unique_tds))\n",
    "\n",
    "                    sections_of_row = []\n",
    "                    for td in unique_tds_list:\n",
    "                        td_section = \"\"\n",
    "                        for i in range(len(row)):\n",
    "                            if i ==0:\n",
    "                                td += \"/\"\n",
    "                            if td in row[i]['path']:\n",
    "                                td_section += row[i]['text'].strip()\n",
    "                                \n",
    "                        sections_of_row.append(td_section)\n",
    "                    \n",
    "                    data[row_key] = sections_of_row\n",
    "                    \n",
    "                df = pd.DataFrame(data, index=row_indexes).T\n",
    "                df.to_csv(output_file_path)\n",
    "                return df    \n",
    "            \n",
    "            df = get_2_index_table(table)\n",
    "            return df\n",
    "        \n",
    "    # If the table only has one index\n",
    "    else:\n",
    "            print(\"This table has 1 index\")\n",
    "\n",
    "            # If the header for this df is the row\n",
    "            if is_row_header(first_row):\n",
    "                print(\"This is a row indexed table\")\n",
    "                def get_row_header_table(table):\n",
    "                    table_rows = set([re.search(r'Table(?:\\[\\d+\\])?/TR(?:\\[\\d+\\])?', item['path']).group() for item in table if re.search(r'Table(?:\\[\\d+\\])?/TR(?:\\[\\d+\\])?', item['path'])])\n",
    "                    uniq_rows = sorted([row_name.split(\"/\")[-1] for row_name in list(table_rows)])\n",
    "\n",
    "                    # Table headers, also the keys\n",
    "                    headers = [el['text'].strip() for el in table if uniq_rows[0]+\"/\" in el['path']]\n",
    "                    rows = []\n",
    "                    for i in range(1,len(uniq_rows)):\n",
    "                        row_name = uniq_rows[i]\n",
    "                        row = [el for el in table if row_name in el['path']]\n",
    "                        # rows.append(row)\n",
    "                        \n",
    "                        unique_tds = set()\n",
    "                        for item in row:\n",
    "                            path_parts = item['path'].split('/')\n",
    "                            for part in path_parts:\n",
    "                                if 'TD' in part:\n",
    "                                    unique_tds.add(part)\n",
    "                        # Convert the set to a list and sort it for consistent output\n",
    "                        unique_tds_list = sorted(list(unique_tds))\n",
    "\n",
    "                        sections_of_row = []\n",
    "                        for td in unique_tds_list:\n",
    "                            td_section = \"\"\n",
    "                            for i in range(len(row)):\n",
    "                                if i ==0:\n",
    "                                    td += \"/\"\n",
    "                                if td in row[i]['path']:\n",
    "                                    td_section += row[i]['text'].strip()\n",
    "                                    \n",
    "                            sections_of_row.append(td_section)\n",
    "                        rows.append(sections_of_row)\n",
    "                                    \n",
    "                    df = pd.DataFrame(rows, columns=headers)\n",
    "                    return df\n",
    "                \n",
    "                df = get_row_header_table(table)\n",
    "                df.to_csv(output_file_path, index=False)\n",
    "                return df\n",
    "                    \n",
    "            # The header for this df is the column\n",
    "            else:\n",
    "                print(\"This table is a column indexed table...\")\n",
    "                def get_column_header_table(table):\n",
    "                    table_rows = set([re.search(r'Table(?:\\[\\d+\\])?/TR(?:\\[\\d+\\])?', item['path']).group() for item in table if re.search(r'Table(?:\\[\\d+\\])?/TR(?:\\[\\d+\\])?', item['path'])])\n",
    "                    uniq_rows = sorted([row_name.split(\"/\")[-1] for row_name in list(table_rows)])\n",
    "\n",
    "                    data = {}\n",
    "\n",
    "                    for i in range(len(uniq_rows)):\n",
    "                                        row_name = uniq_rows[i]\n",
    "                                        if i == 0:\n",
    "                                            row = [el for el in table if row_name+\"/\" in el['path']]\n",
    "                                        else:\n",
    "                                            row = [el for el in table if row_name in el['path']]\n",
    "                                        \n",
    "                                        unique_tds = set()\n",
    "                                        for item in row:\n",
    "                                            path_parts = item['path'].split('/')\n",
    "                                            for part in path_parts:\n",
    "                                                if 'TD' in part:\n",
    "                                                    unique_tds.add(part)\n",
    "                                        # Convert the set to a list and sort it for consistent output\n",
    "                                        unique_tds_list = sorted(list(unique_tds))\n",
    "                                        \n",
    "                                        sections_of_row = []\n",
    "                                        for td in unique_tds_list:\n",
    "                                            td_section = \"\"\n",
    "                                            for i in range(len(row)):\n",
    "                                                if i ==0:\n",
    "                                                    td += \"/\"\n",
    "                                                if td in row[i]['path']:\n",
    "                                                    td_section += row[i]['text'].strip()\n",
    "                                                    \n",
    "                                            sections_of_row.append(td_section)\n",
    "                                                    \n",
    "                                        row_key = row[0]['text'].strip()\n",
    "                                        data[row_key] = sections_of_row\n",
    "                    df = pd.DataFrame.from_dict(data, orient='index').T\n",
    "                    return df\n",
    "                                    \n",
    "                df = get_column_header_table(table)\n",
    "                df.to_csv(output_file_path, index=False)\n",
    "                return df\n",
    "\n",
    "# Function that saves the tables in CSV format to a unique job id       \n",
    "def save_tables_to_csv(extracted_tables, table_output_directory):\n",
    "    \n",
    "    os.makedirs(table_output_directory, exist_ok=True)\n",
    "    for table_num, table in extracted_tables.items():\n",
    "        output_file_path = os.path.join(table_output_directory, f\"table_{table_num}.csv\")\n",
    "        df = transform_table(table, output_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runing the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-04 11:15:23,621 - INFO - Started uploading asset\n",
      "2024-07-04 11:15:26,843 - INFO - Finished uploading asset\n",
      "2024-07-04 11:15:26,845 - INFO - Started submitting EXTRACT_PDF job\n",
      "2024-07-04 11:15:28,278 - INFO - Started getting job result\n",
      "2024-07-04 11:15:33,421 - INFO - Finished polling for status\n",
      "2024-07-04 11:15:33,422 - INFO - Finished getting job result\n",
      "2024-07-04 11:15:33,423 - INFO - Started getting content\n",
      "2024-07-04 11:15:33,716 - INFO - Finished getting content\n",
      "2024-07-04 11:15:33,723 - WARNING - Missing 'Page' key in element at index 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique ID: 55dc8bcd-f418-405f-98ab-ccd4cb8d0ae2\n",
      "\n",
      "Unique ID: 55dc8bcd-f418-405f-98ab-ccd4cb8d0ae2\n",
      "The LLM sees this: \n",
      " Metadata: page=>1\n",
      "-----\n",
      "Content: scepticism of this scientific consensus arose in the public arena. How that doubt emerged will be the subject of the next lecture, but let’s consider the changing public perception in the U.S. \n",
      "14.2.3 Public Perception of Consensus\n",
      "--------------------------------------------------------------------------------\n",
      "The Embedding model sees this: \n",
      " Metadata: file_name=>HSI1000-2col-table.pdf::page=>1\n",
      "-----\n",
      "Content: scepticism of this scientific consensus arose in the public arena. How that doubt emerged will be the subject of the next lecture, but let’s consider the changing public perception in the U.S. \n",
      "14.2.3 Public Perception of Consensus\n"
     ]
    }
   ],
   "source": [
    "file_path = '../PDF/HSI1000-2col-table.pdf'\n",
    "extractor, unique_id = ExtractTextTableInfoFromPDF.create_with_unique_id(file_path)\n",
    "print(\"\\nUnique ID:\", unique_id)\n",
    "extracted_data = extractor.extracted_data\n",
    "pdf_data = get_extracted_data(extracted_data)\n",
    "\n",
    "def extract_tables_from_pdf(pdf_data):\n",
    "    table_elements = [el for el in pdf_data['elements'] if \"Table\" in el['Path'] and 'Text' in el and \"TR\" in el['Path']]\n",
    "    print(\"\\nUnique ID:\", unique_id)\n",
    "\n",
    "    # IF there are even any table elements in the PDF\n",
    "    if table_elements:\n",
    "        table_output_directory = f\"../data/{unique_id}\" \n",
    "        extracted_tables = extract_unique_tables(table_elements)    \n",
    "        save_tables_to_csv(extracted_tables, table_output_directory)\n",
    "\n",
    "# Section here to derive the csvs from the table elements\n",
    "# Uncomment to extract tables (for now its not ready yet)\n",
    "# extract_tables_from_pdf(pdf_data)\n",
    "\n",
    "# Section here to derive the nodes from the text elements\n",
    "text_splitter = initialise_text_splitter(600, 50)\n",
    "text_chunks = get_text_chunks(file_path, pdf_data, text_splitter)\n",
    "text_nodes = convert_textchunks_to_nodes(text_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next thing to do:\n",
    "- Clean the documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_text(content: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove unwanted characters and patterns in text input.\n",
    "    :param content: Text input.\n",
    "    :return: Cleaned version of original text input.\n",
    "    \"\"\"\n",
    "\n",
    "    # Fix hyphenated words broken by newline\n",
    "    content = re.sub(r'(\\w+)-\\n(\\w+)', r'\\1\\2', content)\n",
    "\n",
    "    # Remove specific unwanted patterns and characters\n",
    "    unwanted_patterns = [\n",
    "        \"\\\\n\", \"  —\", \"——————————\", \"—————————\", \"—————\",\n",
    "        r'\\\\u[\\dA-Fa-f]{4}', r'\\uf075', r'\\uf0b7'\n",
    "    ]\n",
    "    for pattern in unwanted_patterns:\n",
    "        content = re.sub(pattern, \"\", content)\n",
    "\n",
    "    # Fix improperly spaced hyphenated words and normalize whitespace\n",
    "    content = re.sub(r'(\\w)\\s*-\\s*(\\w)', r'\\1-\\2', content)\n",
    "    content = re.sub(r'\\s+', ' ', content)\n",
    "\n",
    "    return content\n",
    "\n",
    "# Call function\n",
    "cleaned_docs = []\n",
    "for d in documents:\n",
    "    cleaned_text = clean_up_text(d.text)\n",
    "    d.text = cleaned_text\n",
    "    cleaned_docs.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuation of table extraction (For unidentified tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the table here has the TH's replaced with TD's\n",
    "table = extracted_tables[1]\n",
    "# Replace every 'TH' with 'TD' in the 'path' key\n",
    "for item in table:\n",
    "    item['path'] = item['path'].replace('TH', 'TD')\n",
    "\n",
    "# only need to look at the first row\n",
    "first_row = [el for el in table if \"TR[2]/\" in el['path']]\n",
    "    # Function to decide if table has 2 indexes\n",
    "\n",
    "def get_table_dim(table):\n",
    "    # Regular expressions to find row and column indices\n",
    "    row_regex = re.compile(r'/TR\\[(\\d+)\\]')\n",
    "    col_regex = re.compile(r'/TD\\[(\\d+)\\]')\n",
    "\n",
    "    max_row = 0\n",
    "    max_col = 0\n",
    "\n",
    "    for entry in table:\n",
    "        path = entry['path']  # Extract path from the dictionary\n",
    "        \n",
    "        # Extract row and column indices\n",
    "        row_match = row_regex.search(path)\n",
    "        col_match = col_regex.search(path)\n",
    "        \n",
    "        # Update max row index\n",
    "        if row_match:\n",
    "            row_index = int(row_match.group(1))\n",
    "            max_row = max(max_row, row_index)\n",
    "        \n",
    "        # Update max column index\n",
    "        if col_match:\n",
    "            col_index = int(col_match.group(1))\n",
    "            max_col = max(max_col, col_index)\n",
    "        elif '/TD/' in path and not col_match:  # Check for default first column\n",
    "            max_col = max(max_col, 1)  # Ensure first column is counted\n",
    "    return (max_row, max_col)    \n",
    "\n",
    "# Function to check if the API could not identify the index of the table\n",
    "def is_unidentified(table):\n",
    "    table_dim = get_table_dim(table)\n",
    "    min_headers = min(table_dim)\n",
    "    header_count = len([el for el in table if \"TH\" in el['path']])\n",
    "    if header_count < min_headers:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Function to transform the unidentified table by determining its index using LLM\n",
    "def transform_unidentified_table(table):\n",
    "    # Function to get the first 3 rows \n",
    "    def get_first_three_rows(table):\n",
    "            # Regular expression to extract row index\n",
    "            row_regex = re.compile(r'/TR\\[(\\d+)\\]')\n",
    "            \n",
    "            # List to hold the first three rows table\n",
    "            rows = []\n",
    "            \n",
    "            for entry in table:\n",
    "                path = entry['path']\n",
    "                # Find row index\n",
    "                row_match = row_regex.search(path)\n",
    "                \n",
    "                if row_match:\n",
    "                    row_index = int(row_match.group(1))\n",
    "                    # Check if the row index is within the first three rows\n",
    "                    if 1 <= row_index <= 3:\n",
    "                        rows.append(entry)\n",
    "                elif '/TR/' in path:  # Check for paths that imply the first row implicitly\n",
    "                    rows.append(entry)\n",
    "        \n",
    "            table_first_3_rows = [] # This will be a 2D list\n",
    "            table_rows = set([re.search(r'Table(?:\\[\\d+\\])?/TR(?:\\[\\d+\\])?', item['path']).group() for item in table if re.search(r'Table(?:\\[\\d+\\])?/TR(?:\\[\\d+\\])?', item['path'])])\n",
    "            uniq_rows = sorted([row_name.split(\"/\")[-1] for row_name in list(table_rows)[:3]])\n",
    "            uniq_rows[0] += \"/\"\n",
    "\n",
    "\n",
    "            for i in range(len(uniq_rows)):\n",
    "                row = [el for el in table if uniq_rows[i] in el['path']]\n",
    "                unique_tds = set()\n",
    "                for item in row:\n",
    "                            path_parts = item['path'].split('/')\n",
    "                            for part in path_parts:\n",
    "                                if 'TD' in part:\n",
    "                                    unique_tds.add(part)\n",
    "                # Convert the set to a list and sort it for consistent output\n",
    "                unique_tds_list = sorted(list(unique_tds))\n",
    "                        \n",
    "                sections_of_row = []\n",
    "                for td in unique_tds_list:\n",
    "                            td_section = \"\"\n",
    "                            for i in range(len(row)):\n",
    "                                if i ==0:\n",
    "                                    td += \"/\"\n",
    "                                if td in row[i]['path']:\n",
    "                                    td_section += row[i]['text'].strip()\n",
    "\n",
    "                            sections_of_row.append(td_section)\n",
    "                \n",
    "                # print(sections_of_row)\n",
    "                table_first_3_rows.append(sections_of_row)\n",
    "\n",
    "            # account for case where there are 2 index columns:\n",
    "            min_dim = max(list(map(lambda x: len(x), table_first_3_rows)))\n",
    "            if len(table_first_3_rows[0]) < min_dim:\n",
    "                table_first_3_rows[0].insert(0, \"empty\")\n",
    "            return table_first_3_rows\n",
    "\n",
    "    # Function to get the first 3 rows of the table into a string to parse to the LLM \n",
    "    def get_table_check_string(table_first_3_rows):\n",
    "        table_str = ''\n",
    "        for i in range(len(table_first_3_rows)):\n",
    "            table_str += f'''Row{i+1}: {\" | \".join(table_first_3_rows[i])}\\n'''\n",
    "        return table_str\n",
    "\n",
    "    # Function to clean the output if it decides to explain its choice.\n",
    "    def clean_llama_output_if_string(input_str):\n",
    "        \"\"\"\n",
    "        Extracts a dictionary-like substring from the given input string.\n",
    "        \n",
    "        Args:\n",
    "        input_str (str): The input string containing the dictionary-like substring.\n",
    "        \n",
    "        Returns:\n",
    "        dict: The extracted dictionary as a Python dictionary.\n",
    "        \"\"\"\n",
    "        # Use a regular expression to find the dictionary-like substring\n",
    "        match = re.search(r'\\{.*?\\}', input_str)\n",
    "        \n",
    "        if match:\n",
    "            dict_str = match.group()\n",
    "            try:\n",
    "                # Safely evaluate the dictionary string\n",
    "                extracted_dict = eval(dict_str)\n",
    "                if isinstance(extracted_dict, dict):\n",
    "                    return extracted_dict\n",
    "            except (SyntaxError, NameError):\n",
    "                pass\n",
    "        \n",
    "        return None\n",
    "\n",
    "    # Function to decide the indexing of the table\n",
    "    def eval_table_index_llama(table_str):\n",
    "        class Header(BaseModel):\n",
    "            index: int = Field(description=(\n",
    "                '''Index of the table indicating the type of indexing:\\n\\\n",
    "                    0 - No indexes\\n\\\n",
    "                    1 - Indexed by the first column\\n\\\n",
    "                    2 - Indexed by the first row\\n\\\n",
    "                    3 - Indexed by both the first row and the first column'''))\n",
    "\n",
    "        parser = JsonOutputParser(pydantic_object=Header)\n",
    "\n",
    "        chat = ChatGroq(temperature=0, model_name=\"llama3-8b-8192\")\n",
    "        \n",
    "        template = '''You will assist me in deciding, based on the first 3 entries of a table, whether the first row or the first colum should be the header, or if both the first column and first row are headers\n",
    "    Table:\n",
    "    {table}\n",
    "\n",
    "    Determine the indexing type of the table and output:\n",
    "    - 0 if the table is indexed by the first column.\n",
    "    - 1 if the table is indexed by the first row.\n",
    "    - 2 if the table is indexed by both the first row and the first column.   \n",
    "                \n",
    "    {format_instructions}'''\n",
    "        prompt = PromptTemplate(\n",
    "            template=template,\n",
    "            input_variables=[\"table\"],\n",
    "            partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "        )\n",
    "            \n",
    "        chain = prompt | chat | parser\n",
    "        return chain.invoke({\"table\": table_str})\n",
    "    \n",
    "    table_first_3_rows = get_first_three_rows(table)\n",
    "    table_str = get_table_check_string(table_first_3_rows)\n",
    "    header_output = eval_table_index_llama(table_str)\n",
    "    if isinstance(header_output, str):\n",
    "        header_output = clean_llama_output_if_string(header_output)\n",
    "    header_index = header_output['index']\n",
    "    # Now need diff cases of how to handle the table if the model outputs, 0,1, or 2\n",
    "        # where 0 is the first col, 1 is second col, 2 is for both cols are headers\n",
    "    pass\n",
    "\n",
    "if is_unidentified(table):\n",
    "    processed_unidentified_table = transform_unidentified_table(table)\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the extracted data from the extractor\n",
    "def get_extracted_data(extracted_data):\n",
    "    if 'structuredData.json' in extracted_data:\n",
    "        json_data = json.loads(extracted_data['structuredData.json'])\n",
    "    return json_data\n",
    "\n",
    "\n",
    "\n",
    "# This function converts tables to strings to be able to be processed by LLMs. \n",
    "def get_and_save_table_strings(extractor, table_output_directory):\n",
    "    \n",
    "    # Function to convert each row in a raw unprocessed table (ie index of table not decided) into a string\n",
    "    def get_raw_table_string(df):\n",
    "        table_str = \"\"\n",
    "        for i in range(2):\n",
    "            if i ==1:\n",
    "                table_str += f\"Row {i}: {df.iloc[i].values.tolist()}\"  \n",
    "            else:\n",
    "                table_str += f\"Row {i}: {df.iloc[i].values.tolist()}\\n\"\n",
    "        return table_str\n",
    "    \n",
    "    # Function to decide if header is first row or first column\n",
    "    def evaluate_table_index_llama(table_str):\n",
    "        class Header(BaseModel):\n",
    "            index: int = Field(description=\"Header of the table, 0 for first row as the header, 1 for first column as the header\")\n",
    "            \n",
    "        parser = JsonOutputParser(pydantic_object=Header)\n",
    "\n",
    "        chat = ChatGroq(temperature=0, model_name=\"llama3-8b-8192\")\n",
    "        \n",
    "        template = '''You will assist me in deciding, based on the first 2 entries of a table, whether the first row or the first colum should be the header. \n",
    "                You are to output an int, 0 or 1. Where 0 if the first row is header, and 1 if the first column is the header.\n",
    "                Follow the format instructions carefully.\n",
    "                Table:\n",
    "                {table}\n",
    "                \n",
    "                {format_instructions}\n",
    "                '''\n",
    "        prompt = PromptTemplate(\n",
    "            template=template,\n",
    "            input_variables=[\"table\"],\n",
    "            partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "        )\n",
    "        chain = prompt | chat | parser\n",
    "        return chain.invoke({\"table\": table_str})\n",
    "\n",
    "    # Tables need procecssing when extraced from BytesIO\n",
    "    def clean_table_values(x):\n",
    "        if isinstance(x, str):\n",
    "            return x.replace('_x000D_', '').strip()\n",
    "        return x\n",
    "    \n",
    "    # Code adapted from a medium blog on how to represent rows of tables in strings\n",
    "    def convert_table_to_string(df):\n",
    "        for _, row in df.iterrows():\n",
    "            row_str = \"\"\n",
    "            for col in df.columns:\n",
    "                sentences = re.split(r'(?<=\\.)\\s*', row[col])\n",
    "                row_sentence = \"\"\n",
    "                for i in range(len(sentences)):\n",
    "                    row_sentence += sentences[i] + \"\\n\"\n",
    "                row_str += f\"{col}: {row_sentence}, \"\n",
    "            formatted = row_str[:-2]\n",
    "        return formatted\n",
    "\n",
    "    os.makedirs(table_output_directory, exist_ok=True)\n",
    "    \n",
    "    # The literal extraction of the file itself\n",
    "    excel_files = {k: v for k, v in extractor.extracted_data.items() if k.endswith('.xlsx')}\n",
    "    \n",
    "    table_dataframes = {}\n",
    "\n",
    "    num_tables =0\n",
    "    for _, content in excel_files.items():\n",
    "        excel_stream = BytesIO(content)\n",
    "        df = pd.read_excel(excel_stream, header=None)\n",
    "        \n",
    "        df = df.applymap(clean_table_values)\n",
    "        # # Uncomment bottom code to ensure that Groq decides table header index \n",
    "        \n",
    "        # df_str = get_raw_table_string(df) \n",
    "        # dic = evaluate_table_index_llama(df_str)\n",
    "        # header_index = dic['index']\n",
    "        \n",
    "        # Uncomment this if u uncomment the code above\n",
    "        header_index = 1\n",
    "        \n",
    "        # If header_index is non zero\n",
    "        if header_index == 1:\n",
    "            df = pd.read_excel(excel_stream, header=None)\n",
    "            df = df.applymap(clean_table_values)\n",
    "            df = df.T\n",
    "            # Set the first row as the new header\n",
    "            new_header = df.iloc[0]  # Take the first row for the header\n",
    "            df = df[1:]  # Take the data less the header row\n",
    "            df.columns = new_header  # Set the header row as the df header\n",
    "            # Optionally, reset index if necessary\n",
    "            df.reset_index(drop=True, inplace=True)\n",
    "        else:\n",
    "            df = pd.read_excel(excel_stream, header=0)\n",
    "        \n",
    "        output_file_path = os.path.join(table_output_directory, f\"table_{num_tables}.csv\")\n",
    "\n",
    "        # Writing the table to the corresponding csv file. \n",
    "        df.to_csv(output_file_path, index=False)\n",
    "            \n",
    "        table_str = convert_table_to_string(df)\n",
    "        table_dataframes[num_tables] = table_str\n",
    "        num_tables += 1\n",
    "        \n",
    "    return table_dataframes\n",
    "\n",
    "\n",
    "\n",
    "# Function to initialise a flexible text splitter\n",
    "def initialise_text_splitter(chunk_size, chunk_overlap):\n",
    "    separator_list = [\"\\n\\n\", \"\\n\", \". \", \"!\", \"?\", \",\", \" \", \"\", \")\", \"(\"]\n",
    "    try:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=chunk_overlap,\n",
    "                length_function=len,\n",
    "                separators=separator_list)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to initialize text splitter: {e}\")\n",
    "        return None\n",
    "    return text_splitter\n",
    "\n",
    "\n",
    "\n",
    "# Function to obtain text chunks using the text splitter\n",
    "def get_text_chunks(file_path, json_data, text_splitter):\n",
    "    if 'elements' not in json_data:\n",
    "        logging.error(\"Missing 'elements' key in json_data\")\n",
    "        raise ValueError(\"Missing 'elements' key in json_data\")\n",
    "\n",
    "    if not text_splitter:\n",
    "        logging.error(\"Text splitter not initialised properly. \")\n",
    "        sys.exit()  \n",
    "        \n",
    "    file_name = file_path.split(\"/\")[-1]\n",
    "\n",
    "    # Chunks are split by pages here\n",
    "    page_text = \"\"\n",
    "    start_page = 0\n",
    "    all_chunks = []\n",
    "    \n",
    "    list_label = \"\"\n",
    "    for i in range(len(json_data['elements'])):\n",
    "        try:\n",
    "            current_page = json_data['elements'][i]['Page']\n",
    "        except KeyError:\n",
    "            logging.warning(f\"Missing 'Page' key in element at index {i}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            if current_page > start_page:\n",
    "                # Update the new page number\n",
    "                start_page = current_page               \n",
    "                # Generate the chunks for the previous page\n",
    "                separated_list = text_splitter.split_text(page_text)\n",
    "                for chunk in separated_list:\n",
    "                    if chunk not in [\". \", \".\"]:  # Simplified condition\n",
    "                        all_chunks.append({'ElementType': 'Text', 'file_name': file_name, 'Page': current_page, 'Text': chunk})\n",
    "                # Update the string of text \n",
    "                page_text = \"\"\n",
    "                list_label = \"\"\n",
    "            else:\n",
    "                if 'Text' in json_data['elements'][i]:  # Check if Text is not empty\n",
    "                    if json_data['elements'][i]['Path'].endswith(\"Lbl\") and not json_data['elements'][i]['Path'].startswith(\"//Document/Table\"):\n",
    "                        list_label = json_data['elements'][i]['Text']\n",
    "                    else:\n",
    "                        if list_label:\n",
    "                            page_text += list_label + json_data['elements'][i]['Text']\n",
    "                            list_label = \"\"  # Reset list label to empty string\n",
    "                        else:\n",
    "                            page_text += json_data['elements'][i]['Text'] + \"\\n\"\n",
    "        except KeyError as e:\n",
    "            logging.warning(f\"Key error in json_data['elements'][i] processing at index {i}: {e}\")\n",
    "    \n",
    "    \n",
    "    # Processing the last page of the text\n",
    "    if page_text:\n",
    "        separated_list = text_splitter.split_text(page_text)\n",
    "        for chunk in separated_list:\n",
    "            if chunk not in [\". \", \".\"]:\n",
    "                all_chunks.append({'ElementType': 'Text', 'file_name': file_name, 'Page': current_page, 'Text': chunk})\n",
    "\n",
    "    return all_chunks\n",
    "\n",
    "\n",
    "# Further enhancement to include the tables with the metadata so that it can be parsed to the \n",
    "    # function that takes in the tables + metadata for embeddings generation\n",
    "def get_table_strings_with_metadata(table_dataframes, json_data):\n",
    "    \n",
    "    # Function to obtain the page number of each table\n",
    "    def get_table_pages(json_data):\n",
    "        table_file_pages = {}\n",
    "        # Obtaining the table metadata\n",
    "        for i in range(len(json_data['elements'])):\n",
    "            try:\n",
    "                file_paths = json_data['elements'][i].get('filePaths')\n",
    "                if file_paths:\n",
    "                    page = json_data['elements'][i].get('Page', 'Unknown')\n",
    "                    match = re.search(r'\\d+', file_paths[0])\n",
    "                    table_index = match.group(0)\n",
    "                    table_file_pages[int(table_index)] = {\"Page\": page}\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing file paths at index {i}: {e}\")\n",
    "        return table_file_pages\n",
    "    \n",
    "    table_file_pages = get_table_pages(json_data)\n",
    "    meta_table_batch = []\n",
    "    table_dfs = []\n",
    "    for table_index,_ in table_dataframes.items():\n",
    "        dic = {}\n",
    "        dic['ElementType'] = 'Table'\n",
    "        dic['Page'] = table_file_pages[table_index]['Page']\n",
    "        dic['Table'] = table_dataframes[table_index]\n",
    "        table_dfs.append(dic)\n",
    "\n",
    "        #  Obtain metadata for sparse embeddings\n",
    "        meta_table_batch.append(f\"ElementType 'Table', Page {table_file_pages[table_index]['Page']}, {table_dataframes[table_index]}\")\n",
    "    return table_dfs, meta_table_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-02 18:38:10,933 - INFO - Started uploading asset\n",
      "2024-07-02 18:38:15,314 - INFO - Finished uploading asset\n",
      "2024-07-02 18:38:15,316 - INFO - Started submitting EXTRACT_PDF job\n",
      "2024-07-02 18:38:16,553 - INFO - Started getting job result\n",
      "2024-07-02 18:38:27,172 - INFO - Finished polling for status\n",
      "2024-07-02 18:38:27,176 - INFO - Finished getting job result\n",
      "2024-07-02 18:38:27,178 - INFO - Started getting content\n",
      "2024-07-02 18:38:27,486 - INFO - Finished getting content\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique ID: 0e4dcde5-fe65-4391-ade1-d1389e41d635\n"
     ]
    }
   ],
   "source": [
    "file_path = '../PDF/HSI1000-chapter1.pdf'\n",
    "extractor, unique_id = ExtractTextTableInfoFromPDF.create_with_unique_id(file_path)\n",
    "print(\"\\nUnique ID:\", unique_id)\n",
    "table_output_directory = f\"../data/{unique_id}\"\n",
    "extracted_data = extractor.extracted_data\n",
    "pdf_data = get_extracted_data(extracted_data)\n",
    "table_dataframes = get_and_save_table_strings(extractor, table_output_directory)\n",
    "\n",
    "# Use some form of evaluator to decide chunk size?\n",
    "text_splitter = initialise_text_splitter(300, 50)\n",
    "\n",
    "# Get out important information\n",
    "text_chunks = get_text_chunks(file_path, pdf_data, text_splitter)\n",
    "# table_dfs, meta_table_batch= get_table_strings_with_metadata(table_dataframes, pdf_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
