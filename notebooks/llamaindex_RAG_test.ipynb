{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import json\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "import time\n",
    "import sys\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "from dotenv import load_dotenv\n",
    "from io import BytesIO\n",
    "import io\n",
    "import zipfile\n",
    "import re\n",
    "\n",
    "# Adobe PDF Services imports\n",
    "from adobe.pdfservices.operation.auth.service_principal_credentials import ServicePrincipalCredentials\n",
    "from adobe.pdfservices.operation.exception.exceptions import ServiceApiException, ServiceUsageException, SdkException\n",
    "from adobe.pdfservices.operation.io.cloud_asset import CloudAsset\n",
    "from adobe.pdfservices.operation.io.stream_asset import StreamAsset\n",
    "from adobe.pdfservices.operation.pdf_services import PDFServices\n",
    "from adobe.pdfservices.operation.pdf_services_media_type import PDFServicesMediaType\n",
    "from adobe.pdfservices.operation.pdfjobs.jobs.extract_pdf_job import ExtractPDFJob\n",
    "from adobe.pdfservices.operation.pdfjobs.params.extract_pdf.extract_element_type import ExtractElementType\n",
    "from adobe.pdfservices.operation.pdfjobs.params.extract_pdf.extract_pdf_params import ExtractPDFParams\n",
    "from adobe.pdfservices.operation.pdfjobs.result.extract_pdf_result import ExtractPDFResult\n",
    "\n",
    "# Pinecone and Langchain imports\n",
    "from pinecone import Pinecone\n",
    "from pinecone_text.sparse import BM25Encoder\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "hf_key = os.getenv('HUGGINGFACE_API_KEY')\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "dense_embedder_api = os.getenv(\"HF_API_URL\")\n",
    "\n",
    "# Initialize clients\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "\n",
    "# Define model\n",
    "chat_model = \"llama3-8b-8192\"\n",
    "index = pc.Index('hsi-notes')\n",
    "namespace = 'Chapter-1'\n",
    "\n",
    "from typing import Optional, Union, TypeAlias\n",
    "\n",
    "# Import other necessary modules\n",
    "from llama_index.legacy import Document\n",
    "from llama_index.legacy import VectorStoreIndex, SimpleDirectoryReader, ServiceContext, PromptTemplate\n",
    "from llama_index.legacy.node_parser import SimpleNodeParser, SentenceWindowNodeParser\n",
    "from llama_index.legacy.postprocessor import MetadataReplacementPostProcessor, SentenceTransformerRerank\n",
    "from llama_index.legacy.schema import MetadataMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "class ExtractTextTableInfoFromPDF:\n",
    "    def __init__(self, file_path):\n",
    "        self.unique_id = str(uuid.uuid4())\n",
    "        try:\n",
    "            file = open(file_path, 'rb')\n",
    "            input_stream = file.read()\n",
    "            file.close()\n",
    "\n",
    "            # Initial setup, create credentials instance\n",
    "            credentials = ServicePrincipalCredentials(\n",
    "                client_id=os.getenv('ADOBE_SERVICES_CLIENT_ID'),\n",
    "                client_secret=os.getenv('ADOBE_SERVICES_CLIENT_SECRET')\n",
    "            )\n",
    "\n",
    "            # Creates a PDF Services instance\n",
    "            pdf_services = PDFServices(credentials=credentials)\n",
    "\n",
    "            # Creates an asset(s) from source file(s) and upload\n",
    "            input_asset = pdf_services.upload(input_stream=input_stream, mime_type=PDFServicesMediaType.PDF)\n",
    "\n",
    "            # Create parameters for the job\n",
    "            extract_pdf_params = ExtractPDFParams(\n",
    "                elements_to_extract=[ExtractElementType.TEXT, ExtractElementType.TABLES],\n",
    "            )\n",
    "\n",
    "            # Creates a new job instance\n",
    "            extract_pdf_job = ExtractPDFJob(input_asset=input_asset, extract_pdf_params=extract_pdf_params)\n",
    "\n",
    "            # Submit the job and gets the job result\n",
    "            location = pdf_services.submit(extract_pdf_job)\n",
    "            pdf_services_response = pdf_services.get_job_result(location, ExtractPDFResult)\n",
    "\n",
    "            # Get content from the resulting asset(s)\n",
    "            result_asset: CloudAsset = pdf_services_response.get_result().get_resource()\n",
    "            stream_asset: StreamAsset = pdf_services.get_content(result_asset)\n",
    "            \n",
    "            zip_bytes = io.BytesIO(stream_asset.get_input_stream())\n",
    "            with zipfile.ZipFile(zip_bytes, 'r') as zip_ref:\n",
    "                # Extract all the contents into memory\n",
    "                self.extracted_data = {name: zip_ref.read(name) for name in zip_ref.namelist()}\n",
    "                \n",
    "        except (ServiceApiException, ServiceUsageException, SdkException) as e:\n",
    "            logging.exception(f'Exception encountered while executing operation: {e}')\n",
    "    \n",
    "    # Generates a string containing a directory structure and file name for the output file using unique_id\n",
    "    @staticmethod\n",
    "    def create_output_file_path(unique_id: str) -> str:\n",
    "        os.makedirs(\"../data/Extracted_data\", exist_ok=True)\n",
    "        return f\"../data/Extracted_data/{unique_id}.zip\"\n",
    "\n",
    "    @classmethod\n",
    "    def create_with_unique_id(cls, file_path):\n",
    "        instance = cls(file_path)\n",
    "        return instance, instance.unique_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the extracted data from the extractor\n",
    "def get_extracted_data(extracted_data):\n",
    "    if 'structuredData.json' in extracted_data:\n",
    "        json_data = json.loads(extracted_data['structuredData.json'])\n",
    "    return json_data\n",
    "\n",
    "# Function to initialise a flexible text splitter\n",
    "def initialise_text_splitter(chunk_size, chunk_overlap):\n",
    "    separator_list = [\"\\n\\n\", \"\\n\", \". \", \"!\", \"?\", \",\", \" \", \"\", \")\", \"(\"]\n",
    "    try:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=chunk_overlap,\n",
    "                length_function=len,\n",
    "                separators=separator_list)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to initialize text splitter: {e}\")\n",
    "        return None\n",
    "    return text_splitter\n",
    "\n",
    "# Function to obtain text chunks using the text splitter\n",
    "def get_text_chunks(file_path, json_data, text_splitter):\n",
    "    if 'elements' not in json_data:\n",
    "        logging.error(\"Missing 'elements' key in json_data\")\n",
    "        raise ValueError(\"Missing 'elements' key in json_data\")\n",
    "\n",
    "    if not text_splitter:\n",
    "        logging.error(\"Text splitter not initialised properly. \")\n",
    "        sys.exit()  \n",
    "        \n",
    "    file_name = file_path.split(\"/\")[-1]\n",
    "\n",
    "    # Chunks are split by pages here\n",
    "    page_text = \"\"\n",
    "    start_page = 0\n",
    "    all_chunks = []\n",
    "    \n",
    "    list_label = \"\"\n",
    "    for i in range(len(json_data['elements'])):\n",
    "        try:\n",
    "            current_page = json_data['elements'][i]['Page']\n",
    "        except KeyError:\n",
    "            logging.warning(f\"Missing 'Page' key in element at index {i}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            if current_page > start_page:\n",
    "                # Update the new page number\n",
    "                start_page = current_page               \n",
    "                # Generate the chunks for the previous page\n",
    "                separated_list = text_splitter.split_text(page_text)\n",
    "                for chunk in separated_list:\n",
    "                    if chunk not in [\". \", \".\"]:  # Simplified condition\n",
    "                        all_chunks.append({'ElementType': 'Text', 'file_name': file_name, 'Page': current_page, 'Text': chunk})\n",
    "                # Update the string of text \n",
    "                page_text = \"\"\n",
    "                list_label = \"\"\n",
    "            else:\n",
    "                if 'Text' in json_data['elements'][i]:  # Check if Text is not empty\n",
    "                    if json_data['elements'][i]['Path'].endswith(\"Lbl\") and not json_data['elements'][i]['Path'].startswith(\"//Document/Table\"):\n",
    "                        list_label = json_data['elements'][i]['Text']\n",
    "                    else:\n",
    "                        if list_label:\n",
    "                            page_text += list_label + json_data['elements'][i]['Text']\n",
    "                            list_label = \"\"  # Reset list label to empty string\n",
    "                        else:\n",
    "                            page_text += json_data['elements'][i]['Text'] + \"\\n\"\n",
    "        except KeyError as e:\n",
    "            logging.warning(f\"Key error in json_data['elements'][i] processing at index {i}: {e}\")\n",
    "    \n",
    "    \n",
    "    # Processing the last page of the text\n",
    "    if page_text:\n",
    "        separated_list = text_splitter.split_text(page_text)\n",
    "        for chunk in separated_list:\n",
    "            if chunk not in [\". \", \".\"]:\n",
    "                all_chunks.append({'ElementType': 'Text', 'file_name': file_name, 'Page': current_page, 'Text': chunk})\n",
    "\n",
    "    return all_chunks\n",
    "\n",
    "# Function to derive the nodes from the text chunks\n",
    "def convert_textchunks_to_nodes(text_chunks):\n",
    "    # Conversion of text chunks to Documents\n",
    "    text_documents = [Document(text=chunk['Text'],\n",
    "                            metadata={\n",
    "                                \"file_name\": chunk['file_name'],\n",
    "                                \"page\": chunk['Page']\n",
    "                                },\n",
    "                            excluded_llm_metadata_keys=[\"file_name\"],\n",
    "                            metadata_seperator=\"::\",\n",
    "                            metadata_template=\"{key}=>{value}\",\n",
    "                            text_template=\"Metadata: {metadata_str}\\n-----\\nContent: {content}\")\n",
    "\n",
    "                                for chunk in text_chunks]\n",
    "    print(\n",
    "        \"The LLM sees this: \\n\",\n",
    "        text_documents[0].get_content(metadata_mode=MetadataMode.LLM),\n",
    "    )\n",
    "    print(\"-\"* 80)\n",
    "    print(\n",
    "        \"The Embedding model sees this: \\n\",\n",
    "        text_documents[0].get_content(metadata_mode=MetadataMode.EMBED),\n",
    "    )\n",
    "\n",
    "    # create the sentence window node parser w/ default settings\n",
    "    SW_node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "        window_size=1,\n",
    "        window_metadata_key=\"window\",\n",
    "        original_text_metadata_key=\"original_text\",\n",
    "    )\n",
    "\n",
    "    # Create the nodes\n",
    "    nodes = SW_node_parser.get_nodes_from_documents(text_documents)\n",
    "    return nodes\n",
    "\n",
    "# Function to get the unique tables from all the table elements\n",
    "def extract_unique_tables(table_elements):\n",
    "    tables = set()\n",
    "    for item in table_elements:\n",
    "        match = re.search(r'/Table(\\[\\d+\\])?', item['Path'])\n",
    "        if match:\n",
    "            tables.add('Table' + (match.group(1) if match.group(1) else ''))\n",
    "    \n",
    "    unique_tables = list(tables)\n",
    "    unique_tables[0] += \"/\"\n",
    "\n",
    "    extracted_tables = {}\n",
    "    i=0\n",
    "    for table_name in unique_tables:\n",
    "        table = []\n",
    "        for el in table_elements:\n",
    "            if table_name in el['Path']:\n",
    "                # ADjust this here if u need to extract more information from the table elements\n",
    "                table.append({\"path\": el['Path'], \"text\": el['Text'], \"Page\": el[\"Page\"]})\n",
    "        extracted_tables[i+1] = table\n",
    "        i += 1\n",
    "\n",
    "    return dict(sorted(extracted_tables.items()))\n",
    "\n",
    "# Function to take in table elements from a specific table and convert it to a pandas dataframe\n",
    "def transform_table(table, output_file_path):\n",
    "    # only need to look at the first row\n",
    "    first_row = [el for el in table if \"TR/\" in el['path']]\n",
    "    # Function to decide if table has 2 indexes\n",
    "    def is_2_index(first_row):\n",
    "        for el in first_row:\n",
    "            if \"TH/\" in el['path']:\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "    \n",
    "    # Function to decide if the table's row is the header   \n",
    "    def is_row_header(first_row):\n",
    "        for el in first_row:\n",
    "            if \"TH\" not in el['path']:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    # If the table has 2 indexes\n",
    "    if is_2_index(first_row):\n",
    "        print(\"This table has 2 indexes\")\n",
    "        # Function to produce table which has 2 indexes\n",
    "        def get_2_index_table(table):\n",
    "            table_rows = set([re.search(r'Table(?:\\[\\d+\\])?/TR(?:\\[\\d+\\])?', item['path']).group() for item in table if re.search(r'Table(?:\\[\\d+\\])?/TR(?:\\[\\d+\\])?', item['path'])])\n",
    "            uniq_rows = sorted([row_name.split(\"/\")[-1] for row_name in list(table_rows)])\n",
    "\n",
    "            # Get indexes from the first row:\n",
    "            row_indexes = [el['text'].strip() for el in table if \"TR/TH\" in el['path']]\n",
    "\n",
    "            data = {}\n",
    "\n",
    "            # Only look at second row onwards\n",
    "            for i in range(1,len(uniq_rows)):\n",
    "                row_name = uniq_rows[i]\n",
    "                row = [el for el in table if row_name in el['path']]\n",
    "                row_key = row[0]['text'].strip()\n",
    "                \n",
    "                unique_tds = set()\n",
    "                for item in row:\n",
    "                    path_parts = item['path'].split('/')\n",
    "                    for part in path_parts:\n",
    "                        if 'TD' in part:\n",
    "                            unique_tds.add(part)\n",
    "                # Convert the set to a list and sort it for consistent output\n",
    "                unique_tds_list = sorted(list(unique_tds))\n",
    "\n",
    "                sections_of_row = []\n",
    "                for td in unique_tds_list:\n",
    "                    td_section = \"\"\n",
    "                    for i in range(len(row)):\n",
    "                        if i ==0:\n",
    "                            td += \"/\"\n",
    "                        if td in row[i]['path']:\n",
    "                            td_section += row[i]['text'].strip()\n",
    "                            \n",
    "                    sections_of_row.append(td_section)\n",
    "                \n",
    "                data[row_key] = sections_of_row\n",
    "                \n",
    "            df = pd.DataFrame(data, index=row_indexes).T\n",
    "            df.to_csv(output_file_path)\n",
    "            return df    \n",
    "        \n",
    "        df = get_2_index_table(table)\n",
    "        return df\n",
    "    \n",
    "    # If the table only has one index\n",
    "    else:\n",
    "        print(\"This table has 1 index\")\n",
    "\n",
    "        # If the header for this df is the row\n",
    "        if is_row_header(first_row):\n",
    "            print(\"This is a row indexed table\")\n",
    "            def get_row_header_table(table):\n",
    "                table_rows = set([re.search(r'Table(?:\\[\\d+\\])?/TR(?:\\[\\d+\\])?', item['path']).group() for item in table if re.search(r'Table(?:\\[\\d+\\])?/TR(?:\\[\\d+\\])?', item['path'])])\n",
    "                uniq_rows = sorted([row_name.split(\"/\")[-1] for row_name in list(table_rows)])\n",
    "\n",
    "                # Table headers, also the keys\n",
    "                headers = [el['text'].strip() for el in table if uniq_rows[0]+\"/\" in el['path']]\n",
    "                rows = []\n",
    "                for i in range(1,len(uniq_rows)):\n",
    "                    row_name = uniq_rows[i]\n",
    "                    row = [el for el in table if row_name in el['path']]\n",
    "                    # rows.append(row)\n",
    "                    \n",
    "                    unique_tds = set()\n",
    "                    for item in row:\n",
    "                        path_parts = item['path'].split('/')\n",
    "                        for part in path_parts:\n",
    "                            if 'TD' in part:\n",
    "                                unique_tds.add(part)\n",
    "                    # Convert the set to a list and sort it for consistent output\n",
    "                    unique_tds_list = sorted(list(unique_tds))\n",
    "\n",
    "                    sections_of_row = []\n",
    "                    for td in unique_tds_list:\n",
    "                        td_section = \"\"\n",
    "                        for i in range(len(row)):\n",
    "                            if i ==0:\n",
    "                                td += \"/\"\n",
    "                            if td in row[i]['path']:\n",
    "                                td_section += row[i]['text'].strip()\n",
    "                                \n",
    "                        sections_of_row.append(td_section)\n",
    "                    rows.append(sections_of_row)\n",
    "                                \n",
    "                df = pd.DataFrame(rows, columns=headers)\n",
    "                return df\n",
    "            \n",
    "            df = get_row_header_table(table)\n",
    "            df.to_csv(output_file_path, index=False)\n",
    "            return df\n",
    "                \n",
    "        # The header for this df is the column\n",
    "        else:\n",
    "            print(\"This table is a column indexed table...\")\n",
    "            def get_column_header_table(table):\n",
    "                table_rows = set([re.search(r'Table(?:\\[\\d+\\])?/TR(?:\\[\\d+\\])?', item['path']).group() for item in table if re.search(r'Table(?:\\[\\d+\\])?/TR(?:\\[\\d+\\])?', item['path'])])\n",
    "                uniq_rows = sorted([row_name.split(\"/\")[-1] for row_name in list(table_rows)])\n",
    "\n",
    "                data = {}\n",
    "\n",
    "                for i in range(len(uniq_rows)):\n",
    "                                    row_name = uniq_rows[i]\n",
    "                                    if i == 0:\n",
    "                                        row = [el for el in table if row_name+\"/\" in el['path']]\n",
    "                                    else:\n",
    "                                        row = [el for el in table if row_name in el['path']]\n",
    "                                    \n",
    "                                    unique_tds = set()\n",
    "                                    for item in row:\n",
    "                                        path_parts = item['path'].split('/')\n",
    "                                        for part in path_parts:\n",
    "                                            if 'TD' in part:\n",
    "                                                unique_tds.add(part)\n",
    "                                    # Convert the set to a list and sort it for consistent output\n",
    "                                    unique_tds_list = sorted(list(unique_tds))\n",
    "                                    \n",
    "                                    sections_of_row = []\n",
    "                                    for td in unique_tds_list:\n",
    "                                        td_section = \"\"\n",
    "                                        for i in range(len(row)):\n",
    "                                            if i ==0:\n",
    "                                                td += \"/\"\n",
    "                                            if td in row[i]['path']:\n",
    "                                                td_section += row[i]['text'].strip()\n",
    "                                                \n",
    "                                        sections_of_row.append(td_section)\n",
    "                                                \n",
    "                                    row_key = row[0]['text'].strip()\n",
    "                                    data[row_key] = sections_of_row\n",
    "                df = pd.DataFrame.from_dict(data, orient='index').T\n",
    "                return df\n",
    "                                \n",
    "            df = get_column_header_table(table)\n",
    "            df.to_csv(output_file_path, index=False)\n",
    "            return df\n",
    "\n",
    "# Function that saves the tables in CSV format to a unique job id       \n",
    "def save_tables_to_csv(extracted_tables, table_output_directory):\n",
    "    \n",
    "    os.makedirs(table_output_directory, exist_ok=True)\n",
    "    for table_num, table in extracted_tables.items():\n",
    "        output_file_path = os.path.join(table_output_directory, f\"table_{table_num}.csv\")\n",
    "        df = transform_table(table, output_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runing the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../PDF/HSI1000-chapter1.pdf'\n",
    "extractor, unique_id = ExtractTextTableInfoFromPDF.create_with_unique_id(file_path)\n",
    "print(\"\\nUnique ID:\", unique_id)\n",
    "extracted_data = extractor.extracted_data\n",
    "pdf_data = get_extracted_data(extracted_data)\n",
    "\n",
    "# Section here to derive the csvs from the table elements\n",
    "table_elements = [el for el in pdf_data['elements'] if \"Table\" in el['Path'] and 'Text' in el and \"TR\" in el['Path']]\n",
    "print(\"\\nUnique ID:\", unique_id)\n",
    "\n",
    "# IF there are even any table elements in the PDF\n",
    "if table_elements:\n",
    "    table_output_directory = f\"../data/{unique_id}\" \n",
    "    extracted_tables = extract_unique_tables(table_elements)    \n",
    "    save_tables_to_csv(extracted_tables, table_output_directory)\n",
    "    \n",
    "# Section here to derive the nodes from the text elements\n",
    "text_splitter = initialise_text_splitter(600, 50)\n",
    "text_chunks = get_text_chunks(file_path, pdf_data, text_splitter)\n",
    "text_nodes = convert_textchunks_to_nodes(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'path': '//Document/Table[4]/TR/TD/P', 'text': 'Observations ', 'Page': 3},\n",
       " {'path': '//Document/Table[4]/TR/TD[2]/L/LI/Lbl', 'text': '(1) ', 'Page': 3},\n",
       " {'path': '//Document/Table[4]/TR/TD[2]/L/LI/LBody',\n",
       "  'text': 'Tea bag bloats and floats on top of the water when boiling water is poured directly on top of it. ',\n",
       "  'Page': 3},\n",
       " {'path': '//Document/Table[4]/TR/TD[2]/L/LI[2]/Lbl',\n",
       "  'text': '(2) ',\n",
       "  'Page': 3},\n",
       " {'path': '//Document/Table[4]/TR/TD[2]/L/LI[2]/LBody',\n",
       "  'text': 'Tea bag doesn’t bloat and sinks in the water when boiling water is poured on the side and not directly onto it. ',\n",
       "  'Page': 3},\n",
       " {'path': '//Document/Table[4]/TR[2]/TD/P', 'text': 'Explanation ', 'Page': 3},\n",
       " {'path': '//Document/Table[4]/TR[2]/TD[2]/P',\n",
       "  'text': 'Water poured on top of the tea bag fills the pores of the teabag itself, trapping any gas inside before it can escape. The hot water heats the trapped air, causing it to expand. The trapped air prevents the tea bag from being dunked. ',\n",
       "  'Page': 3},\n",
       " {'path': '//Document/Table[4]/TR[2]/TD[2]/P[2]/Sub',\n",
       "  'text': 'Pores of the tea bag can get sealed up with water ',\n",
       "  'Page': 3},\n",
       " {'path': '//Document/Table[4]/TR[2]/TD[2]/P[2]/Sub[2]',\n",
       "  'text': 'and prevent air from escaping. ',\n",
       "  'Page': 3},\n",
       " {'path': '//Document/Table[4]/TR[3]/TD/P',\n",
       "  'text': 'Test the explanation ',\n",
       "  'Page': 3},\n",
       " {'path': '//Document/Table[4]/TR[3]/TD[2]/P',\n",
       "  'text': 'Quickly seal the tea bag in cold water, trapping the air, then pour boiling water near to but not directly onto it. ',\n",
       "  'Page': 3},\n",
       " {'path': '//Document/Table[4]/TR[4]/TD/P',\n",
       "  'text': 'Result of test ',\n",
       "  'Page': 3},\n",
       " {'path': '//Document/Table[4]/TR[4]/TD[2]/P',\n",
       "  'text': 'Tea bag bloats and floats on top of the water supporting the explanation. ',\n",
       "  'Page': 3}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = extracted_tables[2]\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only need to look at the first row\n",
    "first_row = [el for el in table if \"TR[2]/\" in el['path']]\n",
    "    # Function to decide if table has 2 indexes\n",
    "\n",
    "# Function to check if the API could not identofy the index of the table\n",
    "def is_unidentified():\n",
    "    pass\n",
    "\n",
    "# Function to transform the unidentified table by determining its index using LLM\n",
    "def transform_unidentified_table():\n",
    "    \n",
    "    # Need to use the groq method\n",
    "    for el in table:\n",
    "        pass\n",
    "        pass\n",
    "\n",
    "\n",
    "def is_2_index(first_row):\n",
    "        for el in first_row:\n",
    "            if \"TH/\" in el['path']:\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "    \n",
    "    # Function to decide if the table's row is the header   \n",
    "def is_row_header(first_row):\n",
    "        for el in first_row:\n",
    "            if \"TH\" not in el['path']:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "is_2_index(first_row)\n",
    "# is_row_header(first_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'path': '//Document/Table[4]/TR[2]/TD/P', 'text': 'Explanation ', 'Page': 3},\n",
       " {'path': '//Document/Table[4]/TR[2]/TD[2]/P',\n",
       "  'text': 'Water poured on top of the tea bag fills the pores of the teabag itself, trapping any gas inside before it can escape. The hot water heats the trapped air, causing it to expand. The trapped air prevents the tea bag from being dunked. ',\n",
       "  'Page': 3},\n",
       " {'path': '//Document/Table[4]/TR[2]/TD[2]/P[2]/Sub',\n",
       "  'text': 'Pores of the tea bag can get sealed up with water ',\n",
       "  'Page': 3},\n",
       " {'path': '//Document/Table[4]/TR[2]/TD[2]/P[2]/Sub[2]',\n",
       "  'text': 'and prevent air from escaping. ',\n",
       "  'Page': 3}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (2) does not match length of index (0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 203\u001b[0m, in \u001b[0;36mtransform_table\u001b[0;34m(table, output_file_path)\u001b[0m\n\u001b[1;32m    200\u001b[0m         df\u001b[38;5;241m.\u001b[39mto_csv(output_file_path)\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m df    \n\u001b[0;32m--> 203\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mget_2_index_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m# If the table only has one index\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;66;03m# If the header for this df is the row\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 199\u001b[0m, in \u001b[0;36mtransform_table.<locals>.get_2_index_table\u001b[0;34m(table)\u001b[0m\n\u001b[1;32m    195\u001b[0m         sections_of_row\u001b[38;5;241m.\u001b[39mappend(td_section)\n\u001b[1;32m    197\u001b[0m     data[row_key] \u001b[38;5;241m=\u001b[39m sections_of_row\n\u001b[0;32m--> 199\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow_indexes\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    200\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(output_file_path)\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/pythonenv/py310/lib/python3.10/site-packages/pandas/core/frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    774\u001b[0m     )\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/pythonenv/py310/lib/python3.10/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pythonenv/py310/lib/python3.10/site-packages/pandas/core/internals/construction.py:119\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# don't force copy because getting jammed in an ndarray anyway\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     arrays, refs \u001b[38;5;241m=\u001b[39m \u001b[43m_homogenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# _homogenize ensures\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m#  - all(len(x) == len(index) for x in arrays)\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m#  - all(x.ndim == 1 for x in arrays)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    125\u001b[0m \n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    127\u001b[0m     index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/pythonenv/py310/lib/python3.10/site-packages/pandas/core/internals/construction.py:630\u001b[0m, in \u001b[0;36m_homogenize\u001b[0;34m(data, index, dtype)\u001b[0m\n\u001b[1;32m    627\u001b[0m         val \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mfast_multiget(val, oindex\u001b[38;5;241m.\u001b[39m_values, default\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mnan)\n\u001b[1;32m    629\u001b[0m     val \u001b[38;5;241m=\u001b[39m sanitize_array(val, index, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 630\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m     refs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    633\u001b[0m homogenized\u001b[38;5;241m.\u001b[39mappend(val)\n",
      "File \u001b[0;32m~/pythonenv/py310/lib/python3.10/site-packages/pandas/core/common.py:573\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[0;32m--> 573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    578\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (2) does not match length of index (0)"
     ]
    }
   ],
   "source": [
    "df = transform_table(table, \"nice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next thing to do:\n",
    "- Clean the documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_text(content: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove unwanted characters and patterns in text input.\n",
    "\n",
    "    :param content: Text input.\n",
    "\n",
    "    :return: Cleaned version of original text input.\n",
    "    \"\"\"\n",
    "\n",
    "    # Fix hyphenated words broken by newline\n",
    "    content = re.sub(r'(\\w+)-\\n(\\w+)', r'\\1\\2', content)\n",
    "\n",
    "    # Remove specific unwanted patterns and characters\n",
    "    unwanted_patterns = [\n",
    "        \"\\\\n\", \"  —\", \"——————————\", \"—————————\", \"—————\",\n",
    "        r'\\\\u[\\dA-Fa-f]{4}', r'\\uf075', r'\\uf0b7'\n",
    "    ]\n",
    "    for pattern in unwanted_patterns:\n",
    "        content = re.sub(pattern, \"\", content)\n",
    "\n",
    "    # Fix improperly spaced hyphenated words and normalize whitespace\n",
    "    content = re.sub(r'(\\w)\\s*-\\s*(\\w)', r'\\1-\\2', content)\n",
    "    content = re.sub(r'\\s+', ' ', content)\n",
    "\n",
    "    return content\n",
    "\n",
    "# Call function\n",
    "cleaned_docs = []\n",
    "for d in documents:\n",
    "    cleaned_text = clean_up_text(d.text)\n",
    "    d.text = cleaned_text\n",
    "    cleaned_docs.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the extracted data from the extractor\n",
    "def get_extracted_data(extracted_data):\n",
    "    if 'structuredData.json' in extracted_data:\n",
    "        json_data = json.loads(extracted_data['structuredData.json'])\n",
    "    return json_data\n",
    "\n",
    "\n",
    "\n",
    "# This function converts tables to strings to be able to be processed by LLMs. \n",
    "def get_and_save_table_strings(extractor, table_output_directory):\n",
    "    \n",
    "    # Function to convert each row in a raw unprocessed table (ie index of table not decided) into a string\n",
    "    def get_raw_table_string(df):\n",
    "        table_str = \"\"\n",
    "        for i in range(2):\n",
    "            if i ==1:\n",
    "                table_str += f\"Row {i}: {df.iloc[i].values.tolist()}\"  \n",
    "            else:\n",
    "                table_str += f\"Row {i}: {df.iloc[i].values.tolist()}\\n\"\n",
    "        return table_str\n",
    "    \n",
    "    # Function to decide if header is first row or first column\n",
    "    def evaluate_table_index_llama(table_str):\n",
    "        class Header(BaseModel):\n",
    "            index: int = Field(description=\"Header of the table, 0 for first row as the header, 1 for first column as the header\")\n",
    "            \n",
    "        parser = JsonOutputParser(pydantic_object=Header)\n",
    "\n",
    "        chat = ChatGroq(temperature=0, model_name=\"llama3-8b-8192\")\n",
    "        \n",
    "        template = '''You will assist me in deciding, based on the first 2 entries of a table, whether the first row or the first colum should be the header. \n",
    "                You are to output an int, 0 or 1. Where 0 if the first row is header, and 1 if the first column is the header.\n",
    "                Follow the format instructions carefully.\n",
    "                Table:\n",
    "                {table}\n",
    "                \n",
    "                {format_instructions}\n",
    "                '''\n",
    "        prompt = PromptTemplate(\n",
    "            template=template,\n",
    "            input_variables=[\"table\"],\n",
    "            partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "        )\n",
    "        chain = prompt | chat | parser\n",
    "        return chain.invoke({\"table\": table_str})\n",
    "\n",
    "    # Tables need procecssing when extraced from BytesIO\n",
    "    def clean_table_values(x):\n",
    "        if isinstance(x, str):\n",
    "            return x.replace('_x000D_', '').strip()\n",
    "        return x\n",
    "    \n",
    "    # Code adapted from a medium blog on how to represent rows of tables in strings\n",
    "    def convert_table_to_string(df):\n",
    "        for _, row in df.iterrows():\n",
    "            row_str = \"\"\n",
    "            for col in df.columns:\n",
    "                sentences = re.split(r'(?<=\\.)\\s*', row[col])\n",
    "                row_sentence = \"\"\n",
    "                for i in range(len(sentences)):\n",
    "                    row_sentence += sentences[i] + \"\\n\"\n",
    "                row_str += f\"{col}: {row_sentence}, \"\n",
    "            formatted = row_str[:-2]\n",
    "        return formatted\n",
    "\n",
    "    os.makedirs(table_output_directory, exist_ok=True)\n",
    "    \n",
    "    # The literal extraction of the file itself\n",
    "    excel_files = {k: v for k, v in extractor.extracted_data.items() if k.endswith('.xlsx')}\n",
    "    \n",
    "    table_dataframes = {}\n",
    "\n",
    "    num_tables =0\n",
    "    for _, content in excel_files.items():\n",
    "        excel_stream = BytesIO(content)\n",
    "        df = pd.read_excel(excel_stream, header=None)\n",
    "        \n",
    "        df = df.applymap(clean_table_values)\n",
    "        # # Uncomment bottom code to ensure that Groq decides table header index \n",
    "        \n",
    "        # df_str = get_raw_table_string(df) \n",
    "        # dic = evaluate_table_index_llama(df_str)\n",
    "        # header_index = dic['index']\n",
    "        \n",
    "        # Uncomment this if u uncomment the code above\n",
    "        header_index = 1\n",
    "        \n",
    "        # If header_index is non zero\n",
    "        if header_index == 1:\n",
    "            df = pd.read_excel(excel_stream, header=None)\n",
    "            df = df.applymap(clean_table_values)\n",
    "            df = df.T\n",
    "            # Set the first row as the new header\n",
    "            new_header = df.iloc[0]  # Take the first row for the header\n",
    "            df = df[1:]  # Take the data less the header row\n",
    "            df.columns = new_header  # Set the header row as the df header\n",
    "            # Optionally, reset index if necessary\n",
    "            df.reset_index(drop=True, inplace=True)\n",
    "        else:\n",
    "            df = pd.read_excel(excel_stream, header=0)\n",
    "        \n",
    "        output_file_path = os.path.join(table_output_directory, f\"table_{num_tables}.csv\")\n",
    "\n",
    "        # Writing the table to the corresponding csv file. \n",
    "        df.to_csv(output_file_path, index=False)\n",
    "            \n",
    "        table_str = convert_table_to_string(df)\n",
    "        table_dataframes[num_tables] = table_str\n",
    "        num_tables += 1\n",
    "        \n",
    "    return table_dataframes\n",
    "\n",
    "\n",
    "\n",
    "# Function to initialise a flexible text splitter\n",
    "def initialise_text_splitter(chunk_size, chunk_overlap):\n",
    "    separator_list = [\"\\n\\n\", \"\\n\", \". \", \"!\", \"?\", \",\", \" \", \"\", \")\", \"(\"]\n",
    "    try:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=chunk_overlap,\n",
    "                length_function=len,\n",
    "                separators=separator_list)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to initialize text splitter: {e}\")\n",
    "        return None\n",
    "    return text_splitter\n",
    "\n",
    "\n",
    "\n",
    "# Function to obtain text chunks using the text splitter\n",
    "def get_text_chunks(file_path, json_data, text_splitter):\n",
    "    if 'elements' not in json_data:\n",
    "        logging.error(\"Missing 'elements' key in json_data\")\n",
    "        raise ValueError(\"Missing 'elements' key in json_data\")\n",
    "\n",
    "    if not text_splitter:\n",
    "        logging.error(\"Text splitter not initialised properly. \")\n",
    "        sys.exit()  \n",
    "        \n",
    "    file_name = file_path.split(\"/\")[-1]\n",
    "\n",
    "    # Chunks are split by pages here\n",
    "    page_text = \"\"\n",
    "    start_page = 0\n",
    "    all_chunks = []\n",
    "    \n",
    "    list_label = \"\"\n",
    "    for i in range(len(json_data['elements'])):\n",
    "        try:\n",
    "            current_page = json_data['elements'][i]['Page']\n",
    "        except KeyError:\n",
    "            logging.warning(f\"Missing 'Page' key in element at index {i}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            if current_page > start_page:\n",
    "                # Update the new page number\n",
    "                start_page = current_page               \n",
    "                # Generate the chunks for the previous page\n",
    "                separated_list = text_splitter.split_text(page_text)\n",
    "                for chunk in separated_list:\n",
    "                    if chunk not in [\". \", \".\"]:  # Simplified condition\n",
    "                        all_chunks.append({'ElementType': 'Text', 'file_name': file_name, 'Page': current_page, 'Text': chunk})\n",
    "                # Update the string of text \n",
    "                page_text = \"\"\n",
    "                list_label = \"\"\n",
    "            else:\n",
    "                if 'Text' in json_data['elements'][i]:  # Check if Text is not empty\n",
    "                    if json_data['elements'][i]['Path'].endswith(\"Lbl\") and not json_data['elements'][i]['Path'].startswith(\"//Document/Table\"):\n",
    "                        list_label = json_data['elements'][i]['Text']\n",
    "                    else:\n",
    "                        if list_label:\n",
    "                            page_text += list_label + json_data['elements'][i]['Text']\n",
    "                            list_label = \"\"  # Reset list label to empty string\n",
    "                        else:\n",
    "                            page_text += json_data['elements'][i]['Text'] + \"\\n\"\n",
    "        except KeyError as e:\n",
    "            logging.warning(f\"Key error in json_data['elements'][i] processing at index {i}: {e}\")\n",
    "    \n",
    "    \n",
    "    # Processing the last page of the text\n",
    "    if page_text:\n",
    "        separated_list = text_splitter.split_text(page_text)\n",
    "        for chunk in separated_list:\n",
    "            if chunk not in [\". \", \".\"]:\n",
    "                all_chunks.append({'ElementType': 'Text', 'file_name': file_name, 'Page': current_page, 'Text': chunk})\n",
    "\n",
    "    return all_chunks\n",
    "\n",
    "\n",
    "# Further enhancement to include the tables with the metadata so that it can be parsed to the \n",
    "    # function that takes in the tables + metadata for embeddings generation\n",
    "def get_table_strings_with_metadata(table_dataframes, json_data):\n",
    "    \n",
    "    # Function to obtain the page number of each table\n",
    "    def get_table_pages(json_data):\n",
    "        table_file_pages = {}\n",
    "        # Obtaining the table metadata\n",
    "        for i in range(len(json_data['elements'])):\n",
    "            try:\n",
    "                file_paths = json_data['elements'][i].get('filePaths')\n",
    "                if file_paths:\n",
    "                    page = json_data['elements'][i].get('Page', 'Unknown')\n",
    "                    match = re.search(r'\\d+', file_paths[0])\n",
    "                    table_index = match.group(0)\n",
    "                    table_file_pages[int(table_index)] = {\"Page\": page}\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing file paths at index {i}: {e}\")\n",
    "        return table_file_pages\n",
    "    \n",
    "    table_file_pages = get_table_pages(json_data)\n",
    "    meta_table_batch = []\n",
    "    table_dfs = []\n",
    "    for table_index,_ in table_dataframes.items():\n",
    "        dic = {}\n",
    "        dic['ElementType'] = 'Table'\n",
    "        dic['Page'] = table_file_pages[table_index]['Page']\n",
    "        dic['Table'] = table_dataframes[table_index]\n",
    "        table_dfs.append(dic)\n",
    "\n",
    "        #  Obtain metadata for sparse embeddings\n",
    "        meta_table_batch.append(f\"ElementType 'Table', Page {table_file_pages[table_index]['Page']}, {table_dataframes[table_index]}\")\n",
    "    return table_dfs, meta_table_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-02 18:38:10,933 - INFO - Started uploading asset\n",
      "2024-07-02 18:38:15,314 - INFO - Finished uploading asset\n",
      "2024-07-02 18:38:15,316 - INFO - Started submitting EXTRACT_PDF job\n",
      "2024-07-02 18:38:16,553 - INFO - Started getting job result\n",
      "2024-07-02 18:38:27,172 - INFO - Finished polling for status\n",
      "2024-07-02 18:38:27,176 - INFO - Finished getting job result\n",
      "2024-07-02 18:38:27,178 - INFO - Started getting content\n",
      "2024-07-02 18:38:27,486 - INFO - Finished getting content\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique ID: 0e4dcde5-fe65-4391-ade1-d1389e41d635\n"
     ]
    }
   ],
   "source": [
    "file_path = '../PDF/HSI1000-chapter1.pdf'\n",
    "extractor, unique_id = ExtractTextTableInfoFromPDF.create_with_unique_id(file_path)\n",
    "print(\"\\nUnique ID:\", unique_id)\n",
    "table_output_directory = f\"../data/{unique_id}\"\n",
    "extracted_data = extractor.extracted_data\n",
    "pdf_data = get_extracted_data(extracted_data)\n",
    "table_dataframes = get_and_save_table_strings(extractor, table_output_directory)\n",
    "\n",
    "# Use some form of evaluator to decide chunk size?\n",
    "text_splitter = initialise_text_splitter(300, 50)\n",
    "\n",
    "# Get out important information\n",
    "text_chunks = get_text_chunks(file_path, pdf_data, text_splitter)\n",
    "# table_dfs, meta_table_batch= get_table_strings_with_metadata(table_dataframes, pdf_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
