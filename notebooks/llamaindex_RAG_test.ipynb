{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import json\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "import time\n",
    "import sys\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "from dotenv import load_dotenv\n",
    "from io import BytesIO\n",
    "import io\n",
    "import zipfile\n",
    "import re\n",
    "\n",
    "# Adobe PDF Services imports\n",
    "from adobe.pdfservices.operation.auth.service_principal_credentials import ServicePrincipalCredentials\n",
    "from adobe.pdfservices.operation.exception.exceptions import ServiceApiException, ServiceUsageException, SdkException\n",
    "from adobe.pdfservices.operation.io.cloud_asset import CloudAsset\n",
    "from adobe.pdfservices.operation.io.stream_asset import StreamAsset\n",
    "from adobe.pdfservices.operation.pdf_services import PDFServices\n",
    "from adobe.pdfservices.operation.pdf_services_media_type import PDFServicesMediaType\n",
    "from adobe.pdfservices.operation.pdfjobs.jobs.extract_pdf_job import ExtractPDFJob\n",
    "from adobe.pdfservices.operation.pdfjobs.params.extract_pdf.extract_element_type import ExtractElementType\n",
    "from adobe.pdfservices.operation.pdfjobs.params.extract_pdf.extract_pdf_params import ExtractPDFParams\n",
    "from adobe.pdfservices.operation.pdfjobs.result.extract_pdf_result import ExtractPDFResult\n",
    "\n",
    "# Pinecone and Langchain imports\n",
    "from pinecone import Pinecone\n",
    "from pinecone_text.sparse import BM25Encoder\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "hf_key = os.getenv('HUGGINGFACE_API_KEY')\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "dense_embedder_api = os.getenv(\"HF_API_URL\")\n",
    "\n",
    "# Initialize clients\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "\n",
    "# Define model\n",
    "chat_model = \"llama3-8b-8192\"\n",
    "index = pc.Index('hsi-notes')\n",
    "namespace = 'Chapter-1'\n",
    "\n",
    "from typing import Optional, Union, TypeAlias\n",
    "\n",
    "# Import other necessary modules\n",
    "from llama_index.legacy import Document\n",
    "from llama_index.legacy import VectorStoreIndex, SimpleDirectoryReader, ServiceContext, PromptTemplate\n",
    "from llama_index.legacy.node_parser import SimpleNodeParser, SentenceWindowNodeParser\n",
    "from llama_index.legacy.postprocessor import MetadataReplacementPostProcessor, SentenceTransformerRerank\n",
    "from llama_index.legacy.schema import MetadataMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "class ExtractTextTableInfoFromPDF:\n",
    "    def __init__(self, file_path):\n",
    "        self.unique_id = str(uuid.uuid4())\n",
    "        try:\n",
    "            file = open(file_path, 'rb')\n",
    "            input_stream = file.read()\n",
    "            file.close()\n",
    "\n",
    "            # Initial setup, create credentials instance\n",
    "            credentials = ServicePrincipalCredentials(\n",
    "                client_id=os.getenv('ADOBE_SERVICES_CLIENT_ID'),\n",
    "                client_secret=os.getenv('ADOBE_SERVICES_CLIENT_SECRET')\n",
    "            )\n",
    "\n",
    "            # Creates a PDF Services instance\n",
    "            pdf_services = PDFServices(credentials=credentials)\n",
    "\n",
    "            # Creates an asset(s) from source file(s) and upload\n",
    "            input_asset = pdf_services.upload(input_stream=input_stream, mime_type=PDFServicesMediaType.PDF)\n",
    "\n",
    "            # Create parameters for the job\n",
    "            extract_pdf_params = ExtractPDFParams(\n",
    "                elements_to_extract=[ExtractElementType.TEXT, ExtractElementType.TABLES],\n",
    "            )\n",
    "\n",
    "            # Creates a new job instance\n",
    "            extract_pdf_job = ExtractPDFJob(input_asset=input_asset, extract_pdf_params=extract_pdf_params)\n",
    "\n",
    "            # Submit the job and gets the job result\n",
    "            location = pdf_services.submit(extract_pdf_job)\n",
    "            pdf_services_response = pdf_services.get_job_result(location, ExtractPDFResult)\n",
    "\n",
    "            # Get content from the resulting asset(s)\n",
    "            result_asset: CloudAsset = pdf_services_response.get_result().get_resource()\n",
    "            stream_asset: StreamAsset = pdf_services.get_content(result_asset)\n",
    "            \n",
    "            zip_bytes = io.BytesIO(stream_asset.get_input_stream())\n",
    "            with zipfile.ZipFile(zip_bytes, 'r') as zip_ref:\n",
    "                # Extract all the contents into memory\n",
    "                self.extracted_data = {name: zip_ref.read(name) for name in zip_ref.namelist()}\n",
    "                \n",
    "        except (ServiceApiException, ServiceUsageException, SdkException) as e:\n",
    "            logging.exception(f'Exception encountered while executing operation: {e}')\n",
    "    \n",
    "    # Generates a string containing a directory structure and file name for the output file using unique_id\n",
    "    @staticmethod\n",
    "    def create_output_file_path(unique_id: str) -> str:\n",
    "        os.makedirs(\"../data/Extracted_data\", exist_ok=True)\n",
    "        return f\"../data/Extracted_data/{unique_id}.zip\"\n",
    "\n",
    "    @classmethod\n",
    "    def create_with_unique_id(cls, file_path):\n",
    "        instance = cls(file_path)\n",
    "        return instance, instance.unique_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the extracted data from the extractor\n",
    "def get_extracted_data(extracted_data):\n",
    "    if 'structuredData.json' in extracted_data:\n",
    "        json_data = json.loads(extracted_data['structuredData.json'])\n",
    "    return json_data\n",
    "\n",
    "\n",
    "\n",
    "# This function converts tables to strings to be able to be processed by LLMs. \n",
    "def get_and_save_table_strings(extractor, table_output_directory):\n",
    "    \n",
    "    # Function to convert each row in a raw unprocessed table (ie index of table not decided) into a string\n",
    "    def get_raw_table_string(df):\n",
    "        table_str = \"\"\n",
    "        for i in range(2):\n",
    "            if i ==1:\n",
    "                table_str += f\"Row {i}: {df.iloc[i].values.tolist()}\"  \n",
    "            else:\n",
    "                table_str += f\"Row {i}: {df.iloc[i].values.tolist()}\\n\"\n",
    "        return table_str\n",
    "    \n",
    "    # Function to decide if header is first row or first column\n",
    "    def evaluate_table_index_llama(table_str):\n",
    "        class Header(BaseModel):\n",
    "            index: int = Field(description=\"Header of the table, 0 for first row as the header, 1 for first column as the header\")\n",
    "            \n",
    "        parser = JsonOutputParser(pydantic_object=Header)\n",
    "\n",
    "        chat = ChatGroq(temperature=0, model_name=\"llama3-8b-8192\")\n",
    "        \n",
    "        template = '''You will assist me in deciding, based on the first 2 entries of a table, whether the first row or the first colum should be the header. \n",
    "                You are to output an int, 0 or 1. Where 0 if the first row is header, and 1 if the first column is the header.\n",
    "                Follow the format instructions carefully.\n",
    "                Table:\n",
    "                {table}\n",
    "                \n",
    "                {format_instructions}\n",
    "                '''\n",
    "        prompt = PromptTemplate(\n",
    "            template=template,\n",
    "            input_variables=[\"table\"],\n",
    "            partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "        )\n",
    "        chain = prompt | chat | parser\n",
    "        return chain.invoke({\"table\": table_str})\n",
    "\n",
    "    # Tables need procecssing when extraced from BytesIO\n",
    "    def clean_table_values(x):\n",
    "        if isinstance(x, str):\n",
    "            return x.replace('_x000D_', '').strip()\n",
    "        return x\n",
    "    \n",
    "    # Code adapted from a medium blog on how to represent rows of tables in strings\n",
    "    def convert_table_to_string(df):\n",
    "        for _, row in df.iterrows():\n",
    "            row_str = \"\"\n",
    "            for col in df.columns:\n",
    "                sentences = re.split(r'(?<=\\.)\\s*', row[col])\n",
    "                row_sentence = \"\"\n",
    "                for i in range(len(sentences)):\n",
    "                    row_sentence += sentences[i] + \"\\n\"\n",
    "                row_str += f\"{col}: {row_sentence}, \"\n",
    "            formatted = row_str[:-2]\n",
    "        return formatted\n",
    "\n",
    "    os.makedirs(table_output_directory, exist_ok=True)\n",
    "    \n",
    "    # The literal extraction of the file itself\n",
    "    excel_files = {k: v for k, v in extractor.extracted_data.items() if k.endswith('.xlsx')}\n",
    "    \n",
    "    table_dataframes = {}\n",
    "\n",
    "    num_tables =0\n",
    "    for _, content in excel_files.items():\n",
    "        excel_stream = BytesIO(content)\n",
    "        df = pd.read_excel(excel_stream, header=None)\n",
    "        \n",
    "        df = df.applymap(clean_table_values)\n",
    "        # # Uncomment bottom code to ensure that Groq decides table header index \n",
    "        \n",
    "        # df_str = get_raw_table_string(df) \n",
    "        # dic = evaluate_table_index_llama(df_str)\n",
    "        # header_index = dic['index']\n",
    "        \n",
    "        # Uncomment this if u uncomment the code above\n",
    "        header_index = 1\n",
    "        \n",
    "        # If header_index is non zero\n",
    "        if header_index == 1:\n",
    "            df = pd.read_excel(excel_stream, header=None)\n",
    "            df = df.applymap(clean_table_values)\n",
    "            df = df.T\n",
    "            # Set the first row as the new header\n",
    "            new_header = df.iloc[0]  # Take the first row for the header\n",
    "            df = df[1:]  # Take the data less the header row\n",
    "            df.columns = new_header  # Set the header row as the df header\n",
    "            # Optionally, reset index if necessary\n",
    "            df.reset_index(drop=True, inplace=True)\n",
    "        else:\n",
    "            df = pd.read_excel(excel_stream, header=0)\n",
    "        \n",
    "        output_file_path = os.path.join(table_output_directory, f\"table_{num_tables}.csv\")\n",
    "\n",
    "        # Writing the table to the corresponding csv file. \n",
    "        df.to_csv(output_file_path, index=False)\n",
    "            \n",
    "        table_str = convert_table_to_string(df)\n",
    "        table_dataframes[num_tables] = table_str\n",
    "        num_tables += 1\n",
    "        \n",
    "    return table_dataframes\n",
    "\n",
    "\n",
    "\n",
    "# Function to initialise a flexible text splitter\n",
    "def initialise_text_splitter(chunk_size, chunk_overlap):\n",
    "    separator_list = [\"\\n\\n\", \"\\n\", \". \", \"!\", \"?\", \",\", \" \", \"\", \")\", \"(\"]\n",
    "    try:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=chunk_overlap,\n",
    "                length_function=len,\n",
    "                separators=separator_list)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to initialize text splitter: {e}\")\n",
    "        return None\n",
    "    return text_splitter\n",
    "\n",
    "\n",
    "\n",
    "# Function to obtain text chunks using the text splitter\n",
    "def get_text_chunks(file_path, json_data, text_splitter):\n",
    "    if 'elements' not in json_data:\n",
    "        logging.error(\"Missing 'elements' key in json_data\")\n",
    "        raise ValueError(\"Missing 'elements' key in json_data\")\n",
    "\n",
    "    if not text_splitter:\n",
    "        logging.error(\"Text splitter not initialised properly. \")\n",
    "        sys.exit()  \n",
    "        \n",
    "    file_name = file_path.split(\"/\")[-1]\n",
    "\n",
    "    # Chunks are split by pages here\n",
    "    page_text = \"\"\n",
    "    start_page = 0\n",
    "    all_chunks = []\n",
    "    \n",
    "    list_label = \"\"\n",
    "    for i in range(len(json_data['elements'])):\n",
    "        try:\n",
    "            current_page = json_data['elements'][i]['Page']\n",
    "        except KeyError:\n",
    "            logging.warning(f\"Missing 'Page' key in element at index {i}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            if current_page > start_page:\n",
    "                # Update the new page number\n",
    "                start_page = current_page               \n",
    "                # Generate the chunks for the previous page\n",
    "                separated_list = text_splitter.split_text(page_text)\n",
    "                for chunk in separated_list:\n",
    "                    if chunk not in [\". \", \".\"]:  # Simplified condition\n",
    "                        all_chunks.append({'ElementType': 'Text', 'file_name': file_name, 'Page': current_page, 'Text': chunk})\n",
    "                # Update the string of text \n",
    "                page_text = \"\"\n",
    "                list_label = \"\"\n",
    "            else:\n",
    "                if 'Text' in json_data['elements'][i]:  # Check if Text is not empty\n",
    "                    if json_data['elements'][i]['Path'].endswith(\"Lbl\") and not json_data['elements'][i]['Path'].startswith(\"//Document/Table\"):\n",
    "                        list_label = json_data['elements'][i]['Text']\n",
    "                    else:\n",
    "                        if list_label:\n",
    "                            page_text += list_label + json_data['elements'][i]['Text']\n",
    "                            list_label = \"\"  # Reset list label to empty string\n",
    "                        else:\n",
    "                            page_text += json_data['elements'][i]['Text'] + \"\\n\"\n",
    "        except KeyError as e:\n",
    "            logging.warning(f\"Key error in json_data['elements'][i] processing at index {i}: {e}\")\n",
    "    \n",
    "    \n",
    "    # Processing the last page of the text\n",
    "    if page_text:\n",
    "        separated_list = text_splitter.split_text(page_text)\n",
    "        for chunk in separated_list:\n",
    "            if chunk not in [\". \", \".\"]:\n",
    "                all_chunks.append({'ElementType': 'Text', 'file_name': file_name, 'Page': current_page, 'Text': chunk})\n",
    "\n",
    "    return all_chunks\n",
    "\n",
    "\n",
    "# Further enhancement to include the tables with the metadata so that it can be parsed to the \n",
    "    # function that takes in the tables + metadata for embeddings generation\n",
    "def get_table_strings_with_metadata(table_dataframes, json_data):\n",
    "    \n",
    "    # Function to obtain the page number of each table\n",
    "    def get_table_pages(json_data):\n",
    "        table_file_pages = {}\n",
    "        # Obtaining the table metadata\n",
    "        for i in range(len(json_data['elements'])):\n",
    "            try:\n",
    "                file_paths = json_data['elements'][i].get('filePaths')\n",
    "                if file_paths:\n",
    "                    page = json_data['elements'][i].get('Page', 'Unknown')\n",
    "                    match = re.search(r'\\d+', file_paths[0])\n",
    "                    table_index = match.group(0)\n",
    "                    table_file_pages[int(table_index)] = {\"Page\": page}\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing file paths at index {i}: {e}\")\n",
    "        return table_file_pages\n",
    "    \n",
    "    table_file_pages = get_table_pages(json_data)\n",
    "    meta_table_batch = []\n",
    "    table_dfs = []\n",
    "    for table_index,_ in table_dataframes.items():\n",
    "        dic = {}\n",
    "        dic['ElementType'] = 'Table'\n",
    "        dic['Page'] = table_file_pages[table_index]['Page']\n",
    "        dic['Table'] = table_dataframes[table_index]\n",
    "        table_dfs.append(dic)\n",
    "\n",
    "        #  Obtain metadata for sparse embeddings\n",
    "        meta_table_batch.append(f\"ElementType 'Table', Page {table_file_pages[table_index]['Page']}, {table_dataframes[table_index]}\")\n",
    "    return table_dfs, meta_table_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-02 18:38:10,933 - INFO - Started uploading asset\n",
      "2024-07-02 18:38:15,314 - INFO - Finished uploading asset\n",
      "2024-07-02 18:38:15,316 - INFO - Started submitting EXTRACT_PDF job\n",
      "2024-07-02 18:38:16,553 - INFO - Started getting job result\n",
      "2024-07-02 18:38:27,172 - INFO - Finished polling for status\n",
      "2024-07-02 18:38:27,176 - INFO - Finished getting job result\n",
      "2024-07-02 18:38:27,178 - INFO - Started getting content\n",
      "2024-07-02 18:38:27,486 - INFO - Finished getting content\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique ID: 0e4dcde5-fe65-4391-ade1-d1389e41d635\n"
     ]
    }
   ],
   "source": [
    "file_path = '../PDF/HSI1000-chapter1.pdf'\n",
    "extractor, unique_id = ExtractTextTableInfoFromPDF.create_with_unique_id(file_path)\n",
    "print(\"\\nUnique ID:\", unique_id)\n",
    "table_output_directory = f\"../data/{unique_id}\"\n",
    "extracted_data = extractor.extracted_data\n",
    "pdf_data = get_extracted_data(extracted_data)\n",
    "table_dataframes = get_and_save_table_strings(extractor, table_output_directory)\n",
    "\n",
    "# Use some form of evaluator to decide chunk size?\n",
    "text_splitter = initialise_text_splitter(300, 50)\n",
    "\n",
    "# Get out important information\n",
    "text_chunks = get_text_chunks(file_path, pdf_data, text_splitter)\n",
    "table_dfs, meta_table_batch= get_table_strings_with_metadata(table_dataframes, pdf_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ElementType': 'Text',\n",
       "  'file_name': 'HSI1000-chapter1.pdf',\n",
       "  'Page': 1,\n",
       "  'Text': '1 The Founding of Modern Science \\nIntended Learning Outcomes for Lecture 01 \\nYou should be able to do the following after this lecture.'},\n",
       " {'ElementType': 'Text',\n",
       "  'file_name': 'HSI1000-chapter1.pdf',\n",
       "  'Page': 1,\n",
       "  'Text': '(1) Describe what is science and explain the scientific method “in a nutshell”, illustrating your explanation with a straightforward example. (2) Describe the roles scientific observations play in the scientific method'},\n",
       " {'ElementType': 'Text',\n",
       "  'file_name': 'HSI1000-chapter1.pdf',\n",
       "  'Page': 1,\n",
       "  'Text': '. (3) Explain what are the main concerns that should be addressed when making scientific observations. (4) Explain why anomalous phenomena are important for science, illustrating your explanation with some examples from the scientific revolution'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The LLM sees this: \n",
      " Metadata: page=>1\n",
      "-----\n",
      "Content: 1 The Founding of Modern Science \n",
      "Intended Learning Outcomes for Lecture 01 \n",
      "You should be able to do the following after this lecture.\n",
      "--------------------------------------------------------------------------------\n",
      "The Embedding model sees this: \n",
      " Metadata: file_name=>HSI1000-chapter1.pdf::page=>1\n",
      "-----\n",
      "Content: 1 The Founding of Modern Science \n",
      "Intended Learning Outcomes for Lecture 01 \n",
      "You should be able to do the following after this lecture.\n"
     ]
    }
   ],
   "source": [
    "# Converrsion of text chunks to Documents\n",
    "\n",
    "text_documents = [Document(text=chunk['Text'],\n",
    "                           metadata={\n",
    "                            \"file_name\": chunk['file_name'],\n",
    "                            \"page\": chunk['Page']\n",
    "                            },\n",
    "                           excluded_llm_metadata_keys=[\"file_name\"],\n",
    "                           metadata_seperator=\"::\",\n",
    "                           metadata_template=\"{key}=>{value}\",\n",
    "                           text_template=\"Metadata: {metadata_str}\\n-----\\nContent: {content}\")\n",
    "\n",
    "                            for chunk in text_chunks]\n",
    "print(\n",
    "    \"The LLM sees this: \\n\",\n",
    "    text_documents[0].get_content(metadata_mode=MetadataMode.LLM),\n",
    ")\n",
    "print(\"-\"* 80)\n",
    "print(\n",
    "    \"The Embedding model sees this: \\n\",\n",
    "    text_documents[0].get_content(metadata_mode=MetadataMode.EMBED),\n",
    ")\n",
    "\n",
    "# create the sentence window node parser w/ default settings\n",
    "SW_node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=1,\n",
    "    window_metadata_key=\"window\",\n",
    "    original_text_metadata_key=\"original_text\",\n",
    ")\n",
    "\n",
    "# Create the nodes\n",
    "nodes = SW_node_parser.get_nodes_from_documents(text_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Found that its best not to rely on the .xslx parts for table extraction. heres a new method to extract the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-03 17:42:34,631 - INFO - Started uploading asset\n",
      "2024-07-03 17:42:37,758 - INFO - Finished uploading asset\n",
      "2024-07-03 17:42:37,760 - INFO - Started submitting EXTRACT_PDF job\n",
      "2024-07-03 17:42:39,007 - INFO - Started getting job result\n",
      "2024-07-03 17:42:43,921 - INFO - Finished polling for status\n",
      "2024-07-03 17:42:43,923 - INFO - Finished getting job result\n",
      "2024-07-03 17:42:43,923 - INFO - Started getting content\n",
      "2024-07-03 17:42:44,853 - INFO - Finished getting content\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique ID: 7d55c700-6935-4064-a2ca-4a3cf543c8b0\n"
     ]
    }
   ],
   "source": [
    "file_path_1 = '../PDF/Attention is all you need-table.pdf'\n",
    "file_path_2 = '../PDF/HSI1000-2col-table.pdf'\n",
    "file_path_3 = '../PDF/HSI1000-weird-value.pdf'\n",
    "extractor, unique_id = ExtractTextTableInfoFromPDF.create_with_unique_id(file_path_2)\n",
    "extracted_data = extractor.extracted_data\n",
    "pdf_data = get_extracted_data(extracted_data)\n",
    "table_elements = [el for el in pdf_data['elements'] if \"Table\" in el['Path'] and 'Text' in el and \"TR\" in el['Path']]\n",
    "\n",
    "print(\"\\nUnique ID:\", unique_id)\n",
    "table_output_directory = f\"../data/{unique_id}\"        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: [{'path': '//Document/Sect/Table/TR/TH[2]/P/StyleSpan',\n",
       "   'text': 'Oreskes (2004)35 ',\n",
       "   'Page': 0},\n",
       "  {'path': '//Document/Sect/Table/TR/TH[3]/P/StyleSpan',\n",
       "   'text': 'Cook et al. (2013)36 ',\n",
       "   'Page': 0},\n",
       "  {'path': '//Document/Sect/Table/TR[2]/TH/P',\n",
       "   'text': 'papers surveyed ',\n",
       "   'Page': 0},\n",
       "  {'path': '//Document/Sect/Table/TR[2]/TD/P', 'text': '928 ', 'Page': 0},\n",
       "  {'path': '//Document/Sect/Table/TR[2]/TD[2]/P',\n",
       "   'text': '11,944 ',\n",
       "   'Page': 0},\n",
       "  {'path': '//Document/Sect/Table/TR[3]/TH/P',\n",
       "   'text': 'percentage agreed ',\n",
       "   'Page': 0},\n",
       "  {'path': '//Document/Sect/Table/TR[3]/TD/P', 'text': '100% ', 'Page': 0},\n",
       "  {'path': '//Document/Sect/Table/TR[3]/TD[2]/P', 'text': '97.1% ', 'Page': 0},\n",
       "  {'path': '//Document/Sect/Table/TR[4]/TH/P/StyleSpan',\n",
       "   'text': 'public perception in year of study ',\n",
       "   'Page': 0},\n",
       "  {'path': '//Document/Sect/Table/TR[4]/TD/P', 'text': '64% ', 'Page': 0},\n",
       "  {'path': '//Document/Sect/Table/TR[4]/TD[2]/P', 'text': '62% ', 'Page': 0}]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to extract unique table identifiers\n",
    "def extract_unique_tables(data):\n",
    "    tables = set()\n",
    "    for item in data:\n",
    "        match = re.search(r'/Table(\\[\\d+\\])?', item['Path'])\n",
    "        if match:\n",
    "            tables.add('Table' + (match.group(1) if match.group(1) else ''))\n",
    "    return list(tables)\n",
    "\n",
    "# Extract unique table identifiers\n",
    "unique_tables = extract_unique_tables(table_elements)\n",
    "unique_tables[0] += \"/\"\n",
    "num_tables = len(unique_tables)\n",
    "\n",
    "extracted_tables = {}\n",
    "i=0\n",
    "for table_name in unique_tables:\n",
    "    table = []\n",
    "    for el in table_elements:\n",
    "        if table_name in el['Path']:\n",
    "            # ADjust this here if u need to extract more information from the table elements\n",
    "            table.append({\"path\": el['Path'], \"text\": el['Text'], \"Page\": el[\"Page\"]})\n",
    "    extracted_tables[i+1] = table\n",
    "    i += 1\n",
    "\n",
    "extracted_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_table(table, output_file_path):\n",
    "    # only need to look at the first row\n",
    "    first_row = [el for el in table if \"TR/\" in el['path']]\n",
    "    # Function to decide if table has 2 indexes\n",
    "    def is_2_index(first_row):\n",
    "        for el in first_row:\n",
    "            if \"TH/\" in el['path']:\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "    \n",
    "    # Function to decide if the table's row is the header   \n",
    "    def is_row_header(first_row):\n",
    "        for el in first_row:\n",
    "            if \"TH\" not in el['path']:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    # If the table has 2 indexes\n",
    "    if is_2_index(first_row):\n",
    "        # Function to produce table which has 2 indexes\n",
    "        def get_2_index_table(table):\n",
    "            table_rows = set([re.search(r'Table(?:\\[\\d+\\])?/TR(?:\\[\\d+\\])?', item['path']).group() for item in table if re.search(r'Table(?:\\[\\d+\\])?/TR(?:\\[\\d+\\])?', item['path'])])\n",
    "            uniq_rows = sorted([row_name.split(\"/\")[-1] for row_name in list(table_rows)])\n",
    "\n",
    "            # Get indexes from the first row:\n",
    "            row_indexes = [el['text'].strip() for el in table if \"TR/TH\" in el['path']]\n",
    "\n",
    "            data = {}\n",
    "\n",
    "            # Only look at second row onwards\n",
    "            for i in range(1,len(uniq_rows)):\n",
    "                row_name = uniq_rows[i]\n",
    "                row = [el for el in table if row_name in el['path']]\n",
    "                row_key = row[0]['text'].strip()\n",
    "                \n",
    "                unique_tds = set()\n",
    "                for item in row:\n",
    "                    path_parts = item['path'].split('/')\n",
    "                    for part in path_parts:\n",
    "                        if 'TD' in part:\n",
    "                            unique_tds.add(part)\n",
    "                # Convert the set to a list and sort it for consistent output\n",
    "                unique_tds_list = sorted(list(unique_tds))\n",
    "\n",
    "                sections_of_row = []\n",
    "                for td in unique_tds_list:\n",
    "                    td_section = \"\"\n",
    "                    for i in range(len(row)):\n",
    "                        if i ==0:\n",
    "                            td += \"/\"\n",
    "                        if td in row[i]['path']:\n",
    "                            td_section += row[i]['text'].strip()\n",
    "                            \n",
    "                    sections_of_row.append(td_section)\n",
    "                \n",
    "                data[row_key] = sections_of_row\n",
    "                \n",
    "            df = pd.DataFrame(data, index=row_indexes).T\n",
    "            df.to_csv(output_file_path)\n",
    "            return df    \n",
    "        \n",
    "        df = get_2_index_table(table)\n",
    "        return df\n",
    "    \n",
    "    # If the table only has one index\n",
    "    else:\n",
    "        # If the header for this df is the row\n",
    "        if is_row_header(first_row):\n",
    "            # print(\"this is a row indexed table\")\n",
    "            def get_row_header_table(table):\n",
    "                table_rows = set([re.search(r'Table(?:\\[\\d+\\])?/TR(?:\\[\\d+\\])?', item['path']).group() for item in table if re.search(r'Table(?:\\[\\d+\\])?/TR(?:\\[\\d+\\])?', item['path'])])\n",
    "                uniq_rows = sorted([row_name.split(\"/\")[-1] for row_name in list(table_rows)])\n",
    "\n",
    "                # Table headers, also the keys\n",
    "                headers = [el['text'].strip() for el in table if uniq_rows[0]+\"/\" in el['path']]\n",
    "                rows = []\n",
    "                for i in range(1,len(uniq_rows)):\n",
    "                    row_name = uniq_rows[i]\n",
    "                    row = [el for el in table if row_name in el['path']]\n",
    "                    # rows.append(row)\n",
    "                    \n",
    "                    unique_tds = set()\n",
    "                    for item in row:\n",
    "                        path_parts = item['path'].split('/')\n",
    "                        for part in path_parts:\n",
    "                            if 'TD' in part:\n",
    "                                unique_tds.add(part)\n",
    "                    # Convert the set to a list and sort it for consistent output\n",
    "                    unique_tds_list = sorted(list(unique_tds))\n",
    "\n",
    "                    sections_of_row = []\n",
    "                    for td in unique_tds_list:\n",
    "                        td_section = \"\"\n",
    "                        for i in range(len(row)):\n",
    "                            if i ==0:\n",
    "                                td += \"/\"\n",
    "                            if td in row[i]['path']:\n",
    "                                td_section += row[i]['text'].strip()\n",
    "                                \n",
    "                        sections_of_row.append(td_section)\n",
    "                    rows.append(sections_of_row)\n",
    "                                \n",
    "                df = pd.DataFrame(rows, columns=headers)\n",
    "                return df\n",
    "            \n",
    "            df = get_row_header_table(table)\n",
    "            df.to_csv(output_file_path, index=False)\n",
    "            return df\n",
    "                \n",
    "        # The header for this df is the column\n",
    "        else:\n",
    "            # print(\"This table is a column indexed table...\")\n",
    "            def get_column_header_table(table):\n",
    "                table_rows = set([re.search(r'Table(?:\\[\\d+\\])?/TR(?:\\[\\d+\\])?', item['path']).group() for item in table if re.search(r'Table(?:\\[\\d+\\])?/TR(?:\\[\\d+\\])?', item['path'])])\n",
    "                uniq_rows = sorted([row_name.split(\"/\")[-1] for row_name in list(table_rows)])\n",
    "\n",
    "                data = {}\n",
    "\n",
    "                for i in range(len(uniq_rows)):\n",
    "                                    row_name = uniq_rows[i]\n",
    "                                    if i == 0:\n",
    "                                        row = [el for el in table if row_name+\"/\" in el['path']]\n",
    "                                    else:\n",
    "                                        row = [el for el in table if row_name in el['path']]\n",
    "                                    \n",
    "                                    unique_tds = set()\n",
    "                                    for item in row:\n",
    "                                        path_parts = item['path'].split('/')\n",
    "                                        for part in path_parts:\n",
    "                                            if 'TD' in part:\n",
    "                                                unique_tds.add(part)\n",
    "                                    # Convert the set to a list and sort it for consistent output\n",
    "                                    unique_tds_list = sorted(list(unique_tds))\n",
    "                                    \n",
    "                                    sections_of_row = []\n",
    "                                    for td in unique_tds_list:\n",
    "                                        td_section = \"\"\n",
    "                                        for i in range(len(row)):\n",
    "                                            if i ==0:\n",
    "                                                td += \"/\"\n",
    "                                            if td in row[i]['path']:\n",
    "                                                td_section += row[i]['text'].strip()\n",
    "                                                \n",
    "                                        sections_of_row.append(td_section)\n",
    "                                                \n",
    "                                    row_key = row[0]['text'].strip()\n",
    "                                    data[row_key] = sections_of_row\n",
    "                df = pd.DataFrame.from_dict(data, orient='index').T\n",
    "                return df\n",
    "                                \n",
    "            df = get_column_header_table(table)\n",
    "            df.to_csv(output_file_path, index=False)\n",
    "            return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_output_directory = f\"../data/{unique_id}\"\n",
    "\n",
    "os.makedirs(table_output_directory, exist_ok=True)\n",
    "for table_num, table in extracted_tables.items():\n",
    "    output_file_path = os.path.join(table_output_directory, f\"table_{table_num}.csv\")\n",
    "    df = transform_table(table, output_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
