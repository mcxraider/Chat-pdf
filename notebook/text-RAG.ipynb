{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /Users/Spare/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import json\n",
    "import uuid\n",
    "import warnings\n",
    "from tqdm import trange\n",
    "from dotenv import load_dotenv\n",
    "import io\n",
    "import zipfile\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import nltk\n",
    "\n",
    "# Adobe PDF Services imports\n",
    "from adobe.pdfservices.operation.auth.service_principal_credentials import ServicePrincipalCredentials\n",
    "from adobe.pdfservices.operation.exception.exceptions import ServiceApiException, ServiceUsageException, SdkException\n",
    "from adobe.pdfservices.operation.io.cloud_asset import CloudAsset\n",
    "from adobe.pdfservices.operation.io.stream_asset import StreamAsset\n",
    "from adobe.pdfservices.operation.pdf_services import PDFServices\n",
    "from adobe.pdfservices.operation.pdf_services_media_type import PDFServicesMediaType\n",
    "from adobe.pdfservices.operation.pdfjobs.jobs.extract_pdf_job import ExtractPDFJob\n",
    "from adobe.pdfservices.operation.pdfjobs.params.extract_pdf.extract_element_type import ExtractElementType\n",
    "from adobe.pdfservices.operation.pdfjobs.params.extract_pdf.extract_pdf_params import ExtractPDFParams\n",
    "from adobe.pdfservices.operation.pdfjobs.result.extract_pdf_result import ExtractPDFResult\n",
    "\n",
    "# Pinecone and Langchain imports\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from pinecone_text.sparse import BM25Encoder\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "hf_key = os.getenv('HUGGINGFACE_API_KEY')\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "dense_embedder_api = os.getenv(\"HF_API_URL\")\n",
    "\n",
    "# Define model\n",
    "chat_model = \"llama3-8b-8192\"\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Import other necessary modules\n",
    "from llama_index.legacy import Document\n",
    "from llama_index.legacy.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.legacy.vector_stores import PineconeVectorStore\n",
    "from typing import Any, Callable, List, Optional, Sequence, TypedDict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from llama_index.legacy.bridge.pydantic import Field\n",
    "from llama_index.legacy.callbacks.base import CallbackManager\n",
    "from llama_index.legacy.embeddings.base import BaseEmbedding\n",
    "from llama_index.legacy.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.legacy.node_parser import NodeParser\n",
    "from llama_index.legacy.node_parser.interface import NodeParser\n",
    "from llama_index.legacy.node_parser.node_utils import (\n",
    "    build_nodes_from_splits,\n",
    "    default_id_func,\n",
    ")\n",
    "from llama_index.legacy.node_parser.text.utils import split_by_sentence_tokenizer\n",
    "from llama_index.legacy.schema import BaseNode, Document\n",
    "from llama_index.legacy.utils import get_tqdm_iterable\n",
    "\n",
    "DEFAULT_OG_TEXT_METADATA_KEY = \"original_text\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_file_present(folder_path, file_name):\n",
    "    # Construct the full path to the file\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    # Check if the path is a file\n",
    "    return os.path.isfile(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "class ExtractTextTableInfoFromPDF:\n",
    "    def __init__(self, file_path):\n",
    "        self.unique_id = str(uuid.uuid4())\n",
    "        try:\n",
    "            file = open(file_path, 'rb')\n",
    "            input_stream = file.read()\n",
    "            file.close()\n",
    "\n",
    "            # Initial setup, create credentials instance\n",
    "            credentials = ServicePrincipalCredentials(\n",
    "                client_id=os.getenv('ADOBE_SERVICES_CLIENT_ID'),\n",
    "                client_secret=os.getenv('ADOBE_SERVICES_CLIENT_SECRET')\n",
    "            )\n",
    "\n",
    "            # Creates a PDF Services instance\n",
    "            pdf_services = PDFServices(credentials=credentials)\n",
    "\n",
    "            # Creates an asset(s) from source file(s) and upload\n",
    "            input_asset = pdf_services.upload(input_stream=input_stream, mime_type=PDFServicesMediaType.PDF)\n",
    "\n",
    "            # Create parameters for the job\n",
    "            extract_pdf_params = ExtractPDFParams(\n",
    "                elements_to_extract=[ExtractElementType.TEXT],\n",
    "            )\n",
    "\n",
    "            # Creates a new job instance\n",
    "            extract_pdf_job = ExtractPDFJob(input_asset=input_asset, extract_pdf_params=extract_pdf_params)\n",
    "\n",
    "            # Submit the job and gets the job result\n",
    "            location = pdf_services.submit(extract_pdf_job)\n",
    "            pdf_services_response = pdf_services.get_job_result(location, ExtractPDFResult)\n",
    "\n",
    "            # Get content from the resulting asset(s)\n",
    "            result_asset: CloudAsset = pdf_services_response.get_result().get_resource()\n",
    "            stream_asset: StreamAsset = pdf_services.get_content(result_asset)\n",
    "            \n",
    "            zip_bytes = io.BytesIO(stream_asset.get_input_stream())\n",
    "            with zipfile.ZipFile(zip_bytes, 'r') as zip_ref:\n",
    "                # Extract all the contents into memory\n",
    "                self.extracted_data = {name: zip_ref.read(name) for name in zip_ref.namelist()}\n",
    "                \n",
    "        except (ServiceApiException, ServiceUsageException, SdkException) as e:\n",
    "            logging.exception(f'Exception encountered while executing operation: {e}')\n",
    "    \n",
    "    # Generates a string containing a directory structure and file name for the output file using unique_id\n",
    "    @staticmethod\n",
    "    def create_output_file_path(unique_id: str) -> str:\n",
    "        os.makedirs(\"../data/Extracted_data\", exist_ok=True)\n",
    "        return f\"../data/Extracted_data/{unique_id}.zip\"\n",
    "\n",
    "    @classmethod\n",
    "    def create_with_unique_id(cls, file_path):\n",
    "        instance = cls(file_path)\n",
    "        return instance, instance.unique_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the extracted data from the extractor\n",
    "def get_extracted_data(extracted_data):\n",
    "    if 'structuredData.json' in extracted_data:\n",
    "        json_data = json.loads(extracted_data['structuredData.json'])\n",
    "    return json_data\n",
    "\n",
    "# Function to initialise a flexible text splitter\n",
    "def initialise_text_splitter(chunk_size, chunk_overlap):\n",
    "    separator_list = [\"\\n\\n\", \"\\n\", \". \", \"!\", \"?\", \",\", \" \", \"\", \")\", \"(\"]\n",
    "    try:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=chunk_overlap,\n",
    "                length_function=len,\n",
    "                separators=separator_list)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to initialize text splitter: {e}\")\n",
    "        return None\n",
    "    return text_splitter\n",
    "\n",
    "# Function to obtain text chunks using the text splitter\n",
    "def get_text_chunks(file_path, json_data):\n",
    "    if 'elements' not in json_data:\n",
    "        logging.error(\"Missing 'elements' key in json_data\")\n",
    "        raise ValueError(\"Missing 'elements' key in json_data\")\n",
    "        \n",
    "    file_name = file_path.split(\"/\")[-1]\n",
    "\n",
    "    # Chunks are split by pages here\n",
    "    page_text = \"\"\n",
    "    start_page = 0\n",
    "    \n",
    "    all_texts = []\n",
    "    \n",
    "    list_label = \"\"\n",
    "    for i in range(len(json_data['elements'])):\n",
    "        try:\n",
    "            current_page = json_data['elements'][i]['Page']\n",
    "        except KeyError:\n",
    "            logging.warning(f\"Missing 'Page' key in element at index {i}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            if current_page > start_page:\n",
    "                # Update the new page number\n",
    "                start_page = current_page               \n",
    "         \n",
    "                all_texts.append({'ElementType': 'Text', 'file_name': file_name, 'Page': current_page, 'Text': page_text})\n",
    "                page_text = \"\"\n",
    "                list_label = \"\"\n",
    "            else:\n",
    "                if 'Text' in json_data['elements'][i]:  # Check if Text is not empty\n",
    "                    if json_data['elements'][i]['Path'].endswith(\"Lbl\") and not json_data['elements'][i]['Path'].startswith(\"//Document/Table\"):\n",
    "                        list_label = json_data['elements'][i]['Text']\n",
    "                    else:\n",
    "                        if list_label:\n",
    "                            page_text += f\"{list_label} {json_data['elements'][i]['Text']}\\n\"\n",
    "                            list_label = \"\"  # Reset list label to empty string\n",
    "                        else:\n",
    "                            page_text += f\"{json_data['elements'][i]['Text']}\\n\"\n",
    "        except KeyError as e:\n",
    "            logging.warning(f\"Key error in json_data['elements'][i] processing at index {i}: {e}\")\n",
    "    \n",
    "    \n",
    "    # Processing the last page of the text\n",
    "    if page_text:\n",
    "        all_texts.append({'ElementType': 'Text', 'file_name': file_name, 'Page': current_page, 'Text': page_text})\n",
    "\n",
    "    return all_texts\n",
    "\n",
    "# Function to derive the nodes from the text chunks\n",
    "def convert_pagetexts_to_nodes(text_chunks):\n",
    "    \n",
    "    # Function to clean up the text in each node\n",
    "    def clean_up_text(content: str) -> str:\n",
    "        \"\"\"\n",
    "        Remove unwanted characters and patterns in text input.\n",
    "        :param content: Text input.\n",
    "        :return: Cleaned version of original text input.\n",
    "        \"\"\"\n",
    "\n",
    "        # Fix hyphenated words broken by newline\n",
    "        content = re.sub(r'(\\w+)-\\n(\\w+)', r'\\1\\2', content)\n",
    "\n",
    "        # Remove specific unwanted patterns and characters\n",
    "        unwanted_patterns = [\n",
    "            \"\\\\n\", \"  —\", \"——————————\", \"—————————\", \"—————\",\n",
    "            r'\\\\u[\\dA-Fa-f]{4}', r'\\uf075', r'\\uf0b7'\n",
    "        ]\n",
    "        for pattern in unwanted_patterns:\n",
    "            content = re.sub(pattern, \"\", content)\n",
    "\n",
    "        # Fix improperly spaced hyphenated words and normalize whitespace\n",
    "        content = re.sub(r'(\\w)\\s*-\\s*(\\w)', r'\\1-\\2', content)\n",
    "        content = re.sub(r'\\s+', ' ', content)\n",
    "        return content\n",
    "    \n",
    "    # Conversion of text chunks to Documents\n",
    "    page_documents = [Document(text=chunk['Text'],\n",
    "                            metadata={\n",
    "                                \"file_name\": chunk['file_name'],\n",
    "                                \"page\": chunk['Page']\n",
    "                                },\n",
    "                            excluded_llm_metadata_keys=[\"file_name\"],\n",
    "                            metadata_seperator=\"::\",\n",
    "                            metadata_template=\"{key}=>{value}\",\n",
    "                            text_template=\"Metadata: {metadata_str}\\n-----\\nContent: {content}\")\n",
    "                            \n",
    "                            for chunk in text_chunks]\n",
    "\n",
    "    # Clean the texts in each page\n",
    "    page_nodes = []\n",
    "    for d in page_documents:\n",
    "        cleaned_text = clean_up_text(d.text)\n",
    "        d.text = cleaned_text\n",
    "        page_nodes.append(d)\n",
    "    return page_nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf(file_path):\n",
    "    extractor, unique_id = ExtractTextTableInfoFromPDF.create_with_unique_id(file_path)\n",
    "    extracted_data = extractor.extracted_data\n",
    "    pdf_data = get_extracted_data(extracted_data)\n",
    "    filename = file_path.split(\"/\")[-1]\n",
    "    \n",
    "    # Sent this information to database\n",
    "    def export_to_db(fname):\n",
    "        with open(f\"../data/{fname}.json\", \"w\", encoding=\"utf-8\") as fout:\n",
    "            json.dump(pdf_data, fout)\n",
    "            \n",
    "    export_to_db(filename)\n",
    "    return unique_id\n",
    "\n",
    "def load_pdf(file_path):\n",
    "    fname = file_path.split(\"/\")[-1]\n",
    "    \n",
    "    def load_from_db(fname):\n",
    "        with open(f\"../data/{fname}.json\", \"r\", encoding='utf-8') as fin:\n",
    "            pdf_data = json.load(fin)\n",
    "        return pdf_data\n",
    "    pdf_data = load_from_db(fname)\n",
    "    return pdf_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the semantic chunking myself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceCombination(TypedDict):\n",
    "    sentence: str\n",
    "    index: int\n",
    "    combined_sentence: str\n",
    "    combined_sentence_embedding: List[float]\n",
    "\n",
    "class SemanticSplitterNodeParser(NodeParser):\n",
    "    \"\"\"Semantic node parser.\n",
    "\n",
    "    Splits a document into Nodes, with each node being a group of semantically related sentences.\n",
    "\n",
    "    Args:\n",
    "        buffer_size (int): number of sentences to group together when evaluating semantic similarity\n",
    "        embed_model: (BaseEmbedding): embedding model to use\n",
    "        sentence_splitter (Optional[Callable]): splits text into sentences\n",
    "        include_metadata (bool): whether to include metadata in nodes\n",
    "        include_prev_next_rel (bool): whether to include prev/next relationships\n",
    "    \"\"\"\n",
    "\n",
    "    sentence_splitter: Callable[[str], List[str]] = Field(\n",
    "        default_factory=split_by_sentence_tokenizer,\n",
    "        description=\"The text splitter to use when splitting documents.\",\n",
    "        exclude=True,\n",
    "    )\n",
    "\n",
    "    embed_model: BaseEmbedding = Field(\n",
    "        description=\"The embedding model to use to for semantic comparison\",\n",
    "    )\n",
    "\n",
    "    buffer_size: int = Field(\n",
    "        default=1,\n",
    "        description=(\n",
    "            \"The number of sentences to group together when evaluating semantic similarity. \"\n",
    "            \"Set to 1 to consider each sentence individually. \"\n",
    "            \"Set to >1 to group sentences together.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    breakpoint_percentile_threshold = Field(\n",
    "        default=95,\n",
    "        description=(\n",
    "            \"The percentile of cosine dissimilarity that must be exceeded between a \"\n",
    "            \"group of sentences and the next to form a node.  The smaller this \"\n",
    "            \"number is, the more nodes will be generated\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    @classmethod\n",
    "    def class_name(cls) -> str:\n",
    "        return \"SemanticSplitterNodeParser\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_defaults(\n",
    "        cls,\n",
    "        embed_model: Optional[BaseEmbedding] = None,\n",
    "        breakpoint_percentile_threshold: Optional[int] = 95,\n",
    "        buffer_size: Optional[int] = 1,\n",
    "        sentence_splitter: Optional[Callable[[str], List[str]]] = None,\n",
    "        original_text_metadata_key: str = DEFAULT_OG_TEXT_METADATA_KEY,\n",
    "        include_metadata: bool = True,\n",
    "        include_prev_next_rel: bool = True,\n",
    "        callback_manager: Optional[CallbackManager] = None,\n",
    "        id_func: Optional[Callable[[int, Document], str]] = None,\n",
    "    ) -> \"SemanticSplitterNodeParser\":\n",
    "        callback_manager = callback_manager or CallbackManager([])\n",
    "\n",
    "        sentence_splitter = sentence_splitter or split_by_sentence_tokenizer()\n",
    "        embed_model = embed_model or OpenAIEmbedding()\n",
    "\n",
    "        id_func = id_func or default_id_func\n",
    "\n",
    "        return cls(\n",
    "            embed_model=embed_model,\n",
    "            breakpoint_percentile_threshold=breakpoint_percentile_threshold,\n",
    "            buffer_size=buffer_size,\n",
    "            sentence_splitter=sentence_splitter,\n",
    "            original_text_metadata_key=original_text_metadata_key,\n",
    "            include_metadata=include_metadata,\n",
    "            include_prev_next_rel=include_prev_next_rel,\n",
    "            callback_manager=callback_manager,\n",
    "            id_func=id_func,\n",
    "        )\n",
    "\n",
    "    def _parse_nodes(\n",
    "        self,\n",
    "        nodes: Sequence[BaseNode],\n",
    "        show_progress: bool = False,\n",
    "        **kwargs: Any,\n",
    "    ) -> List[BaseNode]:\n",
    "        \"\"\"Parse document into nodes.\"\"\"\n",
    "        all_nodes: List[BaseNode] = []\n",
    "        nodes_with_progress = get_tqdm_iterable(nodes, show_progress, \"Parsing nodes\")\n",
    "\n",
    "        for node in nodes_with_progress:\n",
    "            nodes = self.build_semantic_nodes_from_documents([node], show_progress)\n",
    "            all_nodes.extend(nodes)\n",
    "\n",
    "        return all_nodes\n",
    "\n",
    "    def build_semantic_nodes_from_documents(\n",
    "        self,\n",
    "        documents: Sequence[Document],\n",
    "        show_progress: bool = False,\n",
    "    ) -> List[BaseNode]:\n",
    "        \"\"\"Build window nodes from documents.\"\"\"\n",
    "        all_nodes: List[BaseNode] = []\n",
    "        for doc in documents:\n",
    "            text = doc.text\n",
    "            text_splits = self.sentence_splitter(text)\n",
    "\n",
    "            sentences = self._build_sentence_groups(text_splits)\n",
    "\n",
    "            combined_sentence_embeddings = self.embed_model.get_text_embedding_batch(\n",
    "                [s[\"combined_sentence\"] for s in sentences],\n",
    "                show_progress=show_progress,\n",
    "            )\n",
    "\n",
    "            for i, embedding in enumerate(combined_sentence_embeddings):\n",
    "                sentences[i][\"combined_sentence_embedding\"] = embedding\n",
    "\n",
    "            distances = self._calculate_distances_between_sentence_groups(sentences)\n",
    "\n",
    "            chunks = self._build_node_chunks(sentences, distances)\n",
    "\n",
    "            nodes = build_nodes_from_splits(\n",
    "                chunks,\n",
    "                doc,\n",
    "                id_func=self.id_func,\n",
    "            )\n",
    "\n",
    "            all_nodes.extend(nodes)\n",
    "\n",
    "        return all_nodes\n",
    "\n",
    "    def _build_sentence_groups(\n",
    "        self, text_splits: List[str]\n",
    "    ) -> List[SentenceCombination]:\n",
    "        sentences: List[SentenceCombination] = [\n",
    "            {\n",
    "                \"sentence\": x,\n",
    "                \"index\": i,\n",
    "                \"combined_sentence\": \"\",\n",
    "                \"combined_sentence_embedding\": [],\n",
    "            }\n",
    "            for i, x in enumerate(text_splits)\n",
    "        ]\n",
    "\n",
    "        # Group sentences and calculate embeddings for sentence groups\n",
    "        for i in range(len(sentences)):\n",
    "            combined_sentence = \"\"\n",
    "\n",
    "            for j in range(i - self.buffer_size, i):\n",
    "                if j >= 0:\n",
    "                    combined_sentence += sentences[j][\"sentence\"]\n",
    "\n",
    "            combined_sentence += sentences[i][\"sentence\"]\n",
    "\n",
    "            for j in range(i + 1, i + 1 + self.buffer_size):\n",
    "                if j < len(sentences):\n",
    "                    combined_sentence += sentences[j][\"sentence\"]\n",
    "\n",
    "            sentences[i][\"combined_sentence\"] = combined_sentence\n",
    "\n",
    "        return sentences\n",
    "\n",
    "    def _calculate_distances_between_sentence_groups(\n",
    "        self, sentences: List[SentenceCombination]\n",
    "    ) -> List[float]:\n",
    "        distances = []\n",
    "        for i in range(len(sentences) - 1):\n",
    "            embedding_current = sentences[i][\"combined_sentence_embedding\"]\n",
    "            embedding_next = sentences[i + 1][\"combined_sentence_embedding\"]\n",
    "\n",
    "            similarity = self.embed_model.similarity(embedding_current, embedding_next)\n",
    "\n",
    "            distance = 1 - similarity\n",
    "\n",
    "            distances.append(distance)\n",
    "\n",
    "        return distances\n",
    "\n",
    "    def _build_node_chunks(\n",
    "        self, sentences: List[SentenceCombination], distances: List[float]\n",
    "    ) -> List[str]:\n",
    "        chunks = []\n",
    "        if len(distances) > 0:\n",
    "            breakpoint_distance_threshold = np.percentile(\n",
    "                distances, self.breakpoint_percentile_threshold\n",
    "            )\n",
    "\n",
    "            indices_above_threshold = [\n",
    "                i for i, x in enumerate(distances) if x > breakpoint_distance_threshold\n",
    "            ]\n",
    "\n",
    "            # Chunk sentences into semantic groups based on percentile breakpoints\n",
    "            start_index = 0\n",
    "\n",
    "            for index in indices_above_threshold:\n",
    "                end_index = index - 1\n",
    "\n",
    "                group = sentences[start_index : end_index + 1]\n",
    "                combined_text = \"\".join([d[\"sentence\"] for d in group])\n",
    "                chunks.append(combined_text)\n",
    "\n",
    "                start_index = index\n",
    "\n",
    "            if start_index < len(sentences):\n",
    "                combined_text = \"\".join(\n",
    "                    [d[\"sentence\"] for d in sentences[start_index:]]\n",
    "                )\n",
    "                chunks.append(combined_text)\n",
    "        else:\n",
    "            # If, for some reason we didn't get any distances (i.e. very, very small documents) just\n",
    "            # treat the whole document as a single node\n",
    "            chunks = [\" \".join([s[\"sentence\"] for s in sentences])]\n",
    "\n",
    "        return chunks\n",
    "    \n",
    "class BM25Singleton:\n",
    "    _instance = None\n",
    "    @classmethod\n",
    "    def get_instance(cls, texts=None):\n",
    "        if cls._instance is None:\n",
    "            if texts is None:\n",
    "                raise ValueError(\"Initial texts required for the first initialization!\")\n",
    "            cls._instance = cls(texts)\n",
    "        return cls._instance\n",
    "\n",
    "    def __init__(self):\n",
    "        self.bm25 = BM25Encoder()\n",
    "        \n",
    "    def fit(self, texts):\n",
    "        self.bm25.fit(texts)\n",
    "\n",
    "    def encode(self, queries):\n",
    "        return self.bm25.encode_documents(queries)\n",
    "\n",
    "\n",
    "def save_bm25_instance(model_instance, model_path):\n",
    "    os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "    # Save bm25 model to use in future queries\n",
    "    with open(model_path, 'wb') as file:\n",
    "        pickle.dump(model_instance, file)\n",
    "    print(\"bm25 model saved\\n\")\n",
    "\n",
    "def load_bm25_instance(pickle_path):\n",
    "    with open(pickle_path, 'rb') as file:\n",
    "        bm25_instance = pickle.load(file)\n",
    "    print(\"bm25 model loaded\")\n",
    "    return bm25_instance\n",
    "\n",
    "# Embedding model is customisable \n",
    "def load_embedding_model(model_name='sentence-transformers/all-mpnet-base-v2'):\n",
    "    embed_model = HuggingFaceEmbedding(model_name)\n",
    "    return embed_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_semantic_nodes(embedding_model, page_documents, buffer_size=1, breakpoint_threshold=95):\n",
    "    parser = SemanticSplitterNodeParser.from_defaults(\n",
    "        embed_model = embedding_model,  \n",
    "        buffer_size = buffer_size,  \n",
    "        breakpoint_percentile_threshold = breakpoint_threshold\n",
    "    )\n",
    "\n",
    "    # Here we semantically chunk the nodes into semantically split nodes\n",
    "    semantic_nodes = parser._parse_nodes(page_documents, show_progress=False)\n",
    "\n",
    "    # proceed to embed each node so that u can upsert the lowercased text with the embeddings\n",
    "    node_texts = [node.text.lower() for node in semantic_nodes]\n",
    "    return node_texts\n",
    "\n",
    "def fit_export_bm25(node_texts, bm_25_path):\n",
    "    bm25_instance = BM25Singleton()\n",
    "    # Fit the bm25 model on lowercased node texts\n",
    "    bm25_instance.fit(texts=node_texts)\n",
    "    \n",
    "    save_bm25_instance(bm25_instance, model_path=bm_25_path)\n",
    "\n",
    "def generate_pinecone_upsert_data(embedding_model, bm25_instance, node_texts):\n",
    "    # Generate embeddings (multiprocessing used here by the function)\n",
    "    dense_embeddings = embedding_model._embed(node_texts)\n",
    "    # Generate sparse embeddings\n",
    "    sparse_embeddings = bm25_instance.encode(node_texts)\n",
    "\n",
    "    pinecone_text_upserts = []\n",
    "\n",
    "    for i in range(len(node_texts)):\n",
    "        pinecone_text_upserts.append({\n",
    "            'id' : f\"vector{i+1}\",\n",
    "            'values': dense_embeddings[i],\n",
    "            'sparse_values': sparse_embeddings[i],\n",
    "            'metadata': {'text': node_texts[i]}\n",
    "            })\n",
    "    return pinecone_text_upserts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pinecone_index(hybrid_search):\n",
    "    if index_name not in pc.list_indexes().names():\n",
    "        logging.info(\"Creating pinecone index...\")\n",
    "        pc.create_index(\n",
    "            index_name,\n",
    "            dimension=768,\n",
    "            metric=\"dotproduct\",\n",
    "            spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "        )\n",
    "    else:\n",
    "        logging.info(f\"Pinecone index with name: \\\"{index_name}\\\" already created\")\n",
    "\n",
    "def upsert_pinecone_data(file_path, pinecone_text_upserts):  \n",
    "    # Generate a new UUID for the namespace\n",
    "    namespace = file_path.split(\"/\")[-1]\n",
    "    # Upsert data into the new namespace\n",
    "    pinecone_index.upsert(vectors=pinecone_text_upserts, namespace=namespace)\n",
    "    logging.info(\"Updating pinecone index...\")\n",
    "    time.sleep(10)\n",
    "    index_status = pinecone_index.describe_index_stats()\n",
    "    time.sleep(5)\n",
    "    if index_status['namespaces'][namespace]['vector_count'] == len(pinecone_text_upserts):          \n",
    "    # if not check_upsert_success(pinecone_index, namespace):\n",
    "        logging.error(f\"Not all vectors were upserted to namespace {namespace}. Exiting...\")\n",
    "        return namespace, False\n",
    "    else:\n",
    "        logging.info(f\"All vectors uploaded successfully to namespace {namespace}\")\n",
    "        return namespace, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_vector_query(dense, sparse, alpha: float):\n",
    "    \"\"\"Hybrid score using a convex combination\n",
    "\n",
    "    alpha * dense + (1 - alpha) * sparse\n",
    "\n",
    "    Args:\n",
    "        dense: Array of floats representing\n",
    "        sparse: a dict of `indices` and `values`\n",
    "        alpha: scale between 0 and 1\n",
    "    \"\"\"\n",
    "    if alpha < 0 or alpha > 1:\n",
    "        raise ValueError(\"Alpha must be between 0 and 1\")\n",
    "    \n",
    "    hs = {\n",
    "        'indices': sparse['indices'],\n",
    "        'values':  [v * (1 - alpha) for v in sparse['values']]\n",
    "    }\n",
    "    return [v * alpha for v in dense], hs\n",
    "\n",
    "# Usage of hybrid vector normaliser\n",
    "# hdense, hsparse = modify_vector_query(dense_vector, sparse_vector, alpha=0.75)\n",
    "\n",
    "\n",
    "def retrieve_context(index, index_name, namespace, query, embedding_model, bm25_model, top_k ):\n",
    "    index_stats = pc.describe_index(index_name)\n",
    "    if index_stats['status']['ready'] and index_stats['status']['state'] == \"Ready\":\n",
    "        dense_query = embedding_model._embed(query)\n",
    "        sparse_query = bm25_model.encode(query)\n",
    "        relevant_matches = index.query( \n",
    "            namespace=namespace,\n",
    "            top_k=top_k, \n",
    "            vector=dense_query, \n",
    "            sparse_vector=sparse_query, \n",
    "            include_metadata=True\n",
    "            )\n",
    "        \n",
    "        processed_context = [{'vector': result['id'], 'text':result['metadata']['text'], 'retrieval_score':result['score']} for result in relevant_matches['matches']]\n",
    "        return processed_context\n",
    "    else:\n",
    "        logging.error(\"Pinecone index not ready for retrieval, check connection properly...\")\n",
    "\n",
    "\n",
    "# Edit this for query re writing function\n",
    "def enhance_query(query):\n",
    "    return query.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(query):\n",
    "    enhanced_query = enhance_query(query)\n",
    "    retrieved_context = retrieve_context(pinecone_index, index_name, namespace, enhanced_query, embed_model, bm25_instance, top_k=5)\n",
    "    return retrieved_context\n",
    "\n",
    "def print_context(context):\n",
    "    for con in context:\n",
    "        print(\"Retrieved context:\\n\")\n",
    "        print(con['text'] + \"\\n\")\n",
    "        print(f\"Similarity Score: {con['retrieval_score']}\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "def chatbot():\n",
    "    while True:\n",
    "        # Take user input\n",
    "        user_query = input(\"Ask me something! or type \\'exit\\' to leave!\")\n",
    "\n",
    "        # Exit the loop if the user wants to quit\n",
    "        if user_query.lower() in ['exit', 'quit', 'q']:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Process the query with your RAG system\n",
    "        response = pipeline(user_query)\n",
    "\n",
    "        # Print the response from the LLM\n",
    "        print_context(response)\n",
    "        \n",
    "        print(\"-\" * 100)\n",
    "        print(\"ASK ME THE NEXT QUESTION\")\n",
    "        print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../PDF/HSI1000-chapter1.pdf'\n",
    "filename = file_path.split(\"/\")[-1]\n",
    "# extract_pdf(file_path)\n",
    "pdf_data = load_pdf(file_path)\n",
    "page_texts = get_text_chunks(file_path, pdf_data)\n",
    "page_documents = convert_pagetexts_to_nodes(page_texts)\n",
    "\n",
    "pickle_path = f'../src/components/hybrid-rag/{filename}bm25_model.pkl'\n",
    "embed_model = load_embedding_model()\n",
    "\n",
    "# Limit the number of page_documents to embed\n",
    "node_texts = get_semantic_nodes(embed_model, page_documents[:3])\n",
    "\n",
    "if not is_file_present(folder_path = \"../src/components/hybrid-rag\", file_name = f\"{filename}bm25_model.pkl\"):\n",
    "    fit_export_bm25(node_texts, bm_25_path=pickle_path)\n",
    "    \n",
    "bm25_instance = load_bm25_instance(pickle_path=pickle_path)\n",
    "pinecone_data = generate_pinecone_upsert_data(embed_model, bm25_instance, node_texts)\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "index_name = \"rag-testing-hybrid\"\n",
    "create_pinecone_index(hybrid_search=True)\n",
    "\n",
    "# Initialize your index\n",
    "pinecone_index = pc.Index(index_name)\n",
    "\n",
    "namespace, success = upsert_pinecone_data(file_path, pinecone_data)\n",
    "if success:\n",
    "    logging.info(f\"Data successfully upserted into namespace: {namespace}\")\n",
    "else:\n",
    "    logging.error(f\"Failed to upsert data into namespace: {namespace}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved context:\n",
      "\n",
      "1.1.2 science is self-correcting you see, science is self-correcting. the vast majority of the knowledge found in today’s textbooks was all hard won, with multiple wrong explanations being proposed and then discarded until arriving at the current version. this may yet be undone if some new test of that understanding reveals it's lacking in some way. this testing and refinement of our knowledge and understanding is the nature of scientific inquiry, the main topic of this course. the best way to understand scientific inquiry is through examples and application. the topic we’ve chosen to look at to gain an understanding of scientific inquiry is perhaps the single most important problem facing our species today – that is, climate change and loss of biodiversity. it is science that offers us our best chance at figuring out how we can get out of this mess – and what all of us need to do to get there. ok, before we get into this serious problem, we need to get a good understanding of this “observe, explain, test” approach to knowledge discovery. did you know that any time you troubleshoot something you’re actually applying the scientific method? for example, let’s take a look at troubleshooting a laptop that doesn’t bootup in our next video. \n",
      "\n",
      "Similarity Score: 1.61195803\n",
      "--------------------------------------------------------------------------------\n",
      "Retrieved context:\n",
      "\n",
      "the graphics card and motherboard are working fine. the laptop’s monitor doesn’t work. a bad connection? screen broken? we have a faulty hdmi cable. 1.2.2 troubleshooting and the scientific method troubleshooting is an example of the scientific method. it is in a sense a trivial example of it – the scientific method is a lot more powerful than that. the scientific method can be used to probe and discover brand new things about nature. this, of course, relies on our explanations and then testing those explanations. if our explanation ends up not being falsified, then we have support for our explanation and the more we test our explanation the more confidence we have that our explanation is, in fact, true. this is how things are discovered using the scientific method, and this is the way we have discovered many things about the world today. all the content in science textbooks have been subjected to this procedure. this example illustrates how science is self-correcting, where our steps taken above can be summarized in the following flow chart. we also note that every time we loop through this chart, regardless of whether the test is consistent with the explanation or not, we gain new information about the world around us. \n",
      "\n",
      "Similarity Score: 1.44471037\n",
      "--------------------------------------------------------------------------------\n",
      "Retrieved context:\n",
      "\n",
      "but is this actually science? knowledge in textbooks? where did this knowledge come from anyway? most people would say that the facts, ideas, and concepts in science textbooks is true to the best of our knowledge, but how do we know it’s true? in fact, just how do we know what we currently know, at all? by answering these questions, we get closer to figuring out what science is. 1.1.1 what is science and the scientific method in a nutshell at this point, we’ll take a look at what science is from our course textbook a beginner’s guide to scientific method. right here in chapter 1, page 5, we read: “science is that activity which aims to further our understanding of why things happen as they do in the natural world. it accomplishes this goal by applications of the scientific method.” so, what exactly is the scientific method? our textbook goes on to explain, “…it is the process of observing nature, isolating a facet that is not well understood, and then proposing and testing possible explanations.” observe. explain. test the explanation. \n",
      "\n",
      "Similarity Score: 1.21140575\n",
      "--------------------------------------------------------------------------------\n",
      "Retrieved context:\n",
      "\n",
      "1.1 what is science? hi all, welcome to the first video in this series. this lecture, which is made up of several videos, is about what science is and is a cut-down bare-bones explanation of the scientific method “in a nutshell” – which we will see illustrated with a few examples. we will take a closer look at the first step in the scientific method (again illustrated with an example) and then briefly review the founding of modern science and what one could say was a direct consequence of that – the industrial revolution, and it’s here we’ll see the beginning of mankind’s dependence on fossil fuels. i hope you have had a look through the intended learning outcomes for this lecture. they are listed right before this video, so let’s get straight into addressing our first learning outcome, which is to answer the question: “what is science actually?” i bet most people think of science in terms of “subjects”, like chemistry, physics, biology, medicine, and pharmacy, just to name a few. \n",
      "\n",
      "Similarity Score: 1.18471479\n",
      "--------------------------------------------------------------------------------\n",
      "Retrieved context:\n",
      "\n",
      "1 the founding of modern science intended learning outcomes for lecture 01 you should be able to do the following after this lecture. (1) describe what is science and explain the scientific method “in a nutshell”, illustrating your explanation with a straightforward example. (2) describe the roles scientific observations play in the scientific method. (3) explain what are the main concerns that should be addressed when making scientific observations. (4) explain why anomalous phenomena are important for science, illustrating your explanation with some examples from the scientific revolution. (5) in the context of the scientific revolution, discuss the difference between an evidence-based understanding of the natural world versus one based on authority. (6) discuss the steam engine’s contribution to the industrial revolution and its impact on population growth in industrialized nations. \n",
      "\n",
      "Similarity Score: 1.07345033\n",
      "--------------------------------------------------------------------------------\n",
      "Retrieved context:\n",
      "\n",
      "but is this actually science? knowledge in textbooks? where did this knowledge come from anyway? most people would say that the facts, ideas, and concepts in science textbooks is true to the best of our knowledge, but how do we know it’s true? in fact, just how do we know what we currently know, at all? by answering these questions, we get closer to figuring out what science is. 1.1.1 what is science and the scientific method in a nutshell at this point, we’ll take a look at what science is from our course textbook a beginner’s guide to scientific method. right here in chapter 1, page 5, we read: “science is that activity which aims to further our understanding of why things happen as they do in the natural world. it accomplishes this goal by applications of the scientific method.” so, what exactly is the scientific method? our textbook goes on to explain, “…it is the process of observing nature, isolating a facet that is not well understood, and then proposing and testing possible explanations.” observe. explain. test the explanation. \n",
      "\n",
      "Similarity Score: 1.20461726\n",
      "--------------------------------------------------------------------------------\n",
      "Retrieved context:\n",
      "\n",
      "1.1 what is science? hi all, welcome to the first video in this series. this lecture, which is made up of several videos, is about what science is and is a cut-down bare-bones explanation of the scientific method “in a nutshell” – which we will see illustrated with a few examples. we will take a closer look at the first step in the scientific method (again illustrated with an example) and then briefly review the founding of modern science and what one could say was a direct consequence of that – the industrial revolution, and it’s here we’ll see the beginning of mankind’s dependence on fossil fuels. i hope you have had a look through the intended learning outcomes for this lecture. they are listed right before this video, so let’s get straight into addressing our first learning outcome, which is to answer the question: “what is science actually?” i bet most people think of science in terms of “subjects”, like chemistry, physics, biology, medicine, and pharmacy, just to name a few. \n",
      "\n",
      "Similarity Score: 1.18211079\n",
      "--------------------------------------------------------------------------------\n",
      "Retrieved context:\n",
      "\n",
      "1.1.2 science is self-correcting you see, science is self-correcting. the vast majority of the knowledge found in today’s textbooks was all hard won, with multiple wrong explanations being proposed and then discarded until arriving at the current version. this may yet be undone if some new test of that understanding reveals it's lacking in some way. this testing and refinement of our knowledge and understanding is the nature of scientific inquiry, the main topic of this course. the best way to understand scientific inquiry is through examples and application. the topic we’ve chosen to look at to gain an understanding of scientific inquiry is perhaps the single most important problem facing our species today – that is, climate change and loss of biodiversity. it is science that offers us our best chance at figuring out how we can get out of this mess – and what all of us need to do to get there. ok, before we get into this serious problem, we need to get a good understanding of this “observe, explain, test” approach to knowledge discovery. did you know that any time you troubleshoot something you’re actually applying the scientific method? for example, let’s take a look at troubleshooting a laptop that doesn’t bootup in our next video. \n",
      "\n",
      "Similarity Score: 1.156528\n",
      "--------------------------------------------------------------------------------\n",
      "Retrieved context:\n",
      "\n",
      "the graphics card and motherboard are working fine. the laptop’s monitor doesn’t work. a bad connection? screen broken? we have a faulty hdmi cable. 1.2.2 troubleshooting and the scientific method troubleshooting is an example of the scientific method. it is in a sense a trivial example of it – the scientific method is a lot more powerful than that. the scientific method can be used to probe and discover brand new things about nature. this, of course, relies on our explanations and then testing those explanations. if our explanation ends up not being falsified, then we have support for our explanation and the more we test our explanation the more confidence we have that our explanation is, in fact, true. this is how things are discovered using the scientific method, and this is the way we have discovered many things about the world today. all the content in science textbooks have been subjected to this procedure. this example illustrates how science is self-correcting, where our steps taken above can be summarized in the following flow chart. we also note that every time we loop through this chart, regardless of whether the test is consistent with the explanation or not, we gain new information about the world around us. \n",
      "\n",
      "Similarity Score: 1.10191512\n",
      "--------------------------------------------------------------------------------\n",
      "Retrieved context:\n",
      "\n",
      "1 the founding of modern science intended learning outcomes for lecture 01 you should be able to do the following after this lecture. (1) describe what is science and explain the scientific method “in a nutshell”, illustrating your explanation with a straightforward example. (2) describe the roles scientific observations play in the scientific method. (3) explain what are the main concerns that should be addressed when making scientific observations. (4) explain why anomalous phenomena are important for science, illustrating your explanation with some examples from the scientific revolution. (5) in the context of the scientific revolution, discuss the difference between an evidence-based understanding of the natural world versus one based on authority. (6) discuss the steam engine’s contribution to the industrial revolution and its impact on population growth in industrialized nations. \n",
      "\n",
      "Similarity Score: 1.07393956\n",
      "--------------------------------------------------------------------------------\n",
      "Retrieved context:\n",
      "\n",
      "but is this actually science? knowledge in textbooks? where did this knowledge come from anyway? most people would say that the facts, ideas, and concepts in science textbooks is true to the best of our knowledge, but how do we know it’s true? in fact, just how do we know what we currently know, at all? by answering these questions, we get closer to figuring out what science is. 1.1.1 what is science and the scientific method in a nutshell at this point, we’ll take a look at what science is from our course textbook a beginner’s guide to scientific method. right here in chapter 1, page 5, we read: “science is that activity which aims to further our understanding of why things happen as they do in the natural world. it accomplishes this goal by applications of the scientific method.” so, what exactly is the scientific method? our textbook goes on to explain, “…it is the process of observing nature, isolating a facet that is not well understood, and then proposing and testing possible explanations.” observe. explain. test the explanation. \n",
      "\n",
      "Similarity Score: 1.84774446\n",
      "--------------------------------------------------------------------------------\n",
      "Retrieved context:\n",
      "\n",
      "the graphics card and motherboard are working fine. the laptop’s monitor doesn’t work. a bad connection? screen broken? we have a faulty hdmi cable. 1.2.2 troubleshooting and the scientific method troubleshooting is an example of the scientific method. it is in a sense a trivial example of it – the scientific method is a lot more powerful than that. the scientific method can be used to probe and discover brand new things about nature. this, of course, relies on our explanations and then testing those explanations. if our explanation ends up not being falsified, then we have support for our explanation and the more we test our explanation the more confidence we have that our explanation is, in fact, true. this is how things are discovered using the scientific method, and this is the way we have discovered many things about the world today. all the content in science textbooks have been subjected to this procedure. this example illustrates how science is self-correcting, where our steps taken above can be summarized in the following flow chart. we also note that every time we loop through this chart, regardless of whether the test is consistent with the explanation or not, we gain new information about the world around us. \n",
      "\n",
      "Similarity Score: 1.78113139\n",
      "--------------------------------------------------------------------------------\n",
      "Retrieved context:\n",
      "\n",
      "1 the founding of modern science intended learning outcomes for lecture 01 you should be able to do the following after this lecture. (1) describe what is science and explain the scientific method “in a nutshell”, illustrating your explanation with a straightforward example. (2) describe the roles scientific observations play in the scientific method. (3) explain what are the main concerns that should be addressed when making scientific observations. (4) explain why anomalous phenomena are important for science, illustrating your explanation with some examples from the scientific revolution. (5) in the context of the scientific revolution, discuss the difference between an evidence-based understanding of the natural world versus one based on authority. (6) discuss the steam engine’s contribution to the industrial revolution and its impact on population growth in industrialized nations. \n",
      "\n",
      "Similarity Score: 1.65014386\n",
      "--------------------------------------------------------------------------------\n",
      "Retrieved context:\n",
      "\n",
      "1.1 what is science? hi all, welcome to the first video in this series. this lecture, which is made up of several videos, is about what science is and is a cut-down bare-bones explanation of the scientific method “in a nutshell” – which we will see illustrated with a few examples. we will take a closer look at the first step in the scientific method (again illustrated with an example) and then briefly review the founding of modern science and what one could say was a direct consequence of that – the industrial revolution, and it’s here we’ll see the beginning of mankind’s dependence on fossil fuels. i hope you have had a look through the intended learning outcomes for this lecture. they are listed right before this video, so let’s get straight into addressing our first learning outcome, which is to answer the question: “what is science actually?” i bet most people think of science in terms of “subjects”, like chemistry, physics, biology, medicine, and pharmacy, just to name a few. \n",
      "\n",
      "Similarity Score: 1.58852959\n",
      "--------------------------------------------------------------------------------\n",
      "Retrieved context:\n",
      "\n",
      "1.1.2 science is self-correcting you see, science is self-correcting. the vast majority of the knowledge found in today’s textbooks was all hard won, with multiple wrong explanations being proposed and then discarded until arriving at the current version. this may yet be undone if some new test of that understanding reveals it's lacking in some way. this testing and refinement of our knowledge and understanding is the nature of scientific inquiry, the main topic of this course. the best way to understand scientific inquiry is through examples and application. the topic we’ve chosen to look at to gain an understanding of scientific inquiry is perhaps the single most important problem facing our species today – that is, climate change and loss of biodiversity. it is science that offers us our best chance at figuring out how we can get out of this mess – and what all of us need to do to get there. ok, before we get into this serious problem, we need to get a good understanding of this “observe, explain, test” approach to knowledge discovery. did you know that any time you troubleshoot something you’re actually applying the scientific method? for example, let’s take a look at troubleshooting a laptop that doesn’t bootup in our next video. \n",
      "\n",
      "Similarity Score: 1.3412317\n",
      "--------------------------------------------------------------------------------\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# query = \"What is the scientific method used for?\"\n",
    "# query = \"Why is science self correcting?\"\n",
    "# query = \"Why is science self-correcting?\"\n",
    "\n",
    "chatbot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
