{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /Users/Spare/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import json\n",
    "import uuid\n",
    "import warnings\n",
    "from tqdm import trange\n",
    "from dotenv import load_dotenv\n",
    "import io\n",
    "import zipfile\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import nltk\n",
    "from groq import Groq\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Adobe PDF Services imports\n",
    "from adobe.pdfservices.operation.auth.service_principal_credentials import ServicePrincipalCredentials\n",
    "from adobe.pdfservices.operation.exception.exceptions import ServiceApiException, ServiceUsageException, SdkException\n",
    "from adobe.pdfservices.operation.io.cloud_asset import CloudAsset\n",
    "from adobe.pdfservices.operation.io.stream_asset import StreamAsset\n",
    "from adobe.pdfservices.operation.pdf_services import PDFServices\n",
    "from adobe.pdfservices.operation.pdf_services_media_type import PDFServicesMediaType\n",
    "from adobe.pdfservices.operation.pdfjobs.jobs.extract_pdf_job import ExtractPDFJob\n",
    "from adobe.pdfservices.operation.pdfjobs.params.extract_pdf.extract_element_type import ExtractElementType\n",
    "from adobe.pdfservices.operation.pdfjobs.params.extract_pdf.extract_pdf_params import ExtractPDFParams\n",
    "from adobe.pdfservices.operation.pdfjobs.result.extract_pdf_result import ExtractPDFResult\n",
    "\n",
    "# Pinecone and Langchain imports\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from pinecone_text.sparse import BM25Encoder\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "hf_key = os.getenv('HUGGINGFACE_API_KEY')\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "dense_embedder_api = os.getenv(\"HF_API_URL\")\n",
    "\n",
    "GROQ_API_KEY = os.environ[\"GROQ_API_KEY\"]\n",
    "CHAT_MODEL   = \"llama3-70b-8192\"\n",
    "client = Groq()\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Import other necessary modules\n",
    "from llama_index.legacy import Document\n",
    "from llama_index.legacy.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.legacy.vector_stores import PineconeVectorStore\n",
    "from typing import Any, Callable, List, Optional, Sequence, TypedDict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from llama_index.legacy.bridge.pydantic import Field\n",
    "from llama_index.legacy.callbacks.base import CallbackManager\n",
    "from llama_index.legacy.embeddings.base import BaseEmbedding\n",
    "from llama_index.legacy.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.legacy.node_parser import NodeParser\n",
    "from llama_index.legacy.node_parser.interface import NodeParser\n",
    "from llama_index.legacy.node_parser.node_utils import (\n",
    "    build_nodes_from_splits,\n",
    "    default_id_func,\n",
    ")\n",
    "from llama_index.legacy.node_parser.text.utils import split_by_sentence_tokenizer\n",
    "from llama_index.legacy.schema import BaseNode, Document\n",
    "from llama_index.legacy.utils import get_tqdm_iterable\n",
    "\n",
    "DEFAULT_OG_TEXT_METADATA_KEY = \"original_text\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_file_present(folder_path, file_name):\n",
    "    # Construct the full path to the file\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    # Check if the path is a file\n",
    "    return os.path.isfile(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "class ExtractTextTableInfoFromPDF:\n",
    "    def __init__(self, file_path):\n",
    "        self.unique_id = str(uuid.uuid4())\n",
    "        try:\n",
    "            file = open(file_path, 'rb')\n",
    "            input_stream = file.read()\n",
    "            file.close()\n",
    "\n",
    "            # Initial setup, create credentials instance\n",
    "            credentials = ServicePrincipalCredentials(\n",
    "                client_id=os.getenv('ADOBE_SERVICES_CLIENT_ID'),\n",
    "                client_secret=os.getenv('ADOBE_SERVICES_CLIENT_SECRET')\n",
    "            )\n",
    "\n",
    "            # Creates a PDF Services instance\n",
    "            pdf_services = PDFServices(credentials=credentials)\n",
    "\n",
    "            # Creates an asset(s) from source file(s) and upload\n",
    "            input_asset = pdf_services.upload(input_stream=input_stream, mime_type=PDFServicesMediaType.PDF)\n",
    "\n",
    "            # Create parameters for the job\n",
    "            extract_pdf_params = ExtractPDFParams(\n",
    "                elements_to_extract=[ExtractElementType.TEXT],\n",
    "            )\n",
    "\n",
    "            # Creates a new job instance\n",
    "            extract_pdf_job = ExtractPDFJob(input_asset=input_asset, extract_pdf_params=extract_pdf_params)\n",
    "\n",
    "            # Submit the job and gets the job result\n",
    "            location = pdf_services.submit(extract_pdf_job)\n",
    "            pdf_services_response = pdf_services.get_job_result(location, ExtractPDFResult)\n",
    "\n",
    "            # Get content from the resulting asset(s)\n",
    "            result_asset: CloudAsset = pdf_services_response.get_result().get_resource()\n",
    "            stream_asset: StreamAsset = pdf_services.get_content(result_asset)\n",
    "            \n",
    "            zip_bytes = io.BytesIO(stream_asset.get_input_stream())\n",
    "            with zipfile.ZipFile(zip_bytes, 'r') as zip_ref:\n",
    "                # Extract all the contents into memory\n",
    "                self.extracted_data = {name: zip_ref.read(name) for name in zip_ref.namelist()}\n",
    "                \n",
    "        except (ServiceApiException, ServiceUsageException, SdkException) as e:\n",
    "            logging.exception(f'Exception encountered while executing operation: {e}')\n",
    "    \n",
    "    # Generates a string containing a directory structure and file name for the output file using unique_id\n",
    "    @staticmethod\n",
    "    def create_output_file_path(unique_id: str) -> str:\n",
    "        os.makedirs(\"../data/Extracted_data\", exist_ok=True)\n",
    "        return f\"../data/Extracted_data/{unique_id}.zip\"\n",
    "\n",
    "    @classmethod\n",
    "    def create_with_unique_id(cls, file_path):\n",
    "        instance = cls(file_path)\n",
    "        return instance, instance.unique_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the extracted data from the extractor\n",
    "def get_extracted_data(extracted_data):\n",
    "    if 'structuredData.json' in extracted_data:\n",
    "        json_data = json.loads(extracted_data['structuredData.json'])\n",
    "    return json_data\n",
    "\n",
    "# Function to initialise a flexible text splitter\n",
    "def initialise_text_splitter(chunk_size, chunk_overlap):\n",
    "    separator_list = [\"\\n\\n\", \"\\n\", \". \", \"!\", \"?\", \",\", \" \", \"\", \")\", \"(\"]\n",
    "    try:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=chunk_overlap,\n",
    "                length_function=len,\n",
    "                separators=separator_list)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to initialize text splitter: {e}\")\n",
    "        return None\n",
    "    return text_splitter\n",
    "\n",
    "# Function to obtain text chunks using the text splitter\n",
    "def get_text_chunks(file_path, json_data):\n",
    "    if 'elements' not in json_data:\n",
    "        logging.error(\"Missing 'elements' key in json_data\")\n",
    "        raise ValueError(\"Missing 'elements' key in json_data\")\n",
    "        \n",
    "    file_name = file_path.split(\"/\")[-1]\n",
    "\n",
    "    # Chunks are split by pages here\n",
    "    page_text = \"\"\n",
    "    start_page = 0\n",
    "    \n",
    "    all_texts = []\n",
    "    \n",
    "    list_label = \"\"\n",
    "    for i in range(len(json_data['elements'])):\n",
    "        try:\n",
    "            current_page = json_data['elements'][i]['Page']\n",
    "        except KeyError:\n",
    "            logging.warning(f\"Missing 'Page' key in element at index {i}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            if current_page > start_page:\n",
    "                # Update the new page number\n",
    "                start_page = current_page               \n",
    "         \n",
    "                all_texts.append({'ElementType': 'Text', 'file_name': file_name, 'Page': current_page, 'Text': page_text})\n",
    "                page_text = \"\"\n",
    "                list_label = \"\"\n",
    "            else:\n",
    "                if 'Text' in json_data['elements'][i]:  # Check if Text is not empty\n",
    "                    if json_data['elements'][i]['Path'].endswith(\"Lbl\") and not json_data['elements'][i]['Path'].startswith(\"//Document/Table\"):\n",
    "                        list_label = json_data['elements'][i]['Text']\n",
    "                    else:\n",
    "                        if list_label:\n",
    "                            page_text += f\"{list_label} {json_data['elements'][i]['Text']}\\n\"\n",
    "                            list_label = \"\"  # Reset list label to empty string\n",
    "                        else:\n",
    "                            page_text += f\"{json_data['elements'][i]['Text']}\\n\"\n",
    "        except KeyError as e:\n",
    "            logging.warning(f\"Key error in json_data['elements'][i] processing at index {i}: {e}\")\n",
    "    \n",
    "    \n",
    "    # Processing the last page of the text\n",
    "    if page_text:\n",
    "        all_texts.append({'ElementType': 'Text', 'file_name': file_name, 'Page': current_page, 'Text': page_text})\n",
    "\n",
    "    return all_texts\n",
    "\n",
    "# Function to derive the nodes from the text chunks\n",
    "def convert_pagetexts_to_nodes(text_chunks):\n",
    "    \n",
    "    # Function to clean up the text in each node\n",
    "    def clean_up_text(content: str) -> str:\n",
    "        \"\"\"\n",
    "        Remove unwanted characters and patterns in text input.\n",
    "        :param content: Text input.\n",
    "        :return: Cleaned version of original text input.\n",
    "        \"\"\"\n",
    "\n",
    "        # Fix hyphenated words broken by newline\n",
    "        content = re.sub(r'(\\w+)-\\n(\\w+)', r'\\1\\2', content)\n",
    "\n",
    "        # Remove specific unwanted patterns and characters\n",
    "        unwanted_patterns = [\n",
    "            \"\\\\n\", \"  —\", \"——————————\", \"—————————\", \"—————\",\n",
    "            r'\\\\u[\\dA-Fa-f]{4}', r'\\uf075', r'\\uf0b7'\n",
    "        ]\n",
    "        for pattern in unwanted_patterns:\n",
    "            content = re.sub(pattern, \"\", content)\n",
    "\n",
    "        # Fix improperly spaced hyphenated words and normalize whitespace\n",
    "        content = re.sub(r'(\\w)\\s*-\\s*(\\w)', r'\\1-\\2', content)\n",
    "        content = re.sub(r'\\s+', ' ', content)\n",
    "        return content\n",
    "    \n",
    "    # Conversion of text chunks to Documents\n",
    "    page_documents = [Document(text=chunk['Text'],\n",
    "                            metadata={\n",
    "                                \"file_name\": chunk['file_name'],\n",
    "                                \"page\": chunk['Page']\n",
    "                                },\n",
    "                            excluded_llm_metadata_keys=[\"file_name\"],\n",
    "                            metadata_seperator=\"::\",\n",
    "                            metadata_template=\"{key}=>{value}\",\n",
    "                            text_template=\"Metadata: {metadata_str}\\n-----\\nContent: {content}\")\n",
    "                            \n",
    "                            for chunk in text_chunks]\n",
    "\n",
    "    # Clean the texts in each page\n",
    "    page_nodes = []\n",
    "    for d in page_documents:\n",
    "        cleaned_text = clean_up_text(d.text)\n",
    "        d.text = cleaned_text\n",
    "        page_nodes.append(d)\n",
    "    return page_nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf(file_path):\n",
    "    extractor, unique_id = ExtractTextTableInfoFromPDF.create_with_unique_id(file_path)\n",
    "    extracted_data = extractor.extracted_data\n",
    "    pdf_data = get_extracted_data(extracted_data)\n",
    "    filename = file_path.split(\"/\")[-1]\n",
    "    \n",
    "    # Sent this information to database\n",
    "    def export_to_db(fname):\n",
    "        with open(f\"../data/{fname}.json\", \"w\", encoding=\"utf-8\") as fout:\n",
    "            json.dump(pdf_data, fout)\n",
    "            \n",
    "    export_to_db(filename)\n",
    "    return unique_id\n",
    "\n",
    "def load_pdf(file_path):\n",
    "    fname = file_path.split(\"/\")[-1]\n",
    "    \n",
    "    def load_from_db(fname):\n",
    "        with open(f\"../data/{fname}.json\", \"r\", encoding='utf-8') as fin:\n",
    "            pdf_data = json.load(fin)\n",
    "        return pdf_data\n",
    "    pdf_data = load_from_db(fname)\n",
    "    return pdf_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the semantic chunking myself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceCombination(TypedDict):\n",
    "    sentence: str\n",
    "    index: int\n",
    "    combined_sentence: str\n",
    "    combined_sentence_embedding: List[float]\n",
    "\n",
    "class SemanticSplitterNodeParser(NodeParser):\n",
    "    \"\"\"Semantic node parser.\n",
    "\n",
    "    Splits a document into Nodes, with each node being a group of semantically related sentences.\n",
    "\n",
    "    Args:\n",
    "        buffer_size (int): number of sentences to group together when evaluating semantic similarity\n",
    "        embed_model: (BaseEmbedding): embedding model to use\n",
    "        sentence_splitter (Optional[Callable]): splits text into sentences\n",
    "        include_metadata (bool): whether to include metadata in nodes\n",
    "        include_prev_next_rel (bool): whether to include prev/next relationships\n",
    "    \"\"\"\n",
    "\n",
    "    sentence_splitter: Callable[[str], List[str]] = Field(\n",
    "        default_factory=split_by_sentence_tokenizer,\n",
    "        description=\"The text splitter to use when splitting documents.\",\n",
    "        exclude=True,\n",
    "    )\n",
    "\n",
    "    embed_model: BaseEmbedding = Field(\n",
    "        description=\"The embedding model to use to for semantic comparison\",\n",
    "    )\n",
    "\n",
    "    buffer_size: int = Field(\n",
    "        default=1,\n",
    "        description=(\n",
    "            \"The number of sentences to group together when evaluating semantic similarity. \"\n",
    "            \"Set to 1 to consider each sentence individually. \"\n",
    "            \"Set to >1 to group sentences together.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    breakpoint_percentile_threshold = Field(\n",
    "        default=95,\n",
    "        description=(\n",
    "            \"The percentile of cosine dissimilarity that must be exceeded between a \"\n",
    "            \"group of sentences and the next to form a node.  The smaller this \"\n",
    "            \"number is, the more nodes will be generated\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    @classmethod\n",
    "    def class_name(cls) -> str:\n",
    "        return \"SemanticSplitterNodeParser\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_defaults(\n",
    "        cls,\n",
    "        embed_model: Optional[BaseEmbedding] = None,\n",
    "        breakpoint_percentile_threshold: Optional[int] = 95,\n",
    "        buffer_size: Optional[int] = 1,\n",
    "        sentence_splitter: Optional[Callable[[str], List[str]]] = None,\n",
    "        original_text_metadata_key: str = DEFAULT_OG_TEXT_METADATA_KEY,\n",
    "        include_metadata: bool = True,\n",
    "        include_prev_next_rel: bool = True,\n",
    "        callback_manager: Optional[CallbackManager] = None,\n",
    "        id_func: Optional[Callable[[int, Document], str]] = None,\n",
    "    ) -> \"SemanticSplitterNodeParser\":\n",
    "        callback_manager = callback_manager or CallbackManager([])\n",
    "\n",
    "        sentence_splitter = sentence_splitter or split_by_sentence_tokenizer()\n",
    "        embed_model = embed_model or OpenAIEmbedding()\n",
    "\n",
    "        id_func = id_func or default_id_func\n",
    "\n",
    "        return cls(\n",
    "            embed_model=embed_model,\n",
    "            breakpoint_percentile_threshold=breakpoint_percentile_threshold,\n",
    "            buffer_size=buffer_size,\n",
    "            sentence_splitter=sentence_splitter,\n",
    "            original_text_metadata_key=original_text_metadata_key,\n",
    "            include_metadata=include_metadata,\n",
    "            include_prev_next_rel=include_prev_next_rel,\n",
    "            callback_manager=callback_manager,\n",
    "            id_func=id_func,\n",
    "        )\n",
    "\n",
    "    def _parse_nodes(\n",
    "        self,\n",
    "        nodes: Sequence[BaseNode],\n",
    "        show_progress: bool = False,\n",
    "        **kwargs: Any,\n",
    "    ) -> List[BaseNode]:\n",
    "        \"\"\"Parse document into nodes.\"\"\"\n",
    "        all_nodes: List[BaseNode] = []\n",
    "        nodes_with_progress = get_tqdm_iterable(nodes, show_progress, \"Parsing nodes\")\n",
    "\n",
    "        for node in nodes_with_progress:\n",
    "            nodes = self.build_semantic_nodes_from_documents([node], show_progress)\n",
    "            all_nodes.extend(nodes)\n",
    "\n",
    "        return all_nodes\n",
    "\n",
    "    def build_semantic_nodes_from_documents(\n",
    "        self,\n",
    "        documents: Sequence[Document],\n",
    "        show_progress: bool = False,\n",
    "    ) -> List[BaseNode]:\n",
    "        \"\"\"Build window nodes from documents.\"\"\"\n",
    "        all_nodes: List[BaseNode] = []\n",
    "        for doc in documents:\n",
    "            text = doc.text\n",
    "            text_splits = self.sentence_splitter(text)\n",
    "\n",
    "            sentences = self._build_sentence_groups(text_splits)\n",
    "\n",
    "            combined_sentence_embeddings = self.embed_model.get_text_embedding_batch(\n",
    "                [s[\"combined_sentence\"] for s in sentences],\n",
    "                show_progress=show_progress,\n",
    "            )\n",
    "\n",
    "            for i, embedding in enumerate(combined_sentence_embeddings):\n",
    "                sentences[i][\"combined_sentence_embedding\"] = embedding\n",
    "\n",
    "            distances = self._calculate_distances_between_sentence_groups(sentences)\n",
    "\n",
    "            chunks = self._build_node_chunks(sentences, distances)\n",
    "\n",
    "            nodes = build_nodes_from_splits(\n",
    "                chunks,\n",
    "                doc,\n",
    "                id_func=self.id_func,\n",
    "            )\n",
    "            all_nodes.extend(nodes)\n",
    "\n",
    "        return all_nodes\n",
    "\n",
    "    def _build_sentence_groups(\n",
    "        self, text_splits: List[str]\n",
    "    ) -> List[SentenceCombination]:\n",
    "        sentences: List[SentenceCombination] = [\n",
    "            {\n",
    "                \"sentence\": x,\n",
    "                \"index\": i,\n",
    "                \"combined_sentence\": \"\",\n",
    "                \"combined_sentence_embedding\": [],\n",
    "            }\n",
    "            for i, x in enumerate(text_splits)\n",
    "        ]\n",
    "\n",
    "        # Group sentences and calculate embeddings for sentence groups\n",
    "        for i in range(len(sentences)):\n",
    "            combined_sentence = \"\"\n",
    "\n",
    "            for j in range(i - self.buffer_size, i):\n",
    "                if j >= 0:\n",
    "                    combined_sentence += sentences[j][\"sentence\"]\n",
    "\n",
    "            combined_sentence += sentences[i][\"sentence\"]\n",
    "\n",
    "            for j in range(i + 1, i + 1 + self.buffer_size):\n",
    "                if j < len(sentences):\n",
    "                    combined_sentence += sentences[j][\"sentence\"]\n",
    "\n",
    "            sentences[i][\"combined_sentence\"] = combined_sentence\n",
    "\n",
    "        return sentences\n",
    "\n",
    "    def _calculate_distances_between_sentence_groups(\n",
    "        self, sentences: List[SentenceCombination]\n",
    "    ) -> List[float]:\n",
    "        distances = []\n",
    "        for i in range(len(sentences) - 1):\n",
    "            embedding_current = sentences[i][\"combined_sentence_embedding\"]\n",
    "            embedding_next = sentences[i + 1][\"combined_sentence_embedding\"]\n",
    "\n",
    "            similarity = self.embed_model.similarity(embedding_current, embedding_next)\n",
    "\n",
    "            distance = 1 - similarity\n",
    "\n",
    "            distances.append(distance)\n",
    "\n",
    "        return distances\n",
    "\n",
    "    def _build_node_chunks(\n",
    "        self, sentences: List[SentenceCombination], distances: List[float]\n",
    "    ) -> List[str]:\n",
    "        chunks = []\n",
    "        if len(distances) > 0:\n",
    "            breakpoint_distance_threshold = np.percentile(\n",
    "                distances, self.breakpoint_percentile_threshold\n",
    "            )\n",
    "\n",
    "            indices_above_threshold = [\n",
    "                i for i, x in enumerate(distances) if x > breakpoint_distance_threshold\n",
    "            ]\n",
    "\n",
    "            # Chunk sentences into semantic groups based on percentile breakpoints\n",
    "            start_index = 0\n",
    "\n",
    "            for index in indices_above_threshold:\n",
    "                end_index = index - 1\n",
    "\n",
    "                group = sentences[start_index : end_index + 1]\n",
    "                combined_text = \"\".join([d[\"sentence\"] for d in group])\n",
    "                chunks.append(combined_text)\n",
    "\n",
    "                start_index = index\n",
    "\n",
    "            if start_index < len(sentences):\n",
    "                combined_text = \"\".join(\n",
    "                    [d[\"sentence\"] for d in sentences[start_index:]]\n",
    "                )\n",
    "                chunks.append(combined_text)\n",
    "        else:\n",
    "            # If, for some reason we didn't get any distances (i.e. very, very small documents) just\n",
    "            # treat the whole document as a single node\n",
    "            chunks = [\" \".join([s[\"sentence\"] for s in sentences])]\n",
    "\n",
    "        return chunks\n",
    "    \n",
    "class BM25Singleton:\n",
    "    _instance = None\n",
    "    @classmethod\n",
    "    def get_instance(cls, texts=None):\n",
    "        if cls._instance is None:\n",
    "            if texts is None:\n",
    "                raise ValueError(\"Initial texts required for the first initialization!\")\n",
    "            cls._instance = cls(texts)\n",
    "        return cls._instance\n",
    "\n",
    "    def __init__(self):\n",
    "        self.bm25 = BM25Encoder()\n",
    "        \n",
    "    def fit(self, texts):\n",
    "        self.bm25.fit(texts)\n",
    "\n",
    "    def encode(self, queries):\n",
    "        return self.bm25.encode_documents(queries)\n",
    "\n",
    "\n",
    "def save_bm25_instance(model_instance, model_path):\n",
    "    os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "    # Save bm25 model to use in future queries\n",
    "    with open(model_path, 'wb') as file:\n",
    "        pickle.dump(model_instance, file)\n",
    "    print(\"bm25 model saved\\n\")\n",
    "\n",
    "def load_bm25_instance(pickle_path):\n",
    "    with open(pickle_path, 'rb') as file:\n",
    "        bm25_instance = pickle.load(file)\n",
    "    print(\"bm25 model loaded\")\n",
    "    return bm25_instance\n",
    "\n",
    "# Embedding model is customisable \n",
    "def load_embedding_model(model_name='sentence-transformers/all-mpnet-base-v2'):\n",
    "    embed_model = HuggingFaceEmbedding(model_name)\n",
    "    return embed_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_semantic_nodes(embedding_model, page_documents, buffer_size=1, breakpoint_threshold=85):\n",
    "    parser = SemanticSplitterNodeParser.from_defaults(\n",
    "        embed_model = embedding_model,  \n",
    "        buffer_size = buffer_size,  \n",
    "        breakpoint_percentile_threshold = breakpoint_threshold,\n",
    "        include_prev_next_rel = False\n",
    "    )\n",
    "\n",
    "    # Here we semantically chunk the nodes into semantically split nodes\n",
    "    semantic_nodes = parser._parse_nodes(page_documents, show_progress=False)\n",
    "\n",
    "    # proceed to embed each node so that u can upsert the lowercased text with the embeddings\n",
    "    node_texts = [node.text.lower() for node in semantic_nodes]\n",
    "    return node_texts\n",
    "\n",
    "def fit_export_bm25(node_texts, bm_25_path):\n",
    "    bm25_instance = BM25Singleton()\n",
    "    # Fit the bm25 model on lowercased node texts\n",
    "    bm25_instance.fit(texts=node_texts)\n",
    "    \n",
    "    save_bm25_instance(bm25_instance, model_path=bm_25_path)\n",
    "\n",
    "def create_pinecone_index(hybrid_search):\n",
    "    if index_name not in pc.list_indexes().names():\n",
    "        logging.info(\"Creating pinecone index...\")\n",
    "        pc.create_index(\n",
    "            index_name,\n",
    "            dimension=768,\n",
    "            metric=\"dotproduct\",\n",
    "            spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "        )\n",
    "    else:\n",
    "        logging.info(f\"Pinecone index with name: \\\"{index_name}\\\" already created\")\n",
    "\n",
    "def generate_pinecone_upsert_data_batch(embedding_model, bm25_instance, node_texts, batch_size=30):\n",
    "    pinecone_text_upserts = []\n",
    "\n",
    "    for batch_start in tqdm(range(0, len(node_texts), batch_size), desc=\"Processing Batches\"):\n",
    "        batch_end = min(batch_start + batch_size, len(node_texts))\n",
    "        batch_texts = node_texts[batch_start:batch_end]\n",
    "\n",
    "        # Generate embeddings for the batch\n",
    "        dense_embeddings = embedding_model._embed(batch_texts)\n",
    "        # Generate sparse embeddings for the batch\n",
    "        sparse_embeddings = bm25_instance.encode(batch_texts)\n",
    "\n",
    "        for i in range(len(batch_texts)):\n",
    "            pinecone_text_upserts.append({\n",
    "                'id': f\"vector{batch_start + i + 1}\",\n",
    "                'values': dense_embeddings[i],\n",
    "                'sparse_values': sparse_embeddings[i],\n",
    "                'metadata': {'text': batch_texts[i]}\n",
    "            })\n",
    "\n",
    "    return pinecone_text_upserts\n",
    "\n",
    "def upsert_pinecone_data(file_path, pinecone_text_upserts, batch_size=30):  \n",
    "    # Generate a new UUID for the namespace\n",
    "    namespace = file_path.split(\"/\")[-1]\n",
    "    logging.info(f\"Starting upsertion to namespace {namespace}...\")\n",
    "\n",
    "    # Upsert data in batches\n",
    "    for batch_start in tqdm(range(0, len(pinecone_text_upserts), batch_size), desc=\"Upserting Batches\"):\n",
    "        batch_end = min(batch_start + batch_size, len(pinecone_text_upserts))\n",
    "        pinecone_batch = pinecone_text_upserts[batch_start:batch_end]\n",
    "\n",
    "        pinecone_index.upsert(vectors=pinecone_batch, namespace=namespace)\n",
    "        logging.info(f\"Upserting batch {batch_start // batch_size + 1}...\")\n",
    "\n",
    "    time.sleep(10)\n",
    "    index_status = pinecone_index.describe_index_stats()\n",
    "    time.sleep(5)\n",
    "\n",
    "    if index_status['namespaces'][namespace]['vector_count'] == len(pinecone_text_upserts):\n",
    "        logging.info(f\"All vectors uploaded successfully to namespace {namespace}\")\n",
    "        return namespace, True\n",
    "    else:\n",
    "        logging.error(f\"Not all vectors were upserted to namespace {namespace}. Exiting...\")\n",
    "        return namespace, False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_vector_query(dense, sparse, alpha: float):\n",
    "    \"\"\"Hybrid score using a convex combination\n",
    "\n",
    "    alpha * dense + (1 - alpha) * sparse\n",
    "\n",
    "    Args:\n",
    "        dense: Array of floats representing\n",
    "        sparse: a dict of `indices` and `values`\n",
    "        alpha: scale between 0 and 1\n",
    "    \"\"\"\n",
    "    if alpha < 0 or alpha > 1:\n",
    "        raise ValueError(\"Alpha must be between 0 and 1\")\n",
    "    \n",
    "    hs = {\n",
    "        'indices': sparse['indices'],\n",
    "        'values':  [v * (1 - alpha) for v in sparse['values']]\n",
    "    }\n",
    "    return [v * alpha for v in dense], hs\n",
    "\n",
    "# Usage of hybrid vector normaliser\n",
    "# hdense, hsparse = modify_vector_query(dense_vector, sparse_vector, alpha=0.75)\n",
    "\n",
    "def retrieve_context(index, index_name, namespace, query, embedding_model, bm25_model, top_k ):\n",
    "    index_stats = pc.describe_index(index_name)\n",
    "    if index_stats['status']['ready'] and index_stats['status']['state'] == \"Ready\":\n",
    "        dense_query = embedding_model._embed(query)\n",
    "        sparse_query = bm25_model.encode(query)\n",
    "        relevant_matches = index.query( \n",
    "            namespace=namespace,\n",
    "            top_k=top_k, \n",
    "            vector=dense_query, \n",
    "            sparse_vector=sparse_query, \n",
    "            include_metadata=True\n",
    "            )\n",
    "        \n",
    "        processed_context = [{'vector': result['id'], 'text':result['metadata']['text'], 'retrieval_score':result['score']} for result in relevant_matches['matches']]\n",
    "        return processed_context\n",
    "    else:\n",
    "        logging.error(\"Pinecone index not ready for retrieval, check connection properly...\")\n",
    "\n",
    "# Edit this for query re writing function\n",
    "def enhance_query(query):\n",
    "    return query.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_context(context):\n",
    "    for con in context:\n",
    "        print(\"Retrieved context:\\n\")\n",
    "        print(con['text'] + \"\\n\")\n",
    "        print(f\"Similarity Score: {con['retrieval_score']}\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "def extract_context(context, top_k):\n",
    "    new_context = ''''''\n",
    "    for i in range(len(context)):\n",
    "        new_context += context[i]['text']\n",
    "        if i == top_k:\n",
    "            break   \n",
    "    return new_context\n",
    "\n",
    "\n",
    "RAG_GENERATE_ANSWER = \\\n",
    "'''\n",
    "You are an expert assistant, and your task is to answer questions strictly based on the given context. \n",
    "Do not rely on any external knowledge or information beyond what is provided in the context below and provide good quality answers.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Example return format:\n",
    "{{\"Answer\": \"This is the answer\"}}\n",
    "'''\n",
    "def extract_answer(input_string):\n",
    "    # Find the start and end indices of the JSON data within the input string\n",
    "    # Assuming the JSON data starts with '{' and ends with '}'\n",
    "    json_start = input_string.find('{')\n",
    "    json_end = input_string.rfind('}') + 1\n",
    "    \n",
    "    # If either the start or end index is not found, raise an error\n",
    "    if json_start == -1 or json_end == -1:\n",
    "        raise ValueError(\"Invalid input: No JSON data found.\")\n",
    "\n",
    "    # Extract the substring that potentially contains the JSON data\n",
    "    json_data = input_string[json_start:json_end]\n",
    "    \n",
    "    try:\n",
    "        # Attempt to convert the JSON string to a Python dictionary\n",
    "        data_dict = json.loads(json_data)\n",
    "        return data_dict\n",
    "    \n",
    "    except json.JSONDecodeError:\n",
    "        # If JSON decoding fails, search for a JSON object containing the 'questions' key\n",
    "        # Using regex to match a pattern that includes the 'questions' key\n",
    "        pattern = r'{\"Answer\":\\s*\".*?\"}'\n",
    "        match = re.search(pattern, input_string, re.DOTALL)\n",
    "\n",
    "        if match:\n",
    "            # If a match is found, extract the matched JSON string and convert it to a dictionary\n",
    "            data_json_str = match.group()\n",
    "            data_dict = json.loads(data_json_str)\n",
    "            return data_dict\n",
    "\n",
    "        # If no valid JSON is found, the function will Log an error\n",
    "        else:\n",
    "            print(input_string)\n",
    "            logging.error(\"No dictionary with '3/10' as a key found in this input string. Error by LLM\")\n",
    "            return {\"error\": \"No dictionary with '3/10' found\"}\n",
    "        \n",
    "        \n",
    "def rag_chat(question, context, rag_prompt, client):\n",
    "    # Prepare the prompt using the provided answer prompt template, text, and list of questions\n",
    "    prompt = PromptTemplate(\n",
    "        template=rag_prompt,\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    ) \n",
    "    \n",
    "    # Format the final prompt with the actual text data and question list\n",
    "    final_prompt = prompt.format(context=context, question=question)\n",
    "    # Generate the completion by interacting with the language model API\n",
    "    completion = client.chat.completions.create(\n",
    "        model=CHAT_MODEL,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": final_prompt\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,  # Control the randomness of the output (lower means less random)\n",
    "        max_tokens=1024,  # Limit the response length\n",
    "        top_p=1,  # Nucleus sampling parameter (1 means only the most likely tokens are considered)\n",
    "        stream=True,  # Enable streaming of the response chunks\n",
    "        stop=None,  # Define stopping conditions (None means no stopping condition)\n",
    "    )\n",
    "\n",
    "    # Initialize an empty string to accumulate the response content\n",
    "    answer = ''''''\n",
    "    for chunk in completion:\n",
    "        # Append each chunk of content to the answer string\n",
    "        answer += chunk.choices[0].delta.content or \"\"\n",
    "    cleaned_answer = extract_answer(answer)\n",
    "    # Return the dictionary containing the generated answers\n",
    "    return cleaned_answer\n",
    "\n",
    "def pipeline(query, rag_prompt, top_k):\n",
    "    enhanced_query = enhance_query(query)\n",
    "    retrieved_context = retrieve_context(pinecone_index, index_name, namespace, enhanced_query, embed_model, bm25_instance, top_k=5)\n",
    "    extracted_context = extract_context(retrieved_context, top_k)\n",
    "    answer = rag_chat(enhanced_query, extracted_context, rag_prompt, client)['Answer']\n",
    "    return answer\n",
    "\n",
    "def chatbot():\n",
    "    while True:\n",
    "        # Take user input\n",
    "        user_query = input(\"Ask me something! or type \\'exit\\' to leave!\")\n",
    "\n",
    "        # Exit the loop if the user wants to quit\n",
    "        if user_query.lower() in ['exit', 'quit', 'q']:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Process the query with your RAG system\n",
    "        answer = pipeline(user_query, RAG_GENERATE_ANSWER, top_k=5)\n",
    "\n",
    "        print(answer + \"\\n\")\n",
    "        print(\"-\" * 100)\n",
    "        print(\"ASK ME THE NEXT QUESTION\")\n",
    "        print(\"-\" * 100)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../PDF/HSI1000-chapter1.pdf'\n",
    "filename = file_path.split(\"/\")[-1]\n",
    "# extract_pdf(file_path)\n",
    "pdf_data = load_pdf(file_path)\n",
    "page_texts = get_text_chunks(file_path, pdf_data)\n",
    "page_documents = convert_pagetexts_to_nodes(page_texts)\n",
    "\n",
    "pickle_path = f'../src/components/hybrid-rag/{filename}bm25_model.pkl'\n",
    "embed_model = load_embedding_model()\n",
    "\n",
    "# Limit the number of page_documents to embed\n",
    "node_texts = get_semantic_nodes(embed_model, page_documents[:3])\n",
    "\n",
    "if not is_file_present(folder_path = \"../src/components/hybrid-rag\", file_name = f\"{filename}bm25_model.pkl\"):\n",
    "    fit_export_bm25(node_texts, bm_25_path=pickle_path)\n",
    "    \n",
    "bm25_instance = load_bm25_instance(pickle_path=pickle_path)\n",
    "pinecone_data = generate_pinecone_upsert_data_batch(embed_model, bm25_instance, node_texts)\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "index_name = \"rag-testing-hybrid\"\n",
    "create_pinecone_index(hybrid_search=True)\n",
    "\n",
    "# Initialize your index\n",
    "pinecone_index = pc.Index(index_name)\n",
    "\n",
    "namespace, success = upsert_pinecone_data(file_path, pinecone_data)\n",
    "if success:\n",
    "    logging.info(f\"Data successfully upserted into namespace: {namespace}\")\n",
    "else:\n",
    "    logging.error(f\"Failed to upsert data into namespace: {namespace}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-30 00:41:28,427 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Answer': 'Science is self-correcting because every time we loop through the scientific method, regardless of whether the test is consistent with the explanation or not, we gain new information about the world around us, and this testing and refinement of our knowledge and understanding is the nature of scientific inquiry.'}\n",
      "Science is self-correcting because every time we loop through the scientific method, regardless of whether the test is consistent with the explanation or not, we gain new information about the world around us, and this testing and refinement of our knowledge and understanding is the nature of scientific inquiry.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ASK ME THE NEXT QUESTION\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# query = \"What is the scientific method used for?\"\n",
    "# query = \"Why is science self correcting?\"\n",
    "# query = \"Why is science self-correcting?\"\n",
    "\n",
    "chatbot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
