{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import json\n",
    "import uuid\n",
    "import warnings\n",
    "from tqdm import trange\n",
    "from dotenv import load_dotenv\n",
    "import io\n",
    "import zipfile\n",
    "import re\n",
    "import time\n",
    "\n",
    "# Adobe PDF Services imports\n",
    "from adobe.pdfservices.operation.auth.service_principal_credentials import ServicePrincipalCredentials\n",
    "from adobe.pdfservices.operation.exception.exceptions import ServiceApiException, ServiceUsageException, SdkException\n",
    "from adobe.pdfservices.operation.io.cloud_asset import CloudAsset\n",
    "from adobe.pdfservices.operation.io.stream_asset import StreamAsset\n",
    "from adobe.pdfservices.operation.pdf_services import PDFServices\n",
    "from adobe.pdfservices.operation.pdf_services_media_type import PDFServicesMediaType\n",
    "from adobe.pdfservices.operation.pdfjobs.jobs.extract_pdf_job import ExtractPDFJob\n",
    "from adobe.pdfservices.operation.pdfjobs.params.extract_pdf.extract_element_type import ExtractElementType\n",
    "from adobe.pdfservices.operation.pdfjobs.params.extract_pdf.extract_pdf_params import ExtractPDFParams\n",
    "from adobe.pdfservices.operation.pdfjobs.result.extract_pdf_result import ExtractPDFResult\n",
    "\n",
    "# Pinecone and Langchain imports\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from pinecone_text.sparse import BM25Encoder\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "hf_key = os.getenv('HUGGINGFACE_API_KEY')\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "dense_embedder_api = os.getenv(\"HF_API_URL\")\n",
    "\n",
    "# Define model\n",
    "chat_model = \"llama3-8b-8192\"\n",
    "# Import other necessary modules\n",
    "from llama_index.legacy import Document\n",
    "from llama_index.legacy.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.legacy.vector_stores import PineconeVectorStore\n",
    "from typing import Any, Callable, List, Optional, Sequence, TypedDict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from llama_index.legacy.bridge.pydantic import Field\n",
    "from llama_index.legacy.callbacks.base import CallbackManager\n",
    "from llama_index.legacy.embeddings.base import BaseEmbedding\n",
    "from llama_index.legacy.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.legacy.node_parser import NodeParser\n",
    "from llama_index.legacy.node_parser.interface import NodeParser\n",
    "from llama_index.legacy.node_parser.node_utils import (\n",
    "    build_nodes_from_splits,\n",
    "    default_id_func,\n",
    ")\n",
    "from llama_index.legacy.node_parser.text.utils import split_by_sentence_tokenizer\n",
    "from llama_index.legacy.schema import BaseNode, Document\n",
    "from llama_index.legacy.utils import get_tqdm_iterable\n",
    "\n",
    "DEFAULT_OG_TEXT_METADATA_KEY = \"original_text\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "class ExtractTextTableInfoFromPDF:\n",
    "    def __init__(self, file_path):\n",
    "        self.unique_id = str(uuid.uuid4())\n",
    "        try:\n",
    "            file = open(file_path, 'rb')\n",
    "            input_stream = file.read()\n",
    "            file.close()\n",
    "\n",
    "            # Initial setup, create credentials instance\n",
    "            credentials = ServicePrincipalCredentials(\n",
    "                client_id=os.getenv('ADOBE_SERVICES_CLIENT_ID'),\n",
    "                client_secret=os.getenv('ADOBE_SERVICES_CLIENT_SECRET')\n",
    "            )\n",
    "\n",
    "            # Creates a PDF Services instance\n",
    "            pdf_services = PDFServices(credentials=credentials)\n",
    "\n",
    "            # Creates an asset(s) from source file(s) and upload\n",
    "            input_asset = pdf_services.upload(input_stream=input_stream, mime_type=PDFServicesMediaType.PDF)\n",
    "\n",
    "            # Create parameters for the job\n",
    "            extract_pdf_params = ExtractPDFParams(\n",
    "                elements_to_extract=[ExtractElementType.TEXT],\n",
    "            )\n",
    "\n",
    "            # Creates a new job instance\n",
    "            extract_pdf_job = ExtractPDFJob(input_asset=input_asset, extract_pdf_params=extract_pdf_params)\n",
    "\n",
    "            # Submit the job and gets the job result\n",
    "            location = pdf_services.submit(extract_pdf_job)\n",
    "            pdf_services_response = pdf_services.get_job_result(location, ExtractPDFResult)\n",
    "\n",
    "            # Get content from the resulting asset(s)\n",
    "            result_asset: CloudAsset = pdf_services_response.get_result().get_resource()\n",
    "            stream_asset: StreamAsset = pdf_services.get_content(result_asset)\n",
    "            \n",
    "            zip_bytes = io.BytesIO(stream_asset.get_input_stream())\n",
    "            with zipfile.ZipFile(zip_bytes, 'r') as zip_ref:\n",
    "                # Extract all the contents into memory\n",
    "                self.extracted_data = {name: zip_ref.read(name) for name in zip_ref.namelist()}\n",
    "                \n",
    "        except (ServiceApiException, ServiceUsageException, SdkException) as e:\n",
    "            logging.exception(f'Exception encountered while executing operation: {e}')\n",
    "    \n",
    "    # Generates a string containing a directory structure and file name for the output file using unique_id\n",
    "    @staticmethod\n",
    "    def create_output_file_path(unique_id: str) -> str:\n",
    "        os.makedirs(\"../data/Extracted_data\", exist_ok=True)\n",
    "        return f\"../data/Extracted_data/{unique_id}.zip\"\n",
    "\n",
    "    @classmethod\n",
    "    def create_with_unique_id(cls, file_path):\n",
    "        instance = cls(file_path)\n",
    "        return instance, instance.unique_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the extracted data from the extractor\n",
    "def get_extracted_data(extracted_data):\n",
    "    if 'structuredData.json' in extracted_data:\n",
    "        json_data = json.loads(extracted_data['structuredData.json'])\n",
    "    return json_data\n",
    "\n",
    "# Function to initialise a flexible text splitter\n",
    "def initialise_text_splitter(chunk_size, chunk_overlap):\n",
    "    separator_list = [\"\\n\\n\", \"\\n\", \". \", \"!\", \"?\", \",\", \" \", \"\", \")\", \"(\"]\n",
    "    try:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=chunk_overlap,\n",
    "                length_function=len,\n",
    "                separators=separator_list)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to initialize text splitter: {e}\")\n",
    "        return None\n",
    "    return text_splitter\n",
    "\n",
    "# Function to obtain text chunks using the text splitter\n",
    "def get_text_chunks(file_path, json_data):\n",
    "    if 'elements' not in json_data:\n",
    "        logging.error(\"Missing 'elements' key in json_data\")\n",
    "        raise ValueError(\"Missing 'elements' key in json_data\")\n",
    "        \n",
    "    file_name = file_path.split(\"/\")[-1]\n",
    "\n",
    "    # Chunks are split by pages here\n",
    "    page_text = \"\"\n",
    "    start_page = 0\n",
    "    \n",
    "    all_texts = []\n",
    "    \n",
    "    list_label = \"\"\n",
    "    for i in range(len(json_data['elements'])):\n",
    "        try:\n",
    "            current_page = json_data['elements'][i]['Page']\n",
    "        except KeyError:\n",
    "            logging.warning(f\"Missing 'Page' key in element at index {i}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            if current_page > start_page:\n",
    "                # Update the new page number\n",
    "                start_page = current_page               \n",
    "         \n",
    "                all_texts.append({'ElementType': 'Text', 'file_name': file_name, 'Page': current_page, 'Text': page_text})\n",
    "                page_text = \"\"\n",
    "                list_label = \"\"\n",
    "            else:\n",
    "                if 'Text' in json_data['elements'][i]:  # Check if Text is not empty\n",
    "                    if json_data['elements'][i]['Path'].endswith(\"Lbl\") and not json_data['elements'][i]['Path'].startswith(\"//Document/Table\"):\n",
    "                        list_label = json_data['elements'][i]['Text']\n",
    "                    else:\n",
    "                        if list_label:\n",
    "                            page_text += f\"{list_label} {json_data['elements'][i]['Text']}\\n\"\n",
    "                            list_label = \"\"  # Reset list label to empty string\n",
    "                        else:\n",
    "                            page_text += f\"{json_data['elements'][i]['Text']}\\n\"\n",
    "        except KeyError as e:\n",
    "            logging.warning(f\"Key error in json_data['elements'][i] processing at index {i}: {e}\")\n",
    "    \n",
    "    \n",
    "    # Processing the last page of the text\n",
    "    if page_text:\n",
    "        all_texts.append({'ElementType': 'Text', 'file_name': file_name, 'Page': current_page, 'Text': page_text})\n",
    "\n",
    "    return all_texts\n",
    "\n",
    "# Function to derive the nodes from the text chunks\n",
    "def convert_pagetexts_to_nodes(text_chunks):\n",
    "    \n",
    "    # Function to clean up the text in each node\n",
    "    def clean_up_text(content: str) -> str:\n",
    "        \"\"\"\n",
    "        Remove unwanted characters and patterns in text input.\n",
    "        :param content: Text input.\n",
    "        :return: Cleaned version of original text input.\n",
    "        \"\"\"\n",
    "\n",
    "        # Fix hyphenated words broken by newline\n",
    "        content = re.sub(r'(\\w+)-\\n(\\w+)', r'\\1\\2', content)\n",
    "\n",
    "        # Remove specific unwanted patterns and characters\n",
    "        unwanted_patterns = [\n",
    "            \"\\\\n\", \"  —\", \"——————————\", \"—————————\", \"—————\",\n",
    "            r'\\\\u[\\dA-Fa-f]{4}', r'\\uf075', r'\\uf0b7'\n",
    "        ]\n",
    "        for pattern in unwanted_patterns:\n",
    "            content = re.sub(pattern, \"\", content)\n",
    "\n",
    "        # Fix improperly spaced hyphenated words and normalize whitespace\n",
    "        content = re.sub(r'(\\w)\\s*-\\s*(\\w)', r'\\1-\\2', content)\n",
    "        content = re.sub(r'\\s+', ' ', content)\n",
    "        return content\n",
    "    \n",
    "    # Conversion of text chunks to Documents\n",
    "    page_documents = [Document(text=chunk['Text'],\n",
    "                            metadata={\n",
    "                                \"file_name\": chunk['file_name'],\n",
    "                                \"page\": chunk['Page']\n",
    "                                },\n",
    "                            excluded_llm_metadata_keys=[\"file_name\"],\n",
    "                            metadata_seperator=\"::\",\n",
    "                            metadata_template=\"{key}=>{value}\",\n",
    "                            text_template=\"Metadata: {metadata_str}\\n-----\\nContent: {content}\")\n",
    "                            \n",
    "                            for chunk in text_chunks]\n",
    "\n",
    "    # Clean the texts in each page\n",
    "    page_nodes = []\n",
    "    for d in page_documents:\n",
    "        cleaned_text = clean_up_text(d.text)\n",
    "        d.text = cleaned_text\n",
    "        page_nodes.append(d)\n",
    "    return page_nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = '../PDF/HSI1000-chapter1.pdf'\n",
    "# extractor, unique_id = ExtractTextTableInfoFromPDF.create_with_unique_id(file_path)\n",
    "# extracted_data = extractor.extracted_data\n",
    "# pdf_data = get_extracted_data(extracted_data)\n",
    "# fname = file_path.split(\"/\")[-1]\n",
    "# with open(f\"../data/{fname}.json\", \"w\", encoding=\"utf-8\") as fout:\n",
    "#     json.dump(pdf_data, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../PDF/HSI1000-chapter1.pdf'\n",
    "fname = file_path.split(\"/\")[-1]\n",
    "with open(f\"../data/{fname}.json\", \"r\", encoding='utf-8') as fin:\n",
    "    pdf_data = json.load(fin)\n",
    "\n",
    "page_texts = get_text_chunks(file_path, pdf_data)\n",
    "page_documents = convert_pagetexts_to_nodes(page_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing the semantic chunking myself\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceCombination(TypedDict):\n",
    "    sentence: str\n",
    "    index: int\n",
    "    combined_sentence: str\n",
    "    combined_sentence_embedding: List[float]\n",
    "\n",
    "class SemanticSplitterNodeParser(NodeParser):\n",
    "    \"\"\"Semantic node parser.\n",
    "\n",
    "    Splits a document into Nodes, with each node being a group of semantically related sentences.\n",
    "\n",
    "    Args:\n",
    "        buffer_size (int): number of sentences to group together when evaluating semantic similarity\n",
    "        embed_model: (BaseEmbedding): embedding model to use\n",
    "        sentence_splitter (Optional[Callable]): splits text into sentences\n",
    "        include_metadata (bool): whether to include metadata in nodes\n",
    "        include_prev_next_rel (bool): whether to include prev/next relationships\n",
    "    \"\"\"\n",
    "\n",
    "    sentence_splitter: Callable[[str], List[str]] = Field(\n",
    "        default_factory=split_by_sentence_tokenizer,\n",
    "        description=\"The text splitter to use when splitting documents.\",\n",
    "        exclude=True,\n",
    "    )\n",
    "\n",
    "    embed_model: BaseEmbedding = Field(\n",
    "        description=\"The embedding model to use to for semantic comparison\",\n",
    "    )\n",
    "\n",
    "    buffer_size: int = Field(\n",
    "        default=1,\n",
    "        description=(\n",
    "            \"The number of sentences to group together when evaluating semantic similarity. \"\n",
    "            \"Set to 1 to consider each sentence individually. \"\n",
    "            \"Set to >1 to group sentences together.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    breakpoint_percentile_threshold = Field(\n",
    "        default=95,\n",
    "        description=(\n",
    "            \"The percentile of cosine dissimilarity that must be exceeded between a \"\n",
    "            \"group of sentences and the next to form a node.  The smaller this \"\n",
    "            \"number is, the more nodes will be generated\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    @classmethod\n",
    "    def class_name(cls) -> str:\n",
    "        return \"SemanticSplitterNodeParser\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_defaults(\n",
    "        cls,\n",
    "        embed_model: Optional[BaseEmbedding] = None,\n",
    "        breakpoint_percentile_threshold: Optional[int] = 95,\n",
    "        buffer_size: Optional[int] = 1,\n",
    "        sentence_splitter: Optional[Callable[[str], List[str]]] = None,\n",
    "        original_text_metadata_key: str = DEFAULT_OG_TEXT_METADATA_KEY,\n",
    "        include_metadata: bool = True,\n",
    "        include_prev_next_rel: bool = True,\n",
    "        callback_manager: Optional[CallbackManager] = None,\n",
    "        id_func: Optional[Callable[[int, Document], str]] = None,\n",
    "    ) -> \"SemanticSplitterNodeParser\":\n",
    "        callback_manager = callback_manager or CallbackManager([])\n",
    "\n",
    "        sentence_splitter = sentence_splitter or split_by_sentence_tokenizer()\n",
    "        embed_model = embed_model or OpenAIEmbedding()\n",
    "\n",
    "        id_func = id_func or default_id_func\n",
    "\n",
    "        return cls(\n",
    "            embed_model=embed_model,\n",
    "            breakpoint_percentile_threshold=breakpoint_percentile_threshold,\n",
    "            buffer_size=buffer_size,\n",
    "            sentence_splitter=sentence_splitter,\n",
    "            original_text_metadata_key=original_text_metadata_key,\n",
    "            include_metadata=include_metadata,\n",
    "            include_prev_next_rel=include_prev_next_rel,\n",
    "            callback_manager=callback_manager,\n",
    "            id_func=id_func,\n",
    "        )\n",
    "\n",
    "    def _parse_nodes(\n",
    "        self,\n",
    "        nodes: Sequence[BaseNode],\n",
    "        show_progress: bool = False,\n",
    "        **kwargs: Any,\n",
    "    ) -> List[BaseNode]:\n",
    "        \"\"\"Parse document into nodes.\"\"\"\n",
    "        all_nodes: List[BaseNode] = []\n",
    "        nodes_with_progress = get_tqdm_iterable(nodes, show_progress, \"Parsing nodes\")\n",
    "\n",
    "        for node in nodes_with_progress:\n",
    "            nodes = self.build_semantic_nodes_from_documents([node], show_progress)\n",
    "            all_nodes.extend(nodes)\n",
    "\n",
    "        return all_nodes\n",
    "\n",
    "    def build_semantic_nodes_from_documents(\n",
    "        self,\n",
    "        documents: Sequence[Document],\n",
    "        show_progress: bool = False,\n",
    "    ) -> List[BaseNode]:\n",
    "        \"\"\"Build window nodes from documents.\"\"\"\n",
    "        all_nodes: List[BaseNode] = []\n",
    "        for doc in documents:\n",
    "            text = doc.text\n",
    "            text_splits = self.sentence_splitter(text)\n",
    "\n",
    "            sentences = self._build_sentence_groups(text_splits)\n",
    "\n",
    "            combined_sentence_embeddings = self.embed_model.get_text_embedding_batch(\n",
    "                [s[\"combined_sentence\"] for s in sentences],\n",
    "                show_progress=show_progress,\n",
    "            )\n",
    "\n",
    "            for i, embedding in enumerate(combined_sentence_embeddings):\n",
    "                sentences[i][\"combined_sentence_embedding\"] = embedding\n",
    "\n",
    "            distances = self._calculate_distances_between_sentence_groups(sentences)\n",
    "\n",
    "            chunks = self._build_node_chunks(sentences, distances)\n",
    "\n",
    "            nodes = build_nodes_from_splits(\n",
    "                chunks,\n",
    "                doc,\n",
    "                id_func=self.id_func,\n",
    "            )\n",
    "\n",
    "            all_nodes.extend(nodes)\n",
    "\n",
    "        return all_nodes\n",
    "\n",
    "    def _build_sentence_groups(\n",
    "        self, text_splits: List[str]\n",
    "    ) -> List[SentenceCombination]:\n",
    "        sentences: List[SentenceCombination] = [\n",
    "            {\n",
    "                \"sentence\": x,\n",
    "                \"index\": i,\n",
    "                \"combined_sentence\": \"\",\n",
    "                \"combined_sentence_embedding\": [],\n",
    "            }\n",
    "            for i, x in enumerate(text_splits)\n",
    "        ]\n",
    "\n",
    "        # Group sentences and calculate embeddings for sentence groups\n",
    "        for i in range(len(sentences)):\n",
    "            combined_sentence = \"\"\n",
    "\n",
    "            for j in range(i - self.buffer_size, i):\n",
    "                if j >= 0:\n",
    "                    combined_sentence += sentences[j][\"sentence\"]\n",
    "\n",
    "            combined_sentence += sentences[i][\"sentence\"]\n",
    "\n",
    "            for j in range(i + 1, i + 1 + self.buffer_size):\n",
    "                if j < len(sentences):\n",
    "                    combined_sentence += sentences[j][\"sentence\"]\n",
    "\n",
    "            sentences[i][\"combined_sentence\"] = combined_sentence\n",
    "\n",
    "        return sentences\n",
    "\n",
    "    def _calculate_distances_between_sentence_groups(\n",
    "        self, sentences: List[SentenceCombination]\n",
    "    ) -> List[float]:\n",
    "        distances = []\n",
    "        for i in range(len(sentences) - 1):\n",
    "            embedding_current = sentences[i][\"combined_sentence_embedding\"]\n",
    "            embedding_next = sentences[i + 1][\"combined_sentence_embedding\"]\n",
    "\n",
    "            similarity = self.embed_model.similarity(embedding_current, embedding_next)\n",
    "\n",
    "            distance = 1 - similarity\n",
    "\n",
    "            distances.append(distance)\n",
    "\n",
    "        return distances\n",
    "\n",
    "    def _build_node_chunks(\n",
    "        self, sentences: List[SentenceCombination], distances: List[float]\n",
    "    ) -> List[str]:\n",
    "        chunks = []\n",
    "        if len(distances) > 0:\n",
    "            breakpoint_distance_threshold = np.percentile(\n",
    "                distances, self.breakpoint_percentile_threshold\n",
    "            )\n",
    "\n",
    "            indices_above_threshold = [\n",
    "                i for i, x in enumerate(distances) if x > breakpoint_distance_threshold\n",
    "            ]\n",
    "\n",
    "            # Chunk sentences into semantic groups based on percentile breakpoints\n",
    "            start_index = 0\n",
    "\n",
    "            for index in indices_above_threshold:\n",
    "                end_index = index - 1\n",
    "\n",
    "                group = sentences[start_index : end_index + 1]\n",
    "                combined_text = \"\".join([d[\"sentence\"] for d in group])\n",
    "                chunks.append(combined_text)\n",
    "\n",
    "                start_index = index\n",
    "\n",
    "            if start_index < len(sentences):\n",
    "                combined_text = \"\".join(\n",
    "                    [d[\"sentence\"] for d in sentences[start_index:]]\n",
    "                )\n",
    "                chunks.append(combined_text)\n",
    "        else:\n",
    "            # If, for some reason we didn't get any distances (i.e. very, very small documents) just\n",
    "            # treat the whole document as a single node\n",
    "            chunks = [\" \".join([s[\"sentence\"] for s in sentences])]\n",
    "\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding model is customisable \n",
    "embed_model = HuggingFaceEmbedding(model_name='sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = SemanticSplitterNodeParser.from_defaults(\n",
    "    embed_model = embed_model,  # Your embedding model here\n",
    "    buffer_size = 1,  # For example, group sentences in pairs\n",
    "    breakpoint_percentile_threshold = 95,  # Your threshold setting\n",
    ")\n",
    "# Here we semantically chunk the nodes into semantically split nodes\n",
    "semantic_nodes = parser._parse_nodes(page_documents[:3], show_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proceed to embed each node so that u can upsert the text with the embeddings\n",
    "node_texts = [node.text for node in semantic_nodes]\n",
    "# Generate embeddings (multiprocessing used here by the function)\n",
    "embeddings = embed_model._embed(node_texts)\n",
    "# match the embeddings with the semantic nodes\n",
    "for i in range(len(semantic_nodes)):\n",
    "    semantic_nodes[i].embedding = embeddings[i]\n",
    "    \n",
    "pinecone_text_upserts = []\n",
    "\n",
    "for i in range(len(semantic_nodes)):\n",
    "    pinecone_text_upserts.append({\n",
    "        'id'    : semantic_nodes[i].id_,\n",
    "        'values': semantic_nodes[i].embedding,\n",
    "        'metadata': {'text':semantic_nodes[i].text   \n",
    "        }\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 02:20:06,626 - INFO - Discovering subpackages in _NamespacePath(['/Users/Spare/Desktop/chat-pdf/venv/lib/python3.10/site-packages/pinecone_plugins'])\n",
      "2024-08-26 02:20:06,629 - INFO - Looking for plugins in pinecone_plugins.inference\n",
      "2024-08-26 02:20:06,629 - INFO - Installing plugin inference into Pinecone\n",
      "2024-08-26 02:20:07,645 - INFO - Pinecone index with name: \"rag-testing\" already created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allowing pinecone index to update...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 02:20:17,798 - INFO - All vectors uploaded successfully to namespace 63562882-96ef-43dc-946f-97cb2bb050c4\n",
      "2024-08-26 02:20:17,800 - INFO - Data successfully upserted into namespace: 63562882-96ef-43dc-946f-97cb2bb050c4\n"
     ]
    }
   ],
   "source": [
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "index_name = \"rag-testing\"\n",
    "\n",
    "# Create your index (can skip this step if your index already exists)\n",
    "hybrid_search=False\n",
    "if hybrid_search:\n",
    "    pc_similarity_metric = \"dotproduct\"\n",
    "else:\n",
    "    pc_similarity_metric = \"cosine\"\n",
    "    \n",
    "if index_name not in pc.list_indexes().names():\n",
    "    logging.info(\"Creating pinecone index...\")\n",
    "    pc.create_index(\n",
    "        index_name,\n",
    "        dimension=768,\n",
    "        metric=pc_similarity_metric,\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "else:\n",
    "    logging.info(f\"Pinecone index with name: \\\"{index_name}\\\" already created\")\n",
    "\n",
    "# Initialize your index\n",
    "pinecone_index = pc.Index(index_name)\n",
    "\n",
    "def upsert_pinecone_data(pinecone_text_upserts):  \n",
    "    # Generate a new UUID for the namespace\n",
    "    namespace = str(uuid.uuid4())\n",
    "\n",
    "    # Upsert data into the new namespace\n",
    "    pinecone_index.upsert(vectors=pinecone_text_upserts, namespace=namespace)\n",
    "    logging.info(\"Updating pinecone index...\")\n",
    "    time.sleep(5)\n",
    "    index_status = pinecone_index.describe_index_stats()\n",
    "    time.sleep(3)\n",
    "    if index_status['namespaces'][namespace]['vector_count'] == len(pinecone_text_upserts):          \n",
    "    # if not check_upsert_success(pinecone_index, namespace):\n",
    "        logging.error(f\"Not all vectors were upserted to namespace {namespace}. Exiting...\")\n",
    "        return namespace, False\n",
    "    else:\n",
    "        logging.info(f\"All vectors uploaded successfully to namespace {namespace}\")\n",
    "        return namespace, True\n",
    "\n",
    "namespace, success = upsert_pinecone_data(pinecone_text_upserts)\n",
    "if success:\n",
    "    logging.info(f\"Data successfully upserted into namespace: {namespace}\")\n",
    "else:\n",
    "    logging.error(f\"Failed to upsert data into namespace: {namespace}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 768,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'0efdb654-4c54-4b9f-af96-c75dba4e49ed': {'vector_count': 7},\n",
       "                '69f84bb0-c8d3-4abb-8161-95b4bcda2217': {'vector_count': 7},\n",
       "                'e195d0e0-47e5-46dc-9228-e62631c97c88': {'vector_count': 7}},\n",
       " 'total_vector_count': 21}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinecone_index.describe_index_stats()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
