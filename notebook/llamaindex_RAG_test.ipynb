{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import json\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "import time\n",
    "import sys\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "from dotenv import load_dotenv\n",
    "from io import BytesIO\n",
    "import io\n",
    "import zipfile\n",
    "import re\n",
    "\n",
    "# Adobe PDF Services imports\n",
    "from adobe.pdfservices.operation.auth.service_principal_credentials import ServicePrincipalCredentials\n",
    "from adobe.pdfservices.operation.exception.exceptions import ServiceApiException, ServiceUsageException, SdkException\n",
    "from adobe.pdfservices.operation.io.cloud_asset import CloudAsset\n",
    "from adobe.pdfservices.operation.io.stream_asset import StreamAsset\n",
    "from adobe.pdfservices.operation.pdf_services import PDFServices\n",
    "from adobe.pdfservices.operation.pdf_services_media_type import PDFServicesMediaType\n",
    "from adobe.pdfservices.operation.pdfjobs.jobs.extract_pdf_job import ExtractPDFJob\n",
    "from adobe.pdfservices.operation.pdfjobs.params.extract_pdf.extract_element_type import ExtractElementType\n",
    "from adobe.pdfservices.operation.pdfjobs.params.extract_pdf.extract_pdf_params import ExtractPDFParams\n",
    "from adobe.pdfservices.operation.pdfjobs.result.extract_pdf_result import ExtractPDFResult\n",
    "\n",
    "# Pinecone and Langchain imports\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from pinecone_text.sparse import BM25Encoder\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "hf_key = os.getenv('HUGGINGFACE_API_KEY')\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "dense_embedder_api = os.getenv(\"HF_API_URL\")\n",
    "\n",
    "# Define model\n",
    "chat_model = \"llama3-8b-8192\"\n",
    "# Import other necessary modules\n",
    "from llama_index.legacy import Document\n",
    "from llama_index.legacy import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.legacy.node_parser import SentenceWindowNodeParser, SemanticSplitterNodeParser\n",
    "from llama_index.legacy.schema import MetadataMode\n",
    "from llama_index.legacy.ingestion import IngestionPipeline\n",
    "from llama_index.legacy.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.legacy.vector_stores import PineconeVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "class ExtractTextTableInfoFromPDF:\n",
    "    def __init__(self, file_path):\n",
    "        self.unique_id = str(uuid.uuid4())\n",
    "        try:\n",
    "            file = open(file_path, 'rb')\n",
    "            input_stream = file.read()\n",
    "            file.close()\n",
    "\n",
    "            # Initial setup, create credentials instance\n",
    "            credentials = ServicePrincipalCredentials(\n",
    "                client_id=os.getenv('ADOBE_SERVICES_CLIENT_ID'),\n",
    "                client_secret=os.getenv('ADOBE_SERVICES_CLIENT_SECRET')\n",
    "            )\n",
    "\n",
    "            # Creates a PDF Services instance\n",
    "            pdf_services = PDFServices(credentials=credentials)\n",
    "\n",
    "            # Creates an asset(s) from source file(s) and upload\n",
    "            input_asset = pdf_services.upload(input_stream=input_stream, mime_type=PDFServicesMediaType.PDF)\n",
    "\n",
    "            # Create parameters for the job\n",
    "            extract_pdf_params = ExtractPDFParams(\n",
    "                elements_to_extract=[ExtractElementType.TEXT],\n",
    "            )\n",
    "\n",
    "            # Creates a new job instance\n",
    "            extract_pdf_job = ExtractPDFJob(input_asset=input_asset, extract_pdf_params=extract_pdf_params)\n",
    "\n",
    "            # Submit the job and gets the job result\n",
    "            location = pdf_services.submit(extract_pdf_job)\n",
    "            pdf_services_response = pdf_services.get_job_result(location, ExtractPDFResult)\n",
    "\n",
    "            # Get content from the resulting asset(s)\n",
    "            result_asset: CloudAsset = pdf_services_response.get_result().get_resource()\n",
    "            stream_asset: StreamAsset = pdf_services.get_content(result_asset)\n",
    "            \n",
    "            zip_bytes = io.BytesIO(stream_asset.get_input_stream())\n",
    "            with zipfile.ZipFile(zip_bytes, 'r') as zip_ref:\n",
    "                # Extract all the contents into memory\n",
    "                self.extracted_data = {name: zip_ref.read(name) for name in zip_ref.namelist()}\n",
    "                \n",
    "        except (ServiceApiException, ServiceUsageException, SdkException) as e:\n",
    "            logging.exception(f'Exception encountered while executing operation: {e}')\n",
    "    \n",
    "    # Generates a string containing a directory structure and file name for the output file using unique_id\n",
    "    @staticmethod\n",
    "    def create_output_file_path(unique_id: str) -> str:\n",
    "        os.makedirs(\"../data/Extracted_data\", exist_ok=True)\n",
    "        return f\"../data/Extracted_data/{unique_id}.zip\"\n",
    "\n",
    "    @classmethod\n",
    "    def create_with_unique_id(cls, file_path):\n",
    "        instance = cls(file_path)\n",
    "        return instance, instance.unique_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the extracted data from the extractor\n",
    "def get_extracted_data(extracted_data):\n",
    "    if 'structuredData.json' in extracted_data:\n",
    "        json_data = json.loads(extracted_data['structuredData.json'])\n",
    "    return json_data\n",
    "\n",
    "# Function to initialise a flexible text splitter\n",
    "def initialise_text_splitter(chunk_size, chunk_overlap):\n",
    "    separator_list = [\"\\n\\n\", \"\\n\", \". \", \"!\", \"?\", \",\", \" \", \"\", \")\", \"(\"]\n",
    "    try:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=chunk_overlap,\n",
    "                length_function=len,\n",
    "                separators=separator_list)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to initialize text splitter: {e}\")\n",
    "        return None\n",
    "    return text_splitter\n",
    "\n",
    "# Function to obtain text chunks using the text splitter\n",
    "def get_text_chunks(file_path, json_data):\n",
    "    if 'elements' not in json_data:\n",
    "        logging.error(\"Missing 'elements' key in json_data\")\n",
    "        raise ValueError(\"Missing 'elements' key in json_data\")\n",
    "        \n",
    "    file_name = file_path.split(\"/\")[-1]\n",
    "\n",
    "    # Chunks are split by pages here\n",
    "    page_text = \"\"\n",
    "    start_page = 0\n",
    "    \n",
    "    all_texts = []\n",
    "    \n",
    "    list_label = \"\"\n",
    "    for i in range(len(json_data['elements'])):\n",
    "        try:\n",
    "            current_page = json_data['elements'][i]['Page']\n",
    "        except KeyError:\n",
    "            logging.warning(f\"Missing 'Page' key in element at index {i}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            if current_page > start_page:\n",
    "                # Update the new page number\n",
    "                start_page = current_page               \n",
    "         \n",
    "                all_texts.append({'ElementType': 'Text', 'file_name': file_name, 'Page': current_page, 'Text': page_text})\n",
    "                page_text = \"\"\n",
    "                list_label = \"\"\n",
    "            else:\n",
    "                if 'Text' in json_data['elements'][i]:  # Check if Text is not empty\n",
    "                    if json_data['elements'][i]['Path'].endswith(\"Lbl\") and not json_data['elements'][i]['Path'].startswith(\"//Document/Table\"):\n",
    "                        list_label = json_data['elements'][i]['Text']\n",
    "                    else:\n",
    "                        if list_label:\n",
    "                            page_text += f\"{list_label} {json_data['elements'][i]['Text']}\\n\"\n",
    "                            list_label = \"\"  # Reset list label to empty string\n",
    "                        else:\n",
    "                            page_text += f\"{json_data['elements'][i]['Text']}\\n\"\n",
    "        except KeyError as e:\n",
    "            logging.warning(f\"Key error in json_data['elements'][i] processing at index {i}: {e}\")\n",
    "    \n",
    "    \n",
    "    # Processing the last page of the text\n",
    "    if page_text:\n",
    "        all_texts.append({'ElementType': 'Text', 'file_name': file_name, 'Page': current_page, 'Text': page_text})\n",
    "\n",
    "    return all_texts\n",
    "\n",
    "# Function to derive the nodes from the text chunks\n",
    "def convert_pagetexts_to_nodes(text_chunks):\n",
    "    \n",
    "    # Function to clean up the text in each node\n",
    "    def clean_up_text(content: str) -> str:\n",
    "        \"\"\"\n",
    "        Remove unwanted characters and patterns in text input.\n",
    "        :param content: Text input.\n",
    "        :return: Cleaned version of original text input.\n",
    "        \"\"\"\n",
    "\n",
    "        # Fix hyphenated words broken by newline\n",
    "        content = re.sub(r'(\\w+)-\\n(\\w+)', r'\\1\\2', content)\n",
    "\n",
    "        # Remove specific unwanted patterns and characters\n",
    "        unwanted_patterns = [\n",
    "            \"\\\\n\", \"  —\", \"——————————\", \"—————————\", \"—————\",\n",
    "            r'\\\\u[\\dA-Fa-f]{4}', r'\\uf075', r'\\uf0b7'\n",
    "        ]\n",
    "        for pattern in unwanted_patterns:\n",
    "            content = re.sub(pattern, \"\", content)\n",
    "\n",
    "        # Fix improperly spaced hyphenated words and normalize whitespace\n",
    "        content = re.sub(r'(\\w)\\s*-\\s*(\\w)', r'\\1-\\2', content)\n",
    "        content = re.sub(r'\\s+', ' ', content)\n",
    "        return content\n",
    "    \n",
    "    # Conversion of text chunks to Documents\n",
    "    page_documents = [Document(text=chunk['Text'],\n",
    "                            metadata={\n",
    "                                \"file_name\": chunk['file_name'],\n",
    "                                \"page\": chunk['Page']\n",
    "                                },\n",
    "                            excluded_llm_metadata_keys=[\"file_name\"],\n",
    "                            metadata_seperator=\"::\",\n",
    "                            metadata_template=\"{key}=>{value}\",\n",
    "                            text_template=\"Metadata: {metadata_str}\\n-----\\nContent: {content}\")\n",
    "                            \n",
    "                            for chunk in text_chunks]\n",
    "\n",
    "    # Clean the texts in each page\n",
    "    page_nodes = []\n",
    "    for d in page_documents:\n",
    "        cleaned_text = clean_up_text(d.text)\n",
    "        d.text = cleaned_text\n",
    "        page_nodes.append(d)\n",
    "    return page_nodes\n",
    "\n",
    "# Function to get the unique tables from all the table elements\n",
    "def extract_unique_tables(table_elements):\n",
    "    tables = set()\n",
    "    for item in table_elements:\n",
    "        match = re.search(r'/Table(\\[\\d+\\])?', item['Path'])\n",
    "        if match:\n",
    "            tables.add('Table' + (match.group(1) if match.group(1) else ''))\n",
    "    \n",
    "    unique_tables = list(tables)\n",
    "    unique_tables[0] += \"/\"\n",
    "\n",
    "    extracted_tables = {}\n",
    "    i=0\n",
    "    for table_name in unique_tables:\n",
    "        table = []\n",
    "        for el in table_elements:\n",
    "            if table_name in el['Path']:\n",
    "                # ADjust this here if u need to extract more information from the table elements\n",
    "                table.append({\"path\": el['Path'], \"text\": el['Text'], \"Page\": el[\"Page\"]})\n",
    "        extracted_tables[i+1] = table\n",
    "        i += 1\n",
    "\n",
    "    return dict(sorted(extracted_tables.items()))\n",
    "\n",
    "# Function to take in table elements from a specific table and convert it to a pandas dataframe\n",
    "def transform_table(table, output_file_path):\n",
    "    # Function to decide if table has 2 indexes\n",
    "    def is_2_index(first_row):\n",
    "        for el in first_row:\n",
    "            if \"TH/\" in el['path']:\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "    \n",
    "    # Function to decide if the table's row is the header   \n",
    "    def is_row_header(first_row):\n",
    "        for el in first_row:\n",
    "            if \"TH\" not in el['path']:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    # Function to get the table dimensions\n",
    "    def get_table_dim(table):\n",
    "        # Regular expressions to find row and column indices\n",
    "        row_regex = re.compile(r'/TR\\[(\\d+)\\]')\n",
    "        col_regex = re.compile(r'/TD\\[(\\d+)\\]')\n",
    "\n",
    "        max_row = 0\n",
    "        max_col = 0\n",
    "\n",
    "        for entry in table:\n",
    "            path = entry['path']  # Extract path from the dictionary\n",
    "            \n",
    "            # Extract row and column indices\n",
    "            row_match = row_regex.search(path)\n",
    "            col_match = col_regex.search(path)\n",
    "            \n",
    "            # Update max row index\n",
    "            if row_match:\n",
    "                row_index = int(row_match.group(1))\n",
    "                max_row = max(max_row, row_index)\n",
    "            \n",
    "            # Update max column index\n",
    "            if col_match:\n",
    "                col_index = int(col_match.group(1))\n",
    "                max_col = max(max_col, col_index)\n",
    "            elif '/TD/' in path and not col_match:  # Check for default first column\n",
    "                max_col = max(max_col, 1)  # Ensure first column is counted\n",
    "        return (max_row, max_col)    \n",
    "\n",
    "    # Function to check if the API could not identify the index of the table\n",
    "    def is_unidentified(table):\n",
    "        table_dim = get_table_dim(table)\n",
    "        min_headers = min(table_dim)\n",
    "        header_count = len([el for el in table if \"TH\" in el['path']])\n",
    "        if header_count < min_headers:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    # Function to transform the unidentified table by determining its index using LLM\n",
    "    def transform_unidentified_table(table):\n",
    "        # Function to get the first 3 rows \n",
    "        def get_first_three_rows(table):\n",
    "                # Regular expression to extract row index\n",
    "                row_regex = re.compile(r'/TR\\[(\\d+)\\]')\n",
    "                \n",
    "                # List to hold the first three rows table\n",
    "                rows = []\n",
    "                \n",
    "                for entry in table:\n",
    "                    path = entry['path']\n",
    "                    # Find row index\n",
    "                    row_match = row_regex.search(path)\n",
    "                    \n",
    "                    if row_match:\n",
    "                        row_index = int(row_match.group(1))\n",
    "                        # Check if the row index is within the first three rows\n",
    "                        if 1 <= row_index <= 3:\n",
    "                            rows.append(entry)\n",
    "                    elif '/TR/' in path:  # Check for paths that imply the first row implicitly\n",
    "                        rows.append(entry)\n",
    "            \n",
    "                table_first_3_rows = [] # This will be a 2D list\n",
    "                table_rows = set([re.search(r'Table(?:\\[\\d+\\])?/TR(?:\\[\\d+\\])?', item['path']).group() for item in table if re.search(r'Table(?:\\[\\d+\\])?/TR(?:\\[\\d+\\])?', item['path'])])\n",
    "                uniq_rows = sorted([row_name.split(\"/\")[-1] for row_name in list(table_rows)[:3]])\n",
    "                uniq_rows[0] += \"/\"\n",
    "\n",
    "\n",
    "                for i in range(len(uniq_rows)):\n",
    "                    row = [el for el in table if uniq_rows[i] in el['path']]\n",
    "                    unique_tds = set()\n",
    "                    for item in row:\n",
    "                                path_parts = item['path'].split('/')\n",
    "                                for part in path_parts:\n",
    "                                    if 'TD' in part:\n",
    "                                        unique_tds.add(part)\n",
    "                    # Convert the set to a list and sort it for consistent output\n",
    "                    unique_tds_list = sorted(list(unique_tds))\n",
    "                            \n",
    "                    sections_of_row = []\n",
    "                    for td in unique_tds_list:\n",
    "                                td_section = \"\"\n",
    "                                for i in range(len(row)):\n",
    "                                    if i ==0:\n",
    "                                        td += \"/\"\n",
    "                                    if td in row[i]['path']:\n",
    "                                        td_section += row[i]['text'].strip()\n",
    "\n",
    "                                sections_of_row.append(td_section)\n",
    "                    \n",
    "                    # print(sections_of_row)\n",
    "                    table_first_3_rows.append(sections_of_row)\n",
    "\n",
    "                # account for case where there are 2 index columns:\n",
    "                min_dim = max(list(map(lambda x: len(x), table_first_3_rows)))\n",
    "                if len(table_first_3_rows[0]) < min_dim:\n",
    "                    table_first_3_rows[0].insert(0, \"empty\")\n",
    "                return table_first_3_rows\n",
    "\n",
    "        # Function to get the first 3 rows of the table into a string to parse to the LLM \n",
    "        def get_table_check_string(table_first_3_rows):\n",
    "            table_str = ''\n",
    "            for i in range(len(table_first_3_rows)):\n",
    "                table_str += f'''Row{i+1}: {\" | \".join(table_first_3_rows[i])}\\n'''\n",
    "            return table_str\n",
    "\n",
    "        # Function to clean the output if it decides to explain its choice.\n",
    "        def clean_llama_output_if_string(input_str):\n",
    "            \"\"\"\n",
    "            Extracts a dictionary-like substring from the given input string.\n",
    "            \n",
    "            Args:\n",
    "            input_str (str): The input string containing the dictionary-like substring.\n",
    "            \n",
    "            Returns:\n",
    "            dict: The extracted dictionary as a Python dictionary.\n",
    "            \"\"\"\n",
    "            # Use a regular expression to find the dictionary-like substring\n",
    "            match = re.search(r'\\{.*?\\}', input_str)\n",
    "            \n",
    "            if match:\n",
    "                dict_str = match.group()\n",
    "                try:\n",
    "                    # Safely evaluate the dictionary string\n",
    "                    extracted_dict = eval(dict_str)\n",
    "                    if isinstance(extracted_dict, dict):\n",
    "                        return extracted_dict\n",
    "                except (SyntaxError, NameError):\n",
    "                    pass\n",
    "            \n",
    "            return None\n",
    "\n",
    "        # Function to decide the indexing of the table\n",
    "        def eval_table_index_llama(table_str):\n",
    "            class Header(BaseModel):\n",
    "                index: int = Field(description=(\n",
    "                    '''Index of the table indicating the type of indexing:\\n\\\n",
    "                        0 - No indexes\\n\\\n",
    "                        1 - Indexed by the first column\\n\\\n",
    "                        2 - Indexed by the first row\\n\\\n",
    "                        3 - Indexed by both the first row and the first column'''))\n",
    "\n",
    "            parser = JsonOutputParser(pydantic_object=Header)\n",
    "\n",
    "            chat = ChatGroq(temperature=0, model_name=\"llama3-8b-8192\")\n",
    "            \n",
    "            template = '''You will assist me in deciding, based on the first 3 entries of a table, whether the first row or the first colum should be the header, or if both the first column and first row are headers\n",
    "        Table:\n",
    "        {table}\n",
    "\n",
    "        Determine the indexing type of the table and output:\n",
    "        - 0 if the table is indexed by the first column.\n",
    "        - 1 if the table is indexed by the first row.\n",
    "        - 2 if the table is indexed by both the first row and the first column.   \n",
    "                    \n",
    "        {format_instructions}'''\n",
    "            prompt = PromptTemplate(\n",
    "                template=template,\n",
    "                input_variables=[\"table\"],\n",
    "                partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "            )\n",
    "                \n",
    "            chain = prompt | chat | parser\n",
    "            return chain.invoke({\"table\": table_str})\n",
    "        \n",
    "        table_first_3_rows = get_first_three_rows(table)\n",
    "        table_str = get_table_check_string(table_first_3_rows)\n",
    "        header_output = eval_table_index_llama(table_str)\n",
    "        if isinstance(header_output, str):\n",
    "            header_output = clean_llama_output_if_string(header_output)\n",
    "        header_index = header_output['index']\n",
    "        # Now need diff cases of how to handle the table if the model outputs, 0,1, or 2\n",
    "            # where 0 is the first col, 1 is second col, 2 is for both cols are headers\n",
    "        pass\n",
    "    \n",
    "    # If the table cannot be identified by the ExtractTable API\n",
    "    if is_unidentified(table):\n",
    "        processed_unidentified_table = transform_unidentified_table(table)\n",
    "        pass\n",
    "    \n",
    "    # only need to look at the first row\n",
    "    first_row = [el for el in table if \"TR/\" in el['path']]\n",
    "    \n",
    "    # If the table has 2 indexes\n",
    "    if is_2_index(first_row):\n",
    "            print(\"This table has 2 indexes\")\n",
    "            # Function to produce table which has 2 indexes\n",
    "            def get_2_index_table(table):\n",
    "                table_rows = set([re.search(r'Table(?:\\[\\d+\\])?/TR(?:\\[\\d+\\])?', item['path']).group() for item in table if re.search(r'Table(?:\\[\\d+\\])?/TR(?:\\[\\d+\\])?', item['path'])])\n",
    "                uniq_rows = sorted([row_name.split(\"/\")[-1] for row_name in list(table_rows)])\n",
    "\n",
    "                # Get indexes from the first row:\n",
    "                row_indexes = [el['text'].strip() for el in table if \"TR/TH\" in el['path']]\n",
    "\n",
    "                data = {}\n",
    "\n",
    "                # Only look at second row onwards\n",
    "                for i in range(1,len(uniq_rows)):\n",
    "                    row_name = uniq_rows[i]\n",
    "                    row = [el for el in table if row_name in el['path']]\n",
    "                    row_key = row[0]['text'].strip()\n",
    "                    \n",
    "                    unique_tds = set()\n",
    "                    for item in row:\n",
    "                        path_parts = item['path'].split('/')\n",
    "                        for part in path_parts:\n",
    "                            if 'TD' in part:\n",
    "                                unique_tds.add(part)\n",
    "                    # Convert the set to a list and sort it for consistent output\n",
    "                    unique_tds_list = sorted(list(unique_tds))\n",
    "\n",
    "                    sections_of_row = []\n",
    "                    for td in unique_tds_list:\n",
    "                        td_section = \"\"\n",
    "                        for i in range(len(row)):\n",
    "                            if i ==0:\n",
    "                                td += \"/\"\n",
    "                            if td in row[i]['path']:\n",
    "                                td_section += row[i]['text'].strip()\n",
    "                                \n",
    "                        sections_of_row.append(td_section)\n",
    "                    \n",
    "                    data[row_key] = sections_of_row\n",
    "                    \n",
    "                df = pd.DataFrame(data, index=row_indexes).T\n",
    "                df.to_csv(output_file_path)\n",
    "                return df    \n",
    "            \n",
    "            df = get_2_index_table(table)\n",
    "            return df\n",
    "        \n",
    "    # If the table only has one index\n",
    "    else:\n",
    "            print(\"This table has 1 index\")\n",
    "\n",
    "            # If the header for this df is the row\n",
    "            if is_row_header(first_row):\n",
    "                print(\"This is a row indexed table\")\n",
    "                def get_row_header_table(table):\n",
    "                    table_rows = set([re.search(r'Table(?:\\[\\d+\\])?/TR(?:\\[\\d+\\])?', item['path']).group() for item in table if re.search(r'Table(?:\\[\\d+\\])?/TR(?:\\[\\d+\\])?', item['path'])])\n",
    "                    uniq_rows = sorted([row_name.split(\"/\")[-1] for row_name in list(table_rows)])\n",
    "\n",
    "                    # Table headers, also the keys\n",
    "                    headers = [el['text'].strip() for el in table if uniq_rows[0]+\"/\" in el['path']]\n",
    "                    rows = []\n",
    "                    for i in range(1,len(uniq_rows)):\n",
    "                        row_name = uniq_rows[i]\n",
    "                        row = [el for el in table if row_name in el['path']]\n",
    "                        # rows.append(row)\n",
    "                        \n",
    "                        unique_tds = set()\n",
    "                        for item in row:\n",
    "                            path_parts = item['path'].split('/')\n",
    "                            for part in path_parts:\n",
    "                                if 'TD' in part:\n",
    "                                    unique_tds.add(part)\n",
    "                        # Convert the set to a list and sort it for consistent output\n",
    "                        unique_tds_list = sorted(list(unique_tds))\n",
    "\n",
    "                        sections_of_row = []\n",
    "                        for td in unique_tds_list:\n",
    "                            td_section = \"\"\n",
    "                            for i in range(len(row)):\n",
    "                                if i ==0:\n",
    "                                    td += \"/\"\n",
    "                                if td in row[i]['path']:\n",
    "                                    td_section += row[i]['text'].strip()\n",
    "                                    \n",
    "                            sections_of_row.append(td_section)\n",
    "                        rows.append(sections_of_row)\n",
    "                                    \n",
    "                    df = pd.DataFrame(rows, columns=headers)\n",
    "                    return df\n",
    "                \n",
    "                df = get_row_header_table(table)\n",
    "                df.to_csv(output_file_path, index=False)\n",
    "                return df\n",
    "                    \n",
    "            # The header for this df is the column\n",
    "            else:\n",
    "                print(\"This table is a column indexed table...\")\n",
    "                def get_column_header_table(table):\n",
    "                    table_rows = set([re.search(r'Table(?:\\[\\d+\\])?/TR(?:\\[\\d+\\])?', item['path']).group() for item in table if re.search(r'Table(?:\\[\\d+\\])?/TR(?:\\[\\d+\\])?', item['path'])])\n",
    "                    uniq_rows = sorted([row_name.split(\"/\")[-1] for row_name in list(table_rows)])\n",
    "\n",
    "                    data = {}\n",
    "\n",
    "                    for i in range(len(uniq_rows)):\n",
    "                                        row_name = uniq_rows[i]\n",
    "                                        if i == 0:\n",
    "                                            row = [el for el in table if row_name+\"/\" in el['path']]\n",
    "                                        else:\n",
    "                                            row = [el for el in table if row_name in el['path']]\n",
    "                                        \n",
    "                                        unique_tds = set()\n",
    "                                        for item in row:\n",
    "                                            path_parts = item['path'].split('/')\n",
    "                                            for part in path_parts:\n",
    "                                                if 'TD' in part:\n",
    "                                                    unique_tds.add(part)\n",
    "                                        # Convert the set to a list and sort it for consistent output\n",
    "                                        unique_tds_list = sorted(list(unique_tds))\n",
    "                                        \n",
    "                                        sections_of_row = []\n",
    "                                        for td in unique_tds_list:\n",
    "                                            td_section = \"\"\n",
    "                                            for i in range(len(row)):\n",
    "                                                if i ==0:\n",
    "                                                    td += \"/\"\n",
    "                                                if td in row[i]['path']:\n",
    "                                                    td_section += row[i]['text'].strip()\n",
    "                                                    \n",
    "                                            sections_of_row.append(td_section)\n",
    "                                                    \n",
    "                                        row_key = row[0]['text'].strip()\n",
    "                                        data[row_key] = sections_of_row\n",
    "                    df = pd.DataFrame.from_dict(data, orient='index').T\n",
    "                    return df\n",
    "                                    \n",
    "                df = get_column_header_table(table)\n",
    "                df.to_csv(output_file_path, index=False)\n",
    "                return df\n",
    "\n",
    "# Function that saves the tables in CSV format to a unique job id       \n",
    "def save_tables_to_csv(extracted_tables, table_output_directory):\n",
    "    \n",
    "    os.makedirs(table_output_directory, exist_ok=True)\n",
    "    for table_num, table in extracted_tables.items():\n",
    "        output_file_path = os.path.join(table_output_directory, f\"table_{table_num}.csv\")\n",
    "        df = transform_table(table, output_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runing the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-28 11:16:36,242 - INFO - Started uploading asset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-28 11:16:39,442 - INFO - Finished uploading asset\n",
      "2024-08-28 11:16:39,445 - INFO - Started submitting EXTRACT_PDF job\n",
      "2024-08-28 11:16:40,837 - INFO - Started getting job result\n",
      "2024-08-28 11:18:11,556 - INFO - Finished polling for status\n",
      "2024-08-28 11:18:11,562 - INFO - Finished getting job result\n",
      "2024-08-28 11:18:11,563 - INFO - Started getting content\n",
      "2024-08-28 11:18:11,991 - INFO - Finished getting content\n"
     ]
    }
   ],
   "source": [
    "file_path = '../PDF/list-of-previously-mapped-courses-270824.pdf'\n",
    "extractor, unique_id = ExtractTextTableInfoFromPDF.create_with_unique_id(file_path)\n",
    "extracted_data = extractor.extracted_data\n",
    "pdf_data = get_extracted_data(extracted_data)\n",
    "fname = file_path.split(\"/\")[-1]\n",
    "with open(f\"../data/{fname}.json\", \"w\", encoding=\"utf-8\") as fout:\n",
    "    json.dump(pdf_data, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique ID: 8bc6411c-15d3-403f-9607-2bb8df5cfe71\n",
      "This table has 1 index\n",
      "This is a row indexed table\n"
     ]
    }
   ],
   "source": [
    "with open(f\"../data/{fname}.json\", \"r\", encoding='utf-8') as fin:\n",
    "    pdf_data = json.load(fin)\n",
    "    \n",
    "def extract_tables_from_pdf(pdf_data):\n",
    "    table_elements = [el for el in pdf_data['elements'] if \"Table\" in el['Path'] and 'Text' in el and \"TR\" in el['Path']]\n",
    "    print(\"\\nUnique ID:\", unique_id)\n",
    "\n",
    "    # IF there are even any table elements in the PDF\n",
    "    if table_elements:\n",
    "        table_output_directory = f\"../data/{unique_id}\" \n",
    "        extracted_tables = extract_unique_tables(table_elements)    \n",
    "        save_tables_to_csv(extracted_tables, table_output_directory)\n",
    "\n",
    "# Section here to derive the csvs from the table elements\n",
    "# Uncomment to extract tables (for now its not ready yet)\n",
    "extract_tables_from_pdf(pdf_data)\n",
    "\n",
    "# Section here to derive the nodes from the text elements\n",
    "# text_splitter = initialise_text_splitter(1000, 50)\n",
    "page_texts = get_text_chunks(file_path, pdf_data)\n",
    "page_nodes = convert_pagetexts_to_nodes(page_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 The Founding of Modern Science Intended Learning Outcomes for Lecture 01 You should be able to do the following after this lecture. (1) Describe what is science and explain the scientific method “in a nutshell”, illustrating your explanation with a straightforward example. (2) Describe the roles scientific observations play in the scientific method. (3) Explain what are the main concerns that should be addressed when making scientific observations. (4) Explain why anomalous phenomena are important for science, illustrating your explanation with some examples from the scientific revolution. (5) In the context of the scientific revolution, discuss the difference between an evidence-based understanding of the natural world versus one based on authority. (6) Discuss the steam engine’s contribution to the Industrial Revolution and its impact on population growth in industrialized nations. 1.1 What is Science? Hi all, welcome to the first video in this series. This lecture, which is made up of several videos, is about what science is and is a cut-down bare-bones explanation of the scientific method “in a nutshell” – which we will see illustrated with a few examples. We will take a closer look at the first step in the scientific method (again illustrated with an example) and then briefly review the founding of modern science and what one could say was a direct consequence of that – the Industrial Revolution, and it’s here we’ll see the beginning of mankind’s dependence on fossil fuels. I hope you have had a look through the intended learning outcomes for this lecture. They are listed right before this video, so let’s get straight into addressing our first learning outcome, which is to answer the question: “What is science actually?” I bet most people think of science in terms of “subjects”, like Chemistry, Physics, Biology, Medicine, and Pharmacy, just to name a few. But is this actually science? Knowledge in textbooks? Where did this knowledge come from anyway? Most people would say that the facts, ideas, and concepts in science textbooks is true to the best of our knowledge, but how do we know it’s true? In fact, just how do we know what we currently know, at all? By answering these questions, we get closer to figuring out what science is. 1.1.1 What is Science and the Scientific Method in a Nutshell At this point, we’ll take a look at what science is from our course textbook A Beginner’s Guide to Scientific Method. Right here in Chapter 1, page 5, we read: “Science is that activity which aims to further our understanding of why things happen as they do in the natural world. It accomplishes this goal by applications of the scientific method.” So, what exactly is the scientific method? Our textbook goes on to explain, “…it is the process of observing nature, isolating a facet that is not well understood, and then proposing and testing possible explanations.” Observe. Explain. Test the explanation. \n",
      "\n",
      "1.1.2 Science is Self-Correcting You see, science is self-correcting. The vast majority of the knowledge found in today’s textbooks was all hard won, with multiple wrong explanations being proposed and then discarded until arriving at the current version. This may yet be undone if some new test of that understanding reveals it's lacking in some way. This testing and refinement of our knowledge and understanding is the nature of Scientific Inquiry, the main topic of this course. The best way to understand scientific inquiry is through examples and application. The topic we’ve chosen to look at to gain an understanding of scientific inquiry is perhaps the single most important problem facing our species today – that is, climate change and loss of biodiversity. It is science that offers us our best chance at figuring out how we can get out of this mess – and what all of us need to do to get there. OK, before we get into this serious problem, we need to get a good understanding of this “observe, explain, test” approach to knowledge discovery. Did you know that any time you troubleshoot something you’re actually applying the scientific method? For example, let’s take a look at troubleshooting a laptop that doesn’t bootup in our next video. 1.2 A PC Won’t Work 1.2.1 Troubleshooting a Laptop We find that our laptop doesn’t boot, so we troubleshoot it, which is an example of a straightforward application of the scientific method. Observation Laptop doesn’t boot Explanation Battery dead Test the explanation Plug in external power Result of test Laptop seems to boot, so the battery must have been dead But now there’s a new problem. Re-running through our “observe, explain, test” steps again we find: Observation Laptop seems to boot, but there’s nothing on the screen Explanation Laptop monitor not working Test the explanation Try connecting the external monitor with HDMI cable 1 Result of test Laptop seems to boot, but there’s nothing on the screen. (a) Either the graphics card or motherboard has issues, or (b) Something was wrong with our test. \n",
      "\n",
      "Observation Laptop seems boot, but there’s nothing on the screen Explanation Laptop monitor not working Test the explanation Try connecting the external monitor with HDMI cable 2 Result of test Laptop definitely boots and the external monitor shows the start screen. After performing a series of tests in the “real world”, that is, by testing our explanations, we discover the following things we didn’t know before performing any tests: We had a dead battery. The graphics card and motherboard are working fine. The laptop’s monitor doesn’t work. A bad connection? Screen broken? We have a faulty HDMI cable. 1.2.2 Troubleshooting and the Scientific Method Troubleshooting is an example of the scientific method. It is in a sense a trivial example of it – the scientific method is a lot more powerful than that. The scientific method can be used to probe and discover brand new things about nature. This, of course, relies on our explanations and then testing those explanations. If our explanation ends up not being falsified, then we have support for our explanation and the more we test our explanation the more confidence we have that our explanation is, in fact, true. This is how things are discovered using the scientific method, and this is the way we have discovered many things about the world today. All the content in science textbooks have been subjected to this procedure. This example illustrates how science is self-correcting, where our steps taken above can be summarized in the following flow chart. We also note that every time we loop through this chart, regardless of whether the test is consistent with the explanation or not, we gain new information about the world around us. \n",
      "\n",
      "In the previous video, we saw that troubleshooting a laptop was an example of the scientific method. In fact, troubleshooting anything in general represents an application of the scientific method. You observe that there is clearly something wrong. You come up with possible explanations as to what might be the cause of the problem. Finally, you go through the process of checking if the thing you thought might be wrong is indeed the cause of the problem. If it isn’t then you try something else, i.e., you toss aside your previous explanation of what the problem was and make a new explanation for the cause. You continue doing this until you can at least narrow down what’s the cause of the problem. Observe, explain, and test the explanation – the scientific method in a nutshell. The point is by running through this process you will discover things you didn’t know before. Now I’ll illustrate this again, but in the household environment. This time we’ll use the scientific method to discover something new about nature, or at least something that might be new to you. Observations (1) Tea bag bloats and floats on top of the water when boiling water is poured directly on top of it. (2) Tea bag doesn’t bloat and sinks in the water when boiling water is poured on the side and not directly onto it. Explanation Water poured on top of the tea bag fills the pores of the teabag itself, trapping any gas inside before it can escape. The hot water heats the trapped air, causing it to expand. The trapped air prevents the tea bag from being dunked. Pores of the tea bag can get sealed up with water and prevent air from escaping. Test the explanation Quickly seal the tea bag in cold water, trapping the air, then pour boiling water near to but not directly onto it. Result of test Tea bag bloats and floats on top of the water supporting the explanation. The test supports our explanation, rather than confirm it. If we really wanted to check whether this explanation is correct, we would need to do a lot more tests. Compared to troubleshooting the laptop in the previous section, the results of the test with the tea bag are a lot more tentative since we generally know more about the way a laptop works and our tests are more directly indicative of the explanation being correct (or not) than the tea bag. We don’t know that much about what’s going on exactly. We have this speculative \n",
      "\n",
      "There are lots of additional interesting questions we can now ask about this phenomenon of liquid water stretching across tiny gaps in the teabag and effectively sealing the air inside: • If there really is water sealing the pores, then can we see it? • How thick is the film of water sealing the pores of the teabag? • How long does this film last? • How strongly does the water seal in the air? i.e., what level of air pressure is needed to break the seal? • How big do the pores need to be so that the water can’t seal air inside them? • Does adding other substances to the water change this behavior? • What about other liquids? Do they behave in the same way? In science, answering one question often leads to more questions, and an investigation can sometimes take wild and unanticipated twists and turns that lead to new knowledge and understanding of nature. 1.4 Cadaverous Poisoning You may not have realized it, but in the previous video we were just starting to scratch the surface of two entire branches of science: “Interface Science” and “Colloidal Science” – extremely important areas within chemistry, food science, biology, and physics, etc. A good understanding of these areas is also of great importance to industry and for most of the products industry produces. 1.4.1 Semmelweis and Childbed Fever I have one last example of a fairly straightforward application of the scientific method, taken from our textbook and drawn from the annals of science. Our example will be from the area of medicine and we’re going to wind the clock back to 1846, to a time when science had yet to discover the “germ” – that is, those nasty little disease-causing organisms so small they are invisible to the naked eye. Figure 1 Semmelweis from Wikimedia: Jenő Doby, Public domain, via Wikimedia Commons Dr. Ignaz Semmelweis (Figure 1) was hired on a three-year contract into the Vienna General Hospital's maternity clinic from 1846 – 1849. At the time “childbed fever”, aka puerperal fever, was running rampant in hospitals all over Europe and the US. This disease affected mothers after they gave birth or had a miscarriage. Without going into details, it was a particularly nasty disease with a number of easily recognizable signs and symptoms. It usually occurred after the first 24 hours and within the first ten days following delivery. The fever was deadly, killing up to 80% of those diagnosed and sometimes reached as high as 40% of all mothers admitted to the hospital. As with all physicians, Dr. Semmelweis was particularly concerned, so he studied the data on mortality rates within his hospital dating from 1841 – 1846 looking for something, anything, that might give a clue on how to stop the fever. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(page_nodes[i].text)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure 2 Mortality rates in two clinics in the Vienna General Hospital from 1841-1846. clinic were somehow more prone to illness? He noticed something when comparing the mortality rates between two different clinics in his hospital (Figure 2). Now there was no obvious difference between the two sets of mothers in each clinic, so why were a lot more women dying in his clinic where doctors attended to mothers (orange curve in Figure 2)? He showed the findings to his colleagues, but everyone was at a loss as to why this should be so. Was it something the doctors were doing wrong? or perhaps something the midwives were doing right? Maybe it was due to a difference in the clinical environments? Perhaps the patients in his At this time, in the middle of the 19th century, the “germ” or pathogen, was not known to be the cause of disease. Germs had not been observed, and any speculation of their possible existence was not taken seriously. The reining theories for how disease was spread, caused, and treated at the time were quite wrong. 1.4.2 A Key Observation Figure 3 Kolletschka from Wikimedia: Unknown author, Public domain, via Wikimedia Commons Then a rather unfortunate incident occurred in 1847 in an anatomical pathology lab. Dr Semmelweis’ friend, who he greatly admired, died after being accidentally pricked by a scalpel being used by a student doctor while he was assisting in performing an autopsy. Professor Kolletschka (Figure 3) suffered identical signs and symptoms as the mothers who died of childbed fever. Dr Semmelweis wrote about the incident: “Day and night I was haunted by the image of Kolletschka's disease and was forced to recognize, ever more decisively, that the disease from which Kolletschka died was identical to that from which so many maternity patients died.” There is another very pertinent fact, or observation, in this case. Right after the student doctors attended the anatomical pathology lab, where they dissected badly infected corpses, they would go to Dr Semmelweis’ maternity clinic to assist in the births of expectant mothers. Do remember that the way disease was spread, caused, and treated in those days was completely misunderstood. No one knew about germs, so there certainly were no antibiotics and there was no disinfecting of anything – no hand washing of one’s hands especially when, by just looking at them, they were quite obviously clean. So now we have our careful observations, or facts, (1) The mortality rate of mothers due to childbed fever in a clinic attended by doctors was, on average, five times higher than what appears to be a similar clinic with similar mothers but attended by midwives instead of doctors. (2) Doctors attend to mothers in the clinic directly after having been engaged in autopsies of infected corpses in the anatomical pathology lab. (3) A doctor dies from identical signs and symptoms to childbed fever after having been stuck with a scalpel used to dissect infected corpses in the anatomical pathology lab. \n"
     ]
    }
   ],
   "source": [
    "print(page_nodes[5].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text files have been saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Assuming page_nodes is a list containing your page nodes with text\n",
    "page_nodes = [page for page in page_nodes]  # replace this with your actual list of page nodes\n",
    "\n",
    "# Define the base directory where the folders will be created\n",
    "base_directory = '../page_texts'\n",
    "\n",
    "# Create the base directory if it does not exist\n",
    "if not os.path.exists(base_directory):\n",
    "    os.makedirs(base_directory)\n",
    "\n",
    "# Iterate through the pages and save text to .txt files\n",
    "for i in range(len(page_nodes)):\n",
    "    # Determine the folder number (every 5 pages in a new folder)\n",
    "    folder_number = (i // 15) + 1\n",
    "    folder_path = os.path.join(base_directory, f'folder_{folder_number}')\n",
    "\n",
    "    # Create the folder if it does not exist\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    # Define the file name and path\n",
    "    file_name = f'page_{i+1}.txt'\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "    # Save the text to the file\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(page_nodes[i])\n",
    "\n",
    "print(\"Text files have been saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up ingestion pipeline to pipe data into pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.legacy import ServiceContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 22:42:11,963 - INFO - Embedding model loaded...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbedding(model_name='sentence-transformers/all-mpnet-base-v2', embed_batch_size=10, callback_manager=<llama_index.legacy.callbacks.base.CallbackManager object at 0x32a8dac20>, tokenizer_name='sentence-transformers/all-mpnet-base-v2', max_length=514, pooling=<Pooling.MEAN: 'mean'>, normalize=True, query_instruction=None, text_instruction=None, cache_folder=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name='sentence-transformers/all-mpnet-base-v2')\n",
    "if embed_model:\n",
    "    logging.info(\"Embedding model loaded...\")\n",
    "embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the initial pipeline\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SemanticSplitterNodeParser(\n",
    "            buffer_size=1,\n",
    "            breakpoint_percentile_threshold=95,\n",
    "            embed_model=embed_model,  \n",
    "        ),\n",
    "        embed_model,  # Use the model instance directly in the pipeline\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize connection to Pinecone\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "unique_id = \"zac-notes\"\n",
    "index_name = unique_id\n",
    "\n",
    "# Create your index (can skip this step if your index already exists)\n",
    "hybrid_search=True\n",
    "if hybrid_search:\n",
    "    pc_similarity_metric = \"dotproduct\"\n",
    "else:\n",
    "    pc_similarity_metric = \"cosine\"\n",
    "\n",
    "# pc.create_index(\n",
    "#     index_name,\n",
    "#     dimension=768,\n",
    "#     metric=pc_similarity_metric,\n",
    "#     spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "# )\n",
    "\n",
    "# Initialize your index\n",
    "pinecone_index = pc.Index(index_name)\n",
    "\n",
    "# Initialize VectorStore\n",
    "vector_store = PineconeVectorStore(pinecone_index=pinecone_index, llm=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the vector store index into the insertion pipeline\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SemanticSplitterNodeParser(\n",
    "            buffer_size=1,\n",
    "            breakpoint_percentile_threshold=95,\n",
    "            embed_model=embed_model,\n",
    "            ),\n",
    "        embed_model,\n",
    "        ],\n",
    "        vector_store=vector_store  # Our new addition\n",
    "    )\n",
    "\n",
    "# Pushing the first 3 pages of PDF to the pinecone index. \n",
    "first_3_pages = page_nodes[:3]\n",
    "# pipeline.run(documents=first_3_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of embeddings in this index: 768\n",
      "Index fullness is at 0.0\n",
      "Namespaces in this index: {'': {'vector_count': 6}}\n",
      "Total vector count at 6\n"
     ]
    }
   ],
   "source": [
    "def describe_pinecone_index():\n",
    "    pc_stats = pinecone_index.describe_index_stats()\n",
    "    print(f\"Dimension of embeddings in this index: {pc_stats['dimension']}\")\n",
    "    print(f\"Index fullness is at {pc_stats['index_fullness']}\")\n",
    "    print(f\"Namespaces in this index: {pc_stats['namespaces']}\")\n",
    "    print(f\"Total vector count at {pc_stats['total_vector_count']}\")\n",
    "    \n",
    "describe_pinecone_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different User Query Types:\n",
    "User queries in an RAG application vary based on individual intent. For these diverse query types, it’s essential to fine-tune the Alpha parameter. This process involves routing each user query to a specific Alpha value for effective retrieval and response synthesis. Microsoft has identified various user query categories, and we have selected a few for tuning our hybrid search. Below are the different user query types we considered:\n",
    "\n",
    "- Web Search Queries: Brief queries similar to those typically inputted into search engines.\n",
    "- Concept Seeking Queries: Abstract questions that necessitate detailed, multi-sentence answers.\n",
    "- Fact Seeking Queries: Queries that have a single, definitive answer.\n",
    "- Keyword Queries: Concise queries composed solely of crucial identifier words.\n",
    "- Queries With Misspellings: Queries containing typos, transpositions, and common misspellings.\n",
    "- Exact Sub-string Searches: Queries that exactly match sub-strings from the original context.\n",
    "\n",
    "Tuning the alpha value to adjust how much of vector and how much of semantic search we should be using:\n",
    "- https://medium.com/llamaindex-blog/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Groq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Instantiate VectorStoreIndex object from our vector_store object\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mGroq\u001b[49m(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama3-70b-8192\u001b[39m\u001b[38;5;124m\"\u001b[39m, api_key\u001b[38;5;241m=\u001b[39mgroq_api_key)\n\u001b[1;32m      6\u001b[0m response \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mcomplete(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExplain the importance of low latency LLMs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m response\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Groq' is not defined"
     ]
    }
   ],
   "source": [
    "from llama_index.legacy.retrievers import VectorIndexRetriever\n",
    "# Instantiate VectorStoreIndex object from our vector_store object\n",
    "import llama_index.legacy.llms\n",
    "\n",
    "llm = ChatGroq(model=\"llama3-70b-8192\", api_key=groq_api_key)\n",
    "\n",
    "vector_index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n",
    "\n",
    "# Grab 5 search results\n",
    "retriever = VectorIndexRetriever(index=vector_index, similarity_top_k=5)\n",
    "\n",
    "# # Query vector DB\n",
    "answer = retriever.retrieve('What is FWD Invest First max about?')\n",
    "for ans in answer:\n",
    "    print(ans)\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuation of table extraction (For unidentified tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the table here has the TH's replaced with TD's\n",
    "table = extracted_tables[1]\n",
    "# Replace every 'TH' with 'TD' in the 'path' key\n",
    "for item in table:\n",
    "    item['path'] = item['path'].replace('TH', 'TD')\n",
    "\n",
    "# only need to look at the first row\n",
    "first_row = [el for el in table if \"TR[2]/\" in el['path']]\n",
    "    # Function to decide if table has 2 indexes\n",
    "\n",
    "def get_table_dim(table):\n",
    "    # Regular expressions to find row and column indices\n",
    "    row_regex = re.compile(r'/TR\\[(\\d+)\\]')\n",
    "    col_regex = re.compile(r'/TD\\[(\\d+)\\]')\n",
    "\n",
    "    max_row = 0\n",
    "    max_col = 0\n",
    "\n",
    "    for entry in table:\n",
    "        path = entry['path']  # Extract path from the dictionary\n",
    "        \n",
    "        # Extract row and column indices\n",
    "        row_match = row_regex.search(path)\n",
    "        col_match = col_regex.search(path)\n",
    "        \n",
    "        # Update max row index\n",
    "        if row_match:\n",
    "            row_index = int(row_match.group(1))\n",
    "            max_row = max(max_row, row_index)\n",
    "        \n",
    "        # Update max column index\n",
    "        if col_match:\n",
    "            col_index = int(col_match.group(1))\n",
    "            max_col = max(max_col, col_index)\n",
    "        elif '/TD/' in path and not col_match:  # Check for default first column\n",
    "            max_col = max(max_col, 1)  # Ensure first column is counted\n",
    "    return (max_row, max_col)    \n",
    "\n",
    "# Function to check if the API could not identify the index of the table\n",
    "def is_unidentified(table):\n",
    "    table_dim = get_table_dim(table)\n",
    "    min_headers = min(table_dim)\n",
    "    header_count = len([el for el in table if \"TH\" in el['path']])\n",
    "    if header_count < min_headers:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Function to transform the unidentified table by determining its index using LLM\n",
    "def transform_unidentified_table(table):\n",
    "    # Function to get the first 3 rows \n",
    "    def get_first_three_rows(table):\n",
    "            # Regular expression to extract row index\n",
    "            row_regex = re.compile(r'/TR\\[(\\d+)\\]')\n",
    "            \n",
    "            # List to hold the first three rows table\n",
    "            rows = []\n",
    "            \n",
    "            for entry in table:\n",
    "                path = entry['path']\n",
    "                # Find row index\n",
    "                row_match = row_regex.search(path)\n",
    "                \n",
    "                if row_match:\n",
    "                    row_index = int(row_match.group(1))\n",
    "                    # Check if the row index is within the first three rows\n",
    "                    if 1 <= row_index <= 3:\n",
    "                        rows.append(entry)\n",
    "                elif '/TR/' in path:  # Check for paths that imply the first row implicitly\n",
    "                    rows.append(entry)\n",
    "        \n",
    "            table_first_3_rows = [] # This will be a 2D list\n",
    "            table_rows = set([re.search(r'Table(?:\\[\\d+\\])?/TR(?:\\[\\d+\\])?', item['path']).group() for item in table if re.search(r'Table(?:\\[\\d+\\])?/TR(?:\\[\\d+\\])?', item['path'])])\n",
    "            uniq_rows = sorted([row_name.split(\"/\")[-1] for row_name in list(table_rows)[:3]])\n",
    "            uniq_rows[0] += \"/\"\n",
    "\n",
    "\n",
    "            for i in range(len(uniq_rows)):\n",
    "                row = [el for el in table if uniq_rows[i] in el['path']]\n",
    "                unique_tds = set()\n",
    "                for item in row:\n",
    "                            path_parts = item['path'].split('/')\n",
    "                            for part in path_parts:\n",
    "                                if 'TD' in part:\n",
    "                                    unique_tds.add(part)\n",
    "                # Convert the set to a list and sort it for consistent output\n",
    "                unique_tds_list = sorted(list(unique_tds))\n",
    "                        \n",
    "                sections_of_row = []\n",
    "                for td in unique_tds_list:\n",
    "                            td_section = \"\"\n",
    "                            for i in range(len(row)):\n",
    "                                if i ==0:\n",
    "                                    td += \"/\"\n",
    "                                if td in row[i]['path']:\n",
    "                                    td_section += row[i]['text'].strip()\n",
    "\n",
    "                            sections_of_row.append(td_section)\n",
    "                \n",
    "                # print(sections_of_row)\n",
    "                table_first_3_rows.append(sections_of_row)\n",
    "\n",
    "            # account for case where there are 2 index columns:\n",
    "            min_dim = max(list(map(lambda x: len(x), table_first_3_rows)))\n",
    "            if len(table_first_3_rows[0]) < min_dim:\n",
    "                table_first_3_rows[0].insert(0, \"empty\")\n",
    "            return table_first_3_rows\n",
    "\n",
    "    # Function to get the first 3 rows of the table into a string to parse to the LLM \n",
    "    def get_table_check_string(table_first_3_rows):\n",
    "        table_str = ''\n",
    "        for i in range(len(table_first_3_rows)):\n",
    "            table_str += f'''Row{i+1}: {\" | \".join(table_first_3_rows[i])}\\n'''\n",
    "        return table_str\n",
    "\n",
    "    # Function to clean the output if it decides to explain its choice.\n",
    "    def clean_llama_output_if_string(input_str):\n",
    "        \"\"\"\n",
    "        Extracts a dictionary-like substring from the given input string.\n",
    "        \n",
    "        Args:\n",
    "        input_str (str): The input string containing the dictionary-like substring.\n",
    "        \n",
    "        Returns:\n",
    "        dict: The extracted dictionary as a Python dictionary.\n",
    "        \"\"\"\n",
    "        # Use a regular expression to find the dictionary-like substring\n",
    "        match = re.search(r'\\{.*?\\}', input_str)\n",
    "        \n",
    "        if match:\n",
    "            dict_str = match.group()\n",
    "            try:\n",
    "                # Safely evaluate the dictionary string\n",
    "                extracted_dict = eval(dict_str)\n",
    "                if isinstance(extracted_dict, dict):\n",
    "                    return extracted_dict\n",
    "            except (SyntaxError, NameError):\n",
    "                pass\n",
    "        \n",
    "        return None\n",
    "\n",
    "    # Function to decide the indexing of the table\n",
    "    def eval_table_index_llama(table_str):\n",
    "        class Header(BaseModel):\n",
    "            index: int = Field(description=(\n",
    "                '''Index of the table indicating the type of indexing:\\n\\\n",
    "                    0 - No indexes\\n\\\n",
    "                    1 - Indexed by the first column\\n\\\n",
    "                    2 - Indexed by the first row\\n\\\n",
    "                    3 - Indexed by both the first row and the first column'''))\n",
    "\n",
    "        parser = JsonOutputParser(pydantic_object=Header)\n",
    "\n",
    "        chat = ChatGroq(temperature=0, model_name=\"llama3-8b-8192\")\n",
    "        \n",
    "        template = '''You will assist me in deciding, based on the first 3 entries of a table, whether the first row or the first colum should be the header, or if both the first column and first row are headers\n",
    "    Table:\n",
    "    {table}\n",
    "\n",
    "    Determine the indexing type of the table and output:\n",
    "    - 0 if the table is indexed by the first column.\n",
    "    - 1 if the table is indexed by the first row.\n",
    "    - 2 if the table is indexed by both the first row and the first column.   \n",
    "                \n",
    "    {format_instructions}'''\n",
    "        prompt = PromptTemplate(\n",
    "            template=template,\n",
    "            input_variables=[\"table\"],\n",
    "            partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "        )\n",
    "            \n",
    "        chain = prompt | chat | parser\n",
    "        return chain.invoke({\"table\": table_str})\n",
    "    \n",
    "    table_first_3_rows = get_first_three_rows(table)\n",
    "    table_str = get_table_check_string(table_first_3_rows)\n",
    "    header_output = eval_table_index_llama(table_str)\n",
    "    if isinstance(header_output, str):\n",
    "        header_output = clean_llama_output_if_string(header_output)\n",
    "    header_index = header_output['index']\n",
    "    # Now need diff cases of how to handle the table if the model outputs, 0,1, or 2\n",
    "        # where 0 is the first col, 1 is second col, 2 is for both cols are headers\n",
    "    pass\n",
    "\n",
    "if is_unidentified(table):\n",
    "    processed_unidentified_table = transform_unidentified_table(table)\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the extracted data from the extractor\n",
    "def get_extracted_data(extracted_data):\n",
    "    if 'structuredData.json' in extracted_data:\n",
    "        json_data = json.loads(extracted_data['structuredData.json'])\n",
    "    return json_data\n",
    "\n",
    "\n",
    "\n",
    "# This function converts tables to strings to be able to be processed by LLMs. \n",
    "def get_and_save_table_strings(extractor, table_output_directory):\n",
    "    \n",
    "    # Function to convert each row in a raw unprocessed table (ie index of table not decided) into a string\n",
    "    def get_raw_table_string(df):\n",
    "        table_str = \"\"\n",
    "        for i in range(2):\n",
    "            if i ==1:\n",
    "                table_str += f\"Row {i}: {df.iloc[i].values.tolist()}\"  \n",
    "            else:\n",
    "                table_str += f\"Row {i}: {df.iloc[i].values.tolist()}\\n\"\n",
    "        return table_str\n",
    "    \n",
    "    # Function to decide if header is first row or first column\n",
    "    def evaluate_table_index_llama(table_str):\n",
    "        class Header(BaseModel):\n",
    "            index: int = Field(description=\"Header of the table, 0 for first row as the header, 1 for first column as the header\")\n",
    "            \n",
    "        parser = JsonOutputParser(pydantic_object=Header)\n",
    "\n",
    "        chat = ChatGroq(temperature=0, model_name=\"llama3-8b-8192\")\n",
    "        \n",
    "        template = '''You will assist me in deciding, based on the first 2 entries of a table, whether the first row or the first colum should be the header. \n",
    "                You are to output an int, 0 or 1. Where 0 if the first row is header, and 1 if the first column is the header.\n",
    "                Follow the format instructions carefully.\n",
    "                Table:\n",
    "                {table}\n",
    "                \n",
    "                {format_instructions}\n",
    "                '''\n",
    "        prompt = PromptTemplate(\n",
    "            template=template,\n",
    "            input_variables=[\"table\"],\n",
    "            partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "        )\n",
    "        chain = prompt | chat | parser\n",
    "        return chain.invoke({\"table\": table_str})\n",
    "\n",
    "    # Tables need procecssing when extraced from BytesIO\n",
    "    def clean_table_values(x):\n",
    "        if isinstance(x, str):\n",
    "            return x.replace('_x000D_', '').strip()\n",
    "        return x\n",
    "    \n",
    "    # Code adapted from a medium blog on how to represent rows of tables in strings\n",
    "    def convert_table_to_string(df):\n",
    "        for _, row in df.iterrows():\n",
    "            row_str = \"\"\n",
    "            for col in df.columns:\n",
    "                sentences = re.split(r'(?<=\\.)\\s*', row[col])\n",
    "                row_sentence = \"\"\n",
    "                for i in range(len(sentences)):\n",
    "                    row_sentence += sentences[i] + \"\\n\"\n",
    "                row_str += f\"{col}: {row_sentence}, \"\n",
    "            formatted = row_str[:-2]\n",
    "        return formatted\n",
    "\n",
    "    os.makedirs(table_output_directory, exist_ok=True)\n",
    "    \n",
    "    # The literal extraction of the file itself\n",
    "    excel_files = {k: v for k, v in extractor.extracted_data.items() if k.endswith('.xlsx')}\n",
    "    \n",
    "    table_dataframes = {}\n",
    "\n",
    "    num_tables =0\n",
    "    for _, content in excel_files.items():\n",
    "        excel_stream = BytesIO(content)\n",
    "        df = pd.read_excel(excel_stream, header=None)\n",
    "        \n",
    "        df = df.applymap(clean_table_values)\n",
    "        # # Uncomment bottom code to ensure that Groq decides table header index \n",
    "        \n",
    "        # df_str = get_raw_table_string(df) \n",
    "        # dic = evaluate_table_index_llama(df_str)\n",
    "        # header_index = dic['index']\n",
    "        \n",
    "        # Uncomment this if u uncomment the code above\n",
    "        header_index = 1\n",
    "        \n",
    "        # If header_index is non zero\n",
    "        if header_index == 1:\n",
    "            df = pd.read_excel(excel_stream, header=None)\n",
    "            df = df.applymap(clean_table_values)\n",
    "            df = df.T\n",
    "            # Set the first row as the new header\n",
    "            new_header = df.iloc[0]  # Take the first row for the header\n",
    "            df = df[1:]  # Take the data less the header row\n",
    "            df.columns = new_header  # Set the header row as the df header\n",
    "            # Optionally, reset index if necessary\n",
    "            df.reset_index(drop=True, inplace=True)\n",
    "        else:\n",
    "            df = pd.read_excel(excel_stream, header=0)\n",
    "        \n",
    "        output_file_path = os.path.join(table_output_directory, f\"table_{num_tables}.csv\")\n",
    "\n",
    "        # Writing the table to the corresponding csv file. \n",
    "        df.to_csv(output_file_path, index=False)\n",
    "            \n",
    "        table_str = convert_table_to_string(df)\n",
    "        table_dataframes[num_tables] = table_str\n",
    "        num_tables += 1\n",
    "        \n",
    "    return table_dataframes\n",
    "\n",
    "\n",
    "\n",
    "# Function to initialise a flexible text splitter\n",
    "def initialise_text_splitter(chunk_size, chunk_overlap):\n",
    "    separator_list = [\"\\n\\n\", \"\\n\", \". \", \"!\", \"?\", \",\", \" \", \"\", \")\", \"(\"]\n",
    "    try:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=chunk_overlap,\n",
    "                length_function=len,\n",
    "                separators=separator_list)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to initialize text splitter: {e}\")\n",
    "        return None\n",
    "    return text_splitter\n",
    "\n",
    "\n",
    "\n",
    "# Function to obtain text chunks using the text splitter\n",
    "def get_text_chunks(file_path, json_data, text_splitter):\n",
    "    if 'elements' not in json_data:\n",
    "        logging.error(\"Missing 'elements' key in json_data\")\n",
    "        raise ValueError(\"Missing 'elements' key in json_data\")\n",
    "\n",
    "    if not text_splitter:\n",
    "        logging.error(\"Text splitter not initialised properly. \")\n",
    "        sys.exit()  \n",
    "        \n",
    "    file_name = file_path.split(\"/\")[-1]\n",
    "\n",
    "    # Chunks are split by pages here\n",
    "    page_text = \"\"\n",
    "    start_page = 0\n",
    "    all_chunks = []\n",
    "    \n",
    "    list_label = \"\"\n",
    "    for i in range(len(json_data['elements'])):\n",
    "        try:\n",
    "            current_page = json_data['elements'][i]['Page']\n",
    "        except KeyError:\n",
    "            logging.warning(f\"Missing 'Page' key in element at index {i}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            if current_page > start_page:\n",
    "                # Update the new page number\n",
    "                start_page = current_page               \n",
    "                # Generate the chunks for the previous page\n",
    "                separated_list = text_splitter.split_text(page_text)\n",
    "                for chunk in separated_list:\n",
    "                    if chunk not in [\". \", \".\"]:  # Simplified condition\n",
    "                        all_chunks.append({'ElementType': 'Text', 'file_name': file_name, 'Page': current_page, 'Text': chunk})\n",
    "                # Update the string of text \n",
    "                page_text = \"\"\n",
    "                list_label = \"\"\n",
    "            else:\n",
    "                if 'Text' in json_data['elements'][i]:  # Check if Text is not empty\n",
    "                    if json_data['elements'][i]['Path'].endswith(\"Lbl\") and not json_data['elements'][i]['Path'].startswith(\"//Document/Table\"):\n",
    "                        list_label = json_data['elements'][i]['Text']\n",
    "                    else:\n",
    "                        if list_label:\n",
    "                            page_text += list_label + json_data['elements'][i]['Text']\n",
    "                            list_label = \"\"  # Reset list label to empty string\n",
    "                        else:\n",
    "                            page_text += json_data['elements'][i]['Text'] + \"\\n\"\n",
    "        except KeyError as e:\n",
    "            logging.warning(f\"Key error in json_data['elements'][i] processing at index {i}: {e}\")\n",
    "    \n",
    "    \n",
    "    # Processing the last page of the text\n",
    "    if page_text:\n",
    "        separated_list = text_splitter.split_text(page_text)\n",
    "        for chunk in separated_list:\n",
    "            if chunk not in [\". \", \".\"]:\n",
    "                all_chunks.append({'ElementType': 'Text', 'file_name': file_name, 'Page': current_page, 'Text': chunk})\n",
    "\n",
    "    return all_chunks\n",
    "\n",
    "\n",
    "# Further enhancement to include the tables with the metadata so that it can be parsed to the \n",
    "    # function that takes in the tables + metadata for embeddings generation\n",
    "def get_table_strings_with_metadata(table_dataframes, json_data):\n",
    "    \n",
    "    # Function to obtain the page number of each table\n",
    "    def get_table_pages(json_data):\n",
    "        table_file_pages = {}\n",
    "        # Obtaining the table metadata\n",
    "        for i in range(len(json_data['elements'])):\n",
    "            try:\n",
    "                file_paths = json_data['elements'][i].get('filePaths')\n",
    "                if file_paths:\n",
    "                    page = json_data['elements'][i].get('Page', 'Unknown')\n",
    "                    match = re.search(r'\\d+', file_paths[0])\n",
    "                    table_index = match.group(0)\n",
    "                    table_file_pages[int(table_index)] = {\"Page\": page}\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing file paths at index {i}: {e}\")\n",
    "        return table_file_pages\n",
    "    \n",
    "    table_file_pages = get_table_pages(json_data)\n",
    "    meta_table_batch = []\n",
    "    table_dfs = []\n",
    "    for table_index,_ in table_dataframes.items():\n",
    "        dic = {}\n",
    "        dic['ElementType'] = 'Table'\n",
    "        dic['Page'] = table_file_pages[table_index]['Page']\n",
    "        dic['Table'] = table_dataframes[table_index]\n",
    "        table_dfs.append(dic)\n",
    "\n",
    "        #  Obtain metadata for sparse embeddings\n",
    "        meta_table_batch.append(f\"ElementType 'Table', Page {table_file_pages[table_index]['Page']}, {table_dataframes[table_index]}\")\n",
    "    return table_dfs, meta_table_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-02 18:38:10,933 - INFO - Started uploading asset\n",
      "2024-07-02 18:38:15,314 - INFO - Finished uploading asset\n",
      "2024-07-02 18:38:15,316 - INFO - Started submitting EXTRACT_PDF job\n",
      "2024-07-02 18:38:16,553 - INFO - Started getting job result\n",
      "2024-07-02 18:38:27,172 - INFO - Finished polling for status\n",
      "2024-07-02 18:38:27,176 - INFO - Finished getting job result\n",
      "2024-07-02 18:38:27,178 - INFO - Started getting content\n",
      "2024-07-02 18:38:27,486 - INFO - Finished getting content\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique ID: 0e4dcde5-fe65-4391-ade1-d1389e41d635\n"
     ]
    }
   ],
   "source": [
    "file_path = '../PDF/HSI1000-chapter1.pdf'\n",
    "extractor, unique_id = ExtractTextTableInfoFromPDF.create_with_unique_id(file_path)\n",
    "print(\"\\nUnique ID:\", unique_id)\n",
    "table_output_directory = f\"../data/{unique_id}\"\n",
    "extracted_data = extractor.extracted_data\n",
    "pdf_data = get_extracted_data(extracted_data)\n",
    "table_dataframes = get_and_save_table_strings(extractor, table_output_directory)\n",
    "\n",
    "# Use some form of evaluator to decide chunk size?\n",
    "text_splitter = initialise_text_splitter(300, 50)\n",
    "\n",
    "# Get out important information\n",
    "text_chunks = get_text_chunks(file_path, pdf_data, text_splitter)\n",
    "# table_dfs, meta_table_batch= get_table_strings_with_metadata(table_dataframes, pdf_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
