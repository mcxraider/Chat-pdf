{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import json\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "import time\n",
    "import sys\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "from dotenv import load_dotenv\n",
    "from io import BytesIO\n",
    "import io\n",
    "import zipfile\n",
    "import re\n",
    "\n",
    "# Adobe PDF Services imports\n",
    "from adobe.pdfservices.operation.auth.service_principal_credentials import ServicePrincipalCredentials\n",
    "from adobe.pdfservices.operation.exception.exceptions import ServiceApiException, ServiceUsageException, SdkException\n",
    "from adobe.pdfservices.operation.io.cloud_asset import CloudAsset\n",
    "from adobe.pdfservices.operation.io.stream_asset import StreamAsset\n",
    "from adobe.pdfservices.operation.pdf_services import PDFServices\n",
    "from adobe.pdfservices.operation.pdf_services_media_type import PDFServicesMediaType\n",
    "from adobe.pdfservices.operation.pdfjobs.jobs.extract_pdf_job import ExtractPDFJob\n",
    "from adobe.pdfservices.operation.pdfjobs.params.extract_pdf.extract_element_type import ExtractElementType\n",
    "from adobe.pdfservices.operation.pdfjobs.params.extract_pdf.extract_pdf_params import ExtractPDFParams\n",
    "from adobe.pdfservices.operation.pdfjobs.result.extract_pdf_result import ExtractPDFResult\n",
    "\n",
    "# Pinecone and Langchain imports\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from pinecone_text.sparse import BM25Encoder\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "hf_key = os.getenv('HUGGINGFACE_API_KEY')\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "# openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "dense_embedder_api = os.getenv(\"HF_API_URL\")\n",
    "\n",
    "# Define model\n",
    "chat_model = \"llama3-8b-8192\"\n",
    "\n",
    "from typing import Optional, Union, TypeAlias\n",
    "\n",
    "# Import other necessary modules\n",
    "from llama_index.legacy import Document\n",
    "from llama_index.legacy import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.legacy.node_parser import SentenceWindowNodeParser, SemanticSplitterNodeParser\n",
    "from llama_index.legacy.postprocessor import MetadataReplacementPostProcessor, SentenceTransformerRerank\n",
    "from llama_index.legacy.schema import MetadataMode\n",
    "from llama_index.legacy.ingestion import IngestionPipeline\n",
    "from llama_index.legacy.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.legacy.vector_stores import PineconeVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "class ExtractTextTableInfoFromPDF:\n",
    "    def __init__(self, file_path):\n",
    "        self.unique_id = str(uuid.uuid4())\n",
    "        try:\n",
    "            file = open(file_path, 'rb')\n",
    "            input_stream = file.read()\n",
    "            file.close()\n",
    "\n",
    "            # Initial setup, create credentials instance\n",
    "            credentials = ServicePrincipalCredentials(\n",
    "                client_id=os.getenv('ADOBE_SERVICES_CLIENT_ID'),\n",
    "                client_secret=os.getenv('ADOBE_SERVICES_CLIENT_SECRET')\n",
    "            )\n",
    "\n",
    "            # Creates a PDF Services instance\n",
    "            pdf_services = PDFServices(credentials=credentials)\n",
    "\n",
    "            # Creates an asset(s) from source file(s) and upload\n",
    "            input_asset = pdf_services.upload(input_stream=input_stream, mime_type=PDFServicesMediaType.PDF)\n",
    "\n",
    "            # Create parameters for the job\n",
    "            extract_pdf_params = ExtractPDFParams(\n",
    "                elements_to_extract=[ExtractElementType.TEXT, ExtractElementType.TABLES],\n",
    "            )\n",
    "\n",
    "            # Creates a new job instance\n",
    "            extract_pdf_job = ExtractPDFJob(input_asset=input_asset, extract_pdf_params=extract_pdf_params)\n",
    "\n",
    "            # Submit the job and gets the job result\n",
    "            location = pdf_services.submit(extract_pdf_job)\n",
    "            pdf_services_response = pdf_services.get_job_result(location, ExtractPDFResult)\n",
    "\n",
    "            # Get content from the resulting asset(s)\n",
    "            result_asset: CloudAsset = pdf_services_response.get_result().get_resource()\n",
    "            stream_asset: StreamAsset = pdf_services.get_content(result_asset)\n",
    "            \n",
    "            zip_bytes = io.BytesIO(stream_asset.get_input_stream())\n",
    "            with zipfile.ZipFile(zip_bytes, 'r') as zip_ref:\n",
    "                # Extract all the contents into memory\n",
    "                self.extracted_data = {name: zip_ref.read(name) for name in zip_ref.namelist()}\n",
    "                \n",
    "        except (ServiceApiException, ServiceUsageException, SdkException) as e:\n",
    "            logging.exception(f'Exception encountered while executing operation: {e}')\n",
    "    \n",
    "    # Generates a string containing a directory structure and file name for the output file using unique_id\n",
    "    @staticmethod\n",
    "    def create_output_file_path(unique_id: str) -> str:\n",
    "        os.makedirs(\"../data/Extracted_data\", exist_ok=True)\n",
    "        return f\"../data/Extracted_data/{unique_id}.zip\"\n",
    "\n",
    "    @classmethod\n",
    "    def create_with_unique_id(cls, file_path):\n",
    "        instance = cls(file_path)\n",
    "        return instance, instance.unique_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the extracted data from the extractor\n",
    "def get_extracted_data(extracted_data):\n",
    "    if 'structuredData.json' in extracted_data:\n",
    "        json_data = json.loads(extracted_data['structuredData.json'])\n",
    "    return json_data\n",
    "\n",
    "# Function to initialise a flexible text splitter\n",
    "def initialise_text_splitter(chunk_size, chunk_overlap):\n",
    "    separator_list = [\"\\n\\n\", \"\\n\", \". \", \"!\", \"?\", \",\", \" \", \"\", \")\", \"(\"]\n",
    "    try:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=chunk_overlap,\n",
    "                length_function=len,\n",
    "                separators=separator_list)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to initialize text splitter: {e}\")\n",
    "        return None\n",
    "    return text_splitter\n",
    "\n",
    "# Function to obtain text chunks using the text splitter\n",
    "def get_text_chunks(file_path, json_data):\n",
    "    if 'elements' not in json_data:\n",
    "        logging.error(\"Missing 'elements' key in json_data\")\n",
    "        raise ValueError(\"Missing 'elements' key in json_data\")\n",
    "        \n",
    "    file_name = file_path.split(\"/\")[-1]\n",
    "\n",
    "    # Chunks are split by pages here\n",
    "    page_text = \"\"\n",
    "    start_page = 0\n",
    "    \n",
    "    all_texts = []\n",
    "    \n",
    "    list_label = \"\"\n",
    "    for i in range(len(json_data['elements'])):\n",
    "        try:\n",
    "            current_page = json_data['elements'][i]['Page']\n",
    "        except KeyError:\n",
    "            logging.warning(f\"Missing 'Page' key in element at index {i}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            if current_page > start_page:\n",
    "                # Update the new page number\n",
    "                start_page = current_page               \n",
    "         \n",
    "                all_texts.append({'ElementType': 'Text', 'file_name': file_name, 'Page': current_page, 'Text': page_text})\n",
    "                page_text = \"\"\n",
    "                list_label = \"\"\n",
    "            else:\n",
    "                if 'Text' in json_data['elements'][i]:  # Check if Text is not empty\n",
    "                    if json_data['elements'][i]['Path'].endswith(\"Lbl\") and not json_data['elements'][i]['Path'].startswith(\"//Document/Table\"):\n",
    "                        list_label = json_data['elements'][i]['Text']\n",
    "                    else:\n",
    "                        if list_label:\n",
    "                            page_text += list_label + json_data['elements'][i]['Text']\n",
    "                            list_label = \"\"  # Reset list label to empty string\n",
    "                        else:\n",
    "                            page_text += json_data['elements'][i]['Text'] + \"\\n\"\n",
    "        except KeyError as e:\n",
    "            logging.warning(f\"Key error in json_data['elements'][i] processing at index {i}: {e}\")\n",
    "    \n",
    "    \n",
    "    # Processing the last page of the text\n",
    "    if page_text:\n",
    "        all_texts.append({'ElementType': 'Text', 'file_name': file_name, 'Page': current_page, 'Text': page_text})\n",
    "\n",
    "    return all_texts\n",
    "\n",
    "# Function to derive the nodes from the text chunks\n",
    "def convert_pagetexts_to_nodes(text_chunks):\n",
    "    \n",
    "    # Function to clean up the text in each node\n",
    "    def clean_up_text(content: str) -> str:\n",
    "        \"\"\"\n",
    "        Remove unwanted characters and patterns in text input.\n",
    "        :param content: Text input.\n",
    "        :return: Cleaned version of original text input.\n",
    "        \"\"\"\n",
    "\n",
    "        # Fix hyphenated words broken by newline\n",
    "        content = re.sub(r'(\\w+)-\\n(\\w+)', r'\\1\\2', content)\n",
    "\n",
    "        # Remove specific unwanted patterns and characters\n",
    "        unwanted_patterns = [\n",
    "            \"\\\\n\", \"  —\", \"——————————\", \"—————————\", \"—————\",\n",
    "            r'\\\\u[\\dA-Fa-f]{4}', r'\\uf075', r'\\uf0b7'\n",
    "        ]\n",
    "        for pattern in unwanted_patterns:\n",
    "            content = re.sub(pattern, \"\", content)\n",
    "\n",
    "        # Fix improperly spaced hyphenated words and normalize whitespace\n",
    "        content = re.sub(r'(\\w)\\s*-\\s*(\\w)', r'\\1-\\2', content)\n",
    "        content = re.sub(r'\\s+', ' ', content)\n",
    "        return content\n",
    "    \n",
    "    # Conversion of text chunks to Documents\n",
    "    page_documents = [Document(text=chunk['Text'],\n",
    "                            metadata={\n",
    "                                \"file_name\": chunk['file_name'],\n",
    "                                \"page\": chunk['Page']\n",
    "                                },\n",
    "                            excluded_llm_metadata_keys=[\"file_name\"],\n",
    "                            metadata_seperator=\"::\",\n",
    "                            metadata_template=\"{key}=>{value}\",\n",
    "                            text_template=\"Metadata: {metadata_str}\\n-----\\nContent: {content}\")\n",
    "                            \n",
    "                            for chunk in text_chunks]\n",
    "\n",
    "    # Clean the texts in each page\n",
    "    page_nodes = []\n",
    "    for d in page_documents:\n",
    "        cleaned_text = clean_up_text(d.text)\n",
    "        d.text = cleaned_text\n",
    "        page_nodes.append(d)\n",
    "    return page_nodes\n",
    "\n",
    "# Function to get the unique tables from all the table elements\n",
    "def extract_unique_tables(table_elements):\n",
    "    tables = set()\n",
    "    for item in table_elements:\n",
    "        match = re.search(r'/Table(\\[\\d+\\])?', item['Path'])\n",
    "        if match:\n",
    "            tables.add('Table' + (match.group(1) if match.group(1) else ''))\n",
    "    \n",
    "    unique_tables = list(tables)\n",
    "    unique_tables[0] += \"/\"\n",
    "\n",
    "    extracted_tables = {}\n",
    "    i=0\n",
    "    for table_name in unique_tables:\n",
    "        table = []\n",
    "        for el in table_elements:\n",
    "            if table_name in el['Path']:\n",
    "                # ADjust this here if u need to extract more information from the table elements\n",
    "                table.append({\"path\": el['Path'], \"text\": el['Text'], \"Page\": el[\"Page\"]})\n",
    "        extracted_tables[i+1] = table\n",
    "        i += 1\n",
    "\n",
    "    return dict(sorted(extracted_tables.items()))\n",
    "\n",
    "# Function to take in table elements from a specific table and convert it to a pandas dataframe\n",
    "def transform_table(table, output_file_path):\n",
    "    # Function to decide if table has 2 indexes\n",
    "    def is_2_index(first_row):\n",
    "        for el in first_row:\n",
    "            if \"TH/\" in el['path']:\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "    \n",
    "    # Function to decide if the table's row is the header   \n",
    "    def is_row_header(first_row):\n",
    "        for el in first_row:\n",
    "            if \"TH\" not in el['path']:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    # Function to get the table dimensions\n",
    "    def get_table_dim(table):\n",
    "        # Regular expressions to find row and column indices\n",
    "        row_regex = re.compile(r'/TR\\[(\\d+)\\]')\n",
    "        col_regex = re.compile(r'/TD\\[(\\d+)\\]')\n",
    "\n",
    "        max_row = 0\n",
    "        max_col = 0\n",
    "\n",
    "        for entry in table:\n",
    "            path = entry['path']  # Extract path from the dictionary\n",
    "            \n",
    "            # Extract row and column indices\n",
    "            row_match = row_regex.search(path)\n",
    "            col_match = col_regex.search(path)\n",
    "            \n",
    "            # Update max row index\n",
    "            if row_match:\n",
    "                row_index = int(row_match.group(1))\n",
    "                max_row = max(max_row, row_index)\n",
    "            \n",
    "            # Update max column index\n",
    "            if col_match:\n",
    "                col_index = int(col_match.group(1))\n",
    "                max_col = max(max_col, col_index)\n",
    "            elif '/TD/' in path and not col_match:  # Check for default first column\n",
    "                max_col = max(max_col, 1)  # Ensure first column is counted\n",
    "        return (max_row, max_col)    \n",
    "\n",
    "    # Function to check if the API could not identify the index of the table\n",
    "    def is_unidentified(table):\n",
    "        table_dim = get_table_dim(table)\n",
    "        min_headers = min(table_dim)\n",
    "        header_count = len([el for el in table if \"TH\" in el['path']])\n",
    "        if header_count < min_headers:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    # Function to transform the unidentified table by determining its index using LLM\n",
    "    def transform_unidentified_table(table):\n",
    "        # Function to get the first 3 rows \n",
    "        def get_first_three_rows(table):\n",
    "                # Regular expression to extract row index\n",
    "                row_regex = re.compile(r'/TR\\[(\\d+)\\]')\n",
    "                \n",
    "                # List to hold the first three rows table\n",
    "                rows = []\n",
    "                \n",
    "                for entry in table:\n",
    "                    path = entry['path']\n",
    "                    # Find row index\n",
    "                    row_match = row_regex.search(path)\n",
    "                    \n",
    "                    if row_match:\n",
    "                        row_index = int(row_match.group(1))\n",
    "                        # Check if the row index is within the first three rows\n",
    "                        if 1 <= row_index <= 3:\n",
    "                            rows.append(entry)\n",
    "                    elif '/TR/' in path:  # Check for paths that imply the first row implicitly\n",
    "                        rows.append(entry)\n",
    "            \n",
    "                table_first_3_rows = [] # This will be a 2D list\n",
    "                table_rows = set([re.search(r'Table(?:\\[\\d+\\])?/TR(?:\\[\\d+\\])?', item['path']).group() for item in table if re.search(r'Table(?:\\[\\d+\\])?/TR(?:\\[\\d+\\])?', item['path'])])\n",
    "                uniq_rows = sorted([row_name.split(\"/\")[-1] for row_name in list(table_rows)[:3]])\n",
    "                uniq_rows[0] += \"/\"\n",
    "\n",
    "\n",
    "                for i in range(len(uniq_rows)):\n",
    "                    row = [el for el in table if uniq_rows[i] in el['path']]\n",
    "                    unique_tds = set()\n",
    "                    for item in row:\n",
    "                                path_parts = item['path'].split('/')\n",
    "                                for part in path_parts:\n",
    "                                    if 'TD' in part:\n",
    "                                        unique_tds.add(part)\n",
    "                    # Convert the set to a list and sort it for consistent output\n",
    "                    unique_tds_list = sorted(list(unique_tds))\n",
    "                            \n",
    "                    sections_of_row = []\n",
    "                    for td in unique_tds_list:\n",
    "                                td_section = \"\"\n",
    "                                for i in range(len(row)):\n",
    "                                    if i ==0:\n",
    "                                        td += \"/\"\n",
    "                                    if td in row[i]['path']:\n",
    "                                        td_section += row[i]['text'].strip()\n",
    "\n",
    "                                sections_of_row.append(td_section)\n",
    "                    \n",
    "                    # print(sections_of_row)\n",
    "                    table_first_3_rows.append(sections_of_row)\n",
    "\n",
    "                # account for case where there are 2 index columns:\n",
    "                min_dim = max(list(map(lambda x: len(x), table_first_3_rows)))\n",
    "                if len(table_first_3_rows[0]) < min_dim:\n",
    "                    table_first_3_rows[0].insert(0, \"empty\")\n",
    "                return table_first_3_rows\n",
    "\n",
    "        # Function to get the first 3 rows of the table into a string to parse to the LLM \n",
    "        def get_table_check_string(table_first_3_rows):\n",
    "            table_str = ''\n",
    "            for i in range(len(table_first_3_rows)):\n",
    "                table_str += f'''Row{i+1}: {\" | \".join(table_first_3_rows[i])}\\n'''\n",
    "            return table_str\n",
    "\n",
    "        # Function to clean the output if it decides to explain its choice.\n",
    "        def clean_llama_output_if_string(input_str):\n",
    "            \"\"\"\n",
    "            Extracts a dictionary-like substring from the given input string.\n",
    "            \n",
    "            Args:\n",
    "            input_str (str): The input string containing the dictionary-like substring.\n",
    "            \n",
    "            Returns:\n",
    "            dict: The extracted dictionary as a Python dictionary.\n",
    "            \"\"\"\n",
    "            # Use a regular expression to find the dictionary-like substring\n",
    "            match = re.search(r'\\{.*?\\}', input_str)\n",
    "            \n",
    "            if match:\n",
    "                dict_str = match.group()\n",
    "                try:\n",
    "                    # Safely evaluate the dictionary string\n",
    "                    extracted_dict = eval(dict_str)\n",
    "                    if isinstance(extracted_dict, dict):\n",
    "                        return extracted_dict\n",
    "                except (SyntaxError, NameError):\n",
    "                    pass\n",
    "            \n",
    "            return None\n",
    "\n",
    "        # Function to decide the indexing of the table\n",
    "        def eval_table_index_llama(table_str):\n",
    "            class Header(BaseModel):\n",
    "                index: int = Field(description=(\n",
    "                    '''Index of the table indicating the type of indexing:\\n\\\n",
    "                        0 - No indexes\\n\\\n",
    "                        1 - Indexed by the first column\\n\\\n",
    "                        2 - Indexed by the first row\\n\\\n",
    "                        3 - Indexed by both the first row and the first column'''))\n",
    "\n",
    "            parser = JsonOutputParser(pydantic_object=Header)\n",
    "\n",
    "            chat = ChatGroq(temperature=0, model_name=\"llama3-8b-8192\")\n",
    "            \n",
    "            template = '''You will assist me in deciding, based on the first 3 entries of a table, whether the first row or the first colum should be the header, or if both the first column and first row are headers\n",
    "        Table:\n",
    "        {table}\n",
    "\n",
    "        Determine the indexing type of the table and output:\n",
    "        - 0 if the table is indexed by the first column.\n",
    "        - 1 if the table is indexed by the first row.\n",
    "        - 2 if the table is indexed by both the first row and the first column.   \n",
    "                    \n",
    "        {format_instructions}'''\n",
    "            prompt = PromptTemplate(\n",
    "                template=template,\n",
    "                input_variables=[\"table\"],\n",
    "                partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "            )\n",
    "                \n",
    "            chain = prompt | chat | parser\n",
    "            return chain.invoke({\"table\": table_str})\n",
    "        \n",
    "        table_first_3_rows = get_first_three_rows(table)\n",
    "        table_str = get_table_check_string(table_first_3_rows)\n",
    "        header_output = eval_table_index_llama(table_str)\n",
    "        if isinstance(header_output, str):\n",
    "            header_output = clean_llama_output_if_string(header_output)\n",
    "        header_index = header_output['index']\n",
    "        # Now need diff cases of how to handle the table if the model outputs, 0,1, or 2\n",
    "            # where 0 is the first col, 1 is second col, 2 is for both cols are headers\n",
    "        pass\n",
    "    \n",
    "    # If the table cannot be identified by the ExtractTable API\n",
    "    if is_unidentified(table):\n",
    "        processed_unidentified_table = transform_unidentified_table(table)\n",
    "        pass\n",
    "    \n",
    "    # only need to look at the first row\n",
    "    first_row = [el for el in table if \"TR/\" in el['path']]\n",
    "    \n",
    "    # If the table has 2 indexes\n",
    "    if is_2_index(first_row):\n",
    "            print(\"This table has 2 indexes\")\n",
    "            # Function to produce table which has 2 indexes\n",
    "            def get_2_index_table(table):\n",
    "                table_rows = set([re.search(r'Table(?:\\[\\d+\\])?/TR(?:\\[\\d+\\])?', item['path']).group() for item in table if re.search(r'Table(?:\\[\\d+\\])?/TR(?:\\[\\d+\\])?', item['path'])])\n",
    "                uniq_rows = sorted([row_name.split(\"/\")[-1] for row_name in list(table_rows)])\n",
    "\n",
    "                # Get indexes from the first row:\n",
    "                row_indexes = [el['text'].strip() for el in table if \"TR/TH\" in el['path']]\n",
    "\n",
    "                data = {}\n",
    "\n",
    "                # Only look at second row onwards\n",
    "                for i in range(1,len(uniq_rows)):\n",
    "                    row_name = uniq_rows[i]\n",
    "                    row = [el for el in table if row_name in el['path']]\n",
    "                    row_key = row[0]['text'].strip()\n",
    "                    \n",
    "                    unique_tds = set()\n",
    "                    for item in row:\n",
    "                        path_parts = item['path'].split('/')\n",
    "                        for part in path_parts:\n",
    "                            if 'TD' in part:\n",
    "                                unique_tds.add(part)\n",
    "                    # Convert the set to a list and sort it for consistent output\n",
    "                    unique_tds_list = sorted(list(unique_tds))\n",
    "\n",
    "                    sections_of_row = []\n",
    "                    for td in unique_tds_list:\n",
    "                        td_section = \"\"\n",
    "                        for i in range(len(row)):\n",
    "                            if i ==0:\n",
    "                                td += \"/\"\n",
    "                            if td in row[i]['path']:\n",
    "                                td_section += row[i]['text'].strip()\n",
    "                                \n",
    "                        sections_of_row.append(td_section)\n",
    "                    \n",
    "                    data[row_key] = sections_of_row\n",
    "                    \n",
    "                df = pd.DataFrame(data, index=row_indexes).T\n",
    "                df.to_csv(output_file_path)\n",
    "                return df    \n",
    "            \n",
    "            df = get_2_index_table(table)\n",
    "            return df\n",
    "        \n",
    "    # If the table only has one index\n",
    "    else:\n",
    "            print(\"This table has 1 index\")\n",
    "\n",
    "            # If the header for this df is the row\n",
    "            if is_row_header(first_row):\n",
    "                print(\"This is a row indexed table\")\n",
    "                def get_row_header_table(table):\n",
    "                    table_rows = set([re.search(r'Table(?:\\[\\d+\\])?/TR(?:\\[\\d+\\])?', item['path']).group() for item in table if re.search(r'Table(?:\\[\\d+\\])?/TR(?:\\[\\d+\\])?', item['path'])])\n",
    "                    uniq_rows = sorted([row_name.split(\"/\")[-1] for row_name in list(table_rows)])\n",
    "\n",
    "                    # Table headers, also the keys\n",
    "                    headers = [el['text'].strip() for el in table if uniq_rows[0]+\"/\" in el['path']]\n",
    "                    rows = []\n",
    "                    for i in range(1,len(uniq_rows)):\n",
    "                        row_name = uniq_rows[i]\n",
    "                        row = [el for el in table if row_name in el['path']]\n",
    "                        # rows.append(row)\n",
    "                        \n",
    "                        unique_tds = set()\n",
    "                        for item in row:\n",
    "                            path_parts = item['path'].split('/')\n",
    "                            for part in path_parts:\n",
    "                                if 'TD' in part:\n",
    "                                    unique_tds.add(part)\n",
    "                        # Convert the set to a list and sort it for consistent output\n",
    "                        unique_tds_list = sorted(list(unique_tds))\n",
    "\n",
    "                        sections_of_row = []\n",
    "                        for td in unique_tds_list:\n",
    "                            td_section = \"\"\n",
    "                            for i in range(len(row)):\n",
    "                                if i ==0:\n",
    "                                    td += \"/\"\n",
    "                                if td in row[i]['path']:\n",
    "                                    td_section += row[i]['text'].strip()\n",
    "                                    \n",
    "                            sections_of_row.append(td_section)\n",
    "                        rows.append(sections_of_row)\n",
    "                                    \n",
    "                    df = pd.DataFrame(rows, columns=headers)\n",
    "                    return df\n",
    "                \n",
    "                df = get_row_header_table(table)\n",
    "                df.to_csv(output_file_path, index=False)\n",
    "                return df\n",
    "                    \n",
    "            # The header for this df is the column\n",
    "            else:\n",
    "                print(\"This table is a column indexed table...\")\n",
    "                def get_column_header_table(table):\n",
    "                    table_rows = set([re.search(r'Table(?:\\[\\d+\\])?/TR(?:\\[\\d+\\])?', item['path']).group() for item in table if re.search(r'Table(?:\\[\\d+\\])?/TR(?:\\[\\d+\\])?', item['path'])])\n",
    "                    uniq_rows = sorted([row_name.split(\"/\")[-1] for row_name in list(table_rows)])\n",
    "\n",
    "                    data = {}\n",
    "\n",
    "                    for i in range(len(uniq_rows)):\n",
    "                                        row_name = uniq_rows[i]\n",
    "                                        if i == 0:\n",
    "                                            row = [el for el in table if row_name+\"/\" in el['path']]\n",
    "                                        else:\n",
    "                                            row = [el for el in table if row_name in el['path']]\n",
    "                                        \n",
    "                                        unique_tds = set()\n",
    "                                        for item in row:\n",
    "                                            path_parts = item['path'].split('/')\n",
    "                                            for part in path_parts:\n",
    "                                                if 'TD' in part:\n",
    "                                                    unique_tds.add(part)\n",
    "                                        # Convert the set to a list and sort it for consistent output\n",
    "                                        unique_tds_list = sorted(list(unique_tds))\n",
    "                                        \n",
    "                                        sections_of_row = []\n",
    "                                        for td in unique_tds_list:\n",
    "                                            td_section = \"\"\n",
    "                                            for i in range(len(row)):\n",
    "                                                if i ==0:\n",
    "                                                    td += \"/\"\n",
    "                                                if td in row[i]['path']:\n",
    "                                                    td_section += row[i]['text'].strip()\n",
    "                                                    \n",
    "                                            sections_of_row.append(td_section)\n",
    "                                                    \n",
    "                                        row_key = row[0]['text'].strip()\n",
    "                                        data[row_key] = sections_of_row\n",
    "                    df = pd.DataFrame.from_dict(data, orient='index').T\n",
    "                    return df\n",
    "                                    \n",
    "                df = get_column_header_table(table)\n",
    "                df.to_csv(output_file_path, index=False)\n",
    "                return df\n",
    "\n",
    "# Function that saves the tables in CSV format to a unique job id       \n",
    "def save_tables_to_csv(extracted_tables, table_output_directory):\n",
    "    \n",
    "    os.makedirs(table_output_directory, exist_ok=True)\n",
    "    for table_num, table in extracted_tables.items():\n",
    "        output_file_path = os.path.join(table_output_directory, f\"table_{table_num}.csv\")\n",
    "        df = transform_table(table, output_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runing the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../PDF/Invest First Max Summary.pdf'\n",
    "# extractor, unique_id = ExtractTextTableInfoFromPDF.create_with_unique_id(file_path)\n",
    "# extracted_data = extractor.extracted_data\n",
    "# pdf_data = get_extracted_data(extracted_data)\n",
    "fname = file_path.split(\"/\")[-1]\n",
    "# with open(f\"../data/{fname}.json\", \"w\", encoding=\"utf-8\") as fout:\n",
    "    # json.dump(pdf_data, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 22:42:05,019 - WARNING - Missing 'Page' key in element at index 217\n",
      "2024-07-05 22:42:05,020 - WARNING - Missing 'Page' key in element at index 218\n",
      "2024-07-05 22:42:05,020 - WARNING - Missing 'Page' key in element at index 219\n",
      "2024-07-05 22:42:05,020 - WARNING - Missing 'Page' key in element at index 220\n",
      "2024-07-05 22:42:05,021 - WARNING - Missing 'Page' key in element at index 221\n",
      "2024-07-05 22:42:05,021 - WARNING - Missing 'Page' key in element at index 222\n",
      "2024-07-05 22:42:05,021 - WARNING - Missing 'Page' key in element at index 223\n",
      "2024-07-05 22:42:05,022 - WARNING - Missing 'Page' key in element at index 224\n",
      "2024-07-05 22:42:05,022 - WARNING - Missing 'Page' key in element at index 225\n",
      "2024-07-05 22:42:05,022 - WARNING - Missing 'Page' key in element at index 226\n",
      "2024-07-05 22:42:05,023 - WARNING - Missing 'Page' key in element at index 227\n",
      "2024-07-05 22:42:05,023 - WARNING - Missing 'Page' key in element at index 228\n",
      "2024-07-05 22:42:05,023 - WARNING - Missing 'Page' key in element at index 229\n",
      "2024-07-05 22:42:05,024 - WARNING - Missing 'Page' key in element at index 232\n",
      "2024-07-05 22:42:05,024 - WARNING - Missing 'Page' key in element at index 233\n",
      "2024-07-05 22:42:05,024 - WARNING - Missing 'Page' key in element at index 234\n",
      "2024-07-05 22:42:05,025 - WARNING - Missing 'Page' key in element at index 237\n",
      "2024-07-05 22:42:05,025 - WARNING - Missing 'Page' key in element at index 238\n",
      "2024-07-05 22:42:05,025 - WARNING - Missing 'Page' key in element at index 239\n",
      "2024-07-05 22:42:05,025 - WARNING - Missing 'Page' key in element at index 242\n",
      "2024-07-05 22:42:05,026 - WARNING - Missing 'Page' key in element at index 245\n",
      "2024-07-05 22:42:05,026 - WARNING - Missing 'Page' key in element at index 246\n",
      "2024-07-05 22:42:05,026 - WARNING - Missing 'Page' key in element at index 247\n",
      "2024-07-05 22:42:05,027 - WARNING - Missing 'Page' key in element at index 248\n",
      "2024-07-05 22:42:05,027 - WARNING - Missing 'Page' key in element at index 249\n",
      "2024-07-05 22:42:05,027 - WARNING - Missing 'Page' key in element at index 250\n",
      "2024-07-05 22:42:05,027 - WARNING - Missing 'Page' key in element at index 251\n",
      "2024-07-05 22:42:05,028 - WARNING - Missing 'Page' key in element at index 252\n",
      "2024-07-05 22:42:05,028 - WARNING - Missing 'Page' key in element at index 253\n",
      "2024-07-05 22:42:05,028 - WARNING - Missing 'Page' key in element at index 256\n",
      "2024-07-05 22:42:05,028 - WARNING - Missing 'Page' key in element at index 257\n",
      "2024-07-05 22:42:05,029 - WARNING - Missing 'Page' key in element at index 258\n",
      "2024-07-05 22:42:05,029 - WARNING - Missing 'Page' key in element at index 261\n",
      "2024-07-05 22:42:05,029 - WARNING - Missing 'Page' key in element at index 262\n",
      "2024-07-05 22:42:05,030 - WARNING - Missing 'Page' key in element at index 263\n",
      "2024-07-05 22:42:05,030 - WARNING - Missing 'Page' key in element at index 264\n",
      "2024-07-05 22:42:05,030 - WARNING - Missing 'Page' key in element at index 265\n",
      "2024-07-05 22:42:05,031 - WARNING - Missing 'Page' key in element at index 290\n",
      "2024-07-05 22:42:05,031 - WARNING - Missing 'Page' key in element at index 531\n",
      "2024-07-05 22:42:05,031 - WARNING - Missing 'Page' key in element at index 561\n",
      "2024-07-05 22:42:05,032 - WARNING - Missing 'Page' key in element at index 748\n",
      "2024-07-05 22:42:05,032 - WARNING - Missing 'Page' key in element at index 749\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/Invest First Max Summary.pdf.json\", \"r\", encoding='utf-8') as fin:\n",
    "    pdf_data = json.load(fin)\n",
    "    \n",
    "def extract_tables_from_pdf(pdf_data):\n",
    "    table_elements = [el for el in pdf_data['elements'] if \"Table\" in el['Path'] and 'Text' in el and \"TR\" in el['Path']]\n",
    "    print(\"\\nUnique ID:\", unique_id)\n",
    "\n",
    "    # IF there are even any table elements in the PDF\n",
    "    if table_elements:\n",
    "        table_output_directory = f\"../data/{unique_id}\" \n",
    "        extracted_tables = extract_unique_tables(table_elements)    \n",
    "        save_tables_to_csv(extracted_tables, table_output_directory)\n",
    "\n",
    "# Section here to derive the csvs from the table elements\n",
    "# Uncomment to extract tables (for now its not ready yet)\n",
    "# extract_tables_from_pdf(pdf_data)\n",
    "\n",
    "# Section here to derive the nodes from the text elements\n",
    "# text_splitter = initialise_text_splitter(1000, 50)\n",
    "page_texts = get_text_chunks(file_path, pdf_data)\n",
    "page_nodes = convert_pagetexts_to_nodes(page_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up ingestion pipeline to pipe data into pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 22:42:11,963 - INFO - Embedding model loaded...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbedding(model_name='sentence-transformers/all-mpnet-base-v2', embed_batch_size=10, callback_manager=<llama_index.legacy.callbacks.base.CallbackManager object at 0x32a8dac20>, tokenizer_name='sentence-transformers/all-mpnet-base-v2', max_length=514, pooling=<Pooling.MEAN: 'mean'>, normalize=True, query_instruction=None, text_instruction=None, cache_folder=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name='sentence-transformers/all-mpnet-base-v2')\n",
    "if embed_model:\n",
    "    logging.info(\"Embedding model loaded...\")\n",
    "embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the initial pipeline\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SemanticSplitterNodeParser(\n",
    "            buffer_size=1,\n",
    "            breakpoint_percentile_threshold=95,\n",
    "            embed_model=embed_model,  # Pass the dictionary instead of the raw instance\n",
    "        ),\n",
    "        embed_model,  # Use the model instance directly in the pipeline\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize connection to Pinecone\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "unique_id = \"zac-notes\"\n",
    "index_name = unique_id\n",
    "\n",
    "# Create your index (can skip this step if your index already exists)\n",
    "hybrid_search=True\n",
    "if hybrid_search:\n",
    "    pc_similarity_metric = \"dotproduct\"\n",
    "else:\n",
    "    pc_similarity_metric = \"cosine\"\n",
    "\n",
    "# pc.create_index(\n",
    "#     index_name,\n",
    "#     dimension=768,\n",
    "#     metric=pc_similarity_metric,\n",
    "#     spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "# )\n",
    "\n",
    "# Initialize your index\n",
    "pinecone_index = pc.Index(index_name)\n",
    "\n",
    "# Initialize VectorStore\n",
    "vector_store = PineconeVectorStore(pinecone_index=pinecone_index, llm=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the vector store index into the insertion pipeline\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SemanticSplitterNodeParser(\n",
    "            buffer_size=1,\n",
    "            breakpoint_percentile_threshold=95,\n",
    "            embed_model=embed_model,\n",
    "            ),\n",
    "        embed_model,\n",
    "        ],\n",
    "        vector_store=vector_store  # Our new addition\n",
    "    )\n",
    "\n",
    "# Pushing the first 3 pages of PDF to the pinecone index. \n",
    "first_3_pages = page_nodes[:3]\n",
    "# pipeline.run(documents=first_3_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of embeddings in this index: 768\n",
      "Index fullness is at 0.0\n",
      "Namespaces in this index: {'': {'vector_count': 6}}\n",
      "Total vector count at 6\n"
     ]
    }
   ],
   "source": [
    "def describe_pinecone_index():\n",
    "    pc_stats = pinecone_index.describe_index_stats()\n",
    "    print(f\"Dimension of embeddings in this index: {pc_stats['dimension']}\")\n",
    "    print(f\"Index fullness is at {pc_stats['index_fullness']}\")\n",
    "    print(f\"Namespaces in this index: {pc_stats['namespaces']}\")\n",
    "    print(f\"Total vector count at {pc_stats['total_vector_count']}\")\n",
    "    \n",
    "describe_pinecone_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different User Query Types:\n",
    "User queries in an RAG application vary based on individual intent. For these diverse query types, it’s essential to fine-tune the Alpha parameter. This process involves routing each user query to a specific Alpha value for effective retrieval and response synthesis. Microsoft has identified various user query categories, and we have selected a few for tuning our hybrid search. Below are the different user query types we considered:\n",
    "\n",
    "- Web Search Queries: Brief queries similar to those typically inputted into search engines.\n",
    "- Concept Seeking Queries: Abstract questions that necessitate detailed, multi-sentence answers.\n",
    "- Fact Seeking Queries: Queries that have a single, definitive answer.\n",
    "- Keyword Queries: Concise queries composed solely of crucial identifier words.\n",
    "- Queries With Misspellings: Queries containing typos, transpositions, and common misspellings.\n",
    "- Exact Sub-string Searches: Queries that exactly match sub-strings from the original context.\n",
    "\n",
    "Tuning the alpha value to adjust how much of vector and how much of semantic search we should be using:\n",
    "- https://medium.com/llamaindex-blog/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Groq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Instantiate VectorStoreIndex object from our vector_store object\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mGroq\u001b[49m(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama3-70b-8192\u001b[39m\u001b[38;5;124m\"\u001b[39m, api_key\u001b[38;5;241m=\u001b[39mgroq_api_key)\n\u001b[1;32m      6\u001b[0m response \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mcomplete(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExplain the importance of low latency LLMs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m response\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Groq' is not defined"
     ]
    }
   ],
   "source": [
    "from llama_index.legacy.retrievers import VectorIndexRetriever\n",
    "# Instantiate VectorStoreIndex object from our vector_store object\n",
    "import llama_index.legacy.llms\n",
    "\n",
    "llm = ChatGroq(model=\"llama3-70b-8192\", api_key=groq_api_key)\n",
    "\n",
    "vector_index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n",
    "\n",
    "# Grab 5 search results\n",
    "retriever = VectorIndexRetriever(index=vector_index, similarity_top_k=5)\n",
    "\n",
    "# # Query vector DB\n",
    "answer = retriever.retrieve('What is FWD Invest First max about?')\n",
    "for ans in answer:\n",
    "    print(ans)\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuation of table extraction (For unidentified tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the table here has the TH's replaced with TD's\n",
    "table = extracted_tables[1]\n",
    "# Replace every 'TH' with 'TD' in the 'path' key\n",
    "for item in table:\n",
    "    item['path'] = item['path'].replace('TH', 'TD')\n",
    "\n",
    "# only need to look at the first row\n",
    "first_row = [el for el in table if \"TR[2]/\" in el['path']]\n",
    "    # Function to decide if table has 2 indexes\n",
    "\n",
    "def get_table_dim(table):\n",
    "    # Regular expressions to find row and column indices\n",
    "    row_regex = re.compile(r'/TR\\[(\\d+)\\]')\n",
    "    col_regex = re.compile(r'/TD\\[(\\d+)\\]')\n",
    "\n",
    "    max_row = 0\n",
    "    max_col = 0\n",
    "\n",
    "    for entry in table:\n",
    "        path = entry['path']  # Extract path from the dictionary\n",
    "        \n",
    "        # Extract row and column indices\n",
    "        row_match = row_regex.search(path)\n",
    "        col_match = col_regex.search(path)\n",
    "        \n",
    "        # Update max row index\n",
    "        if row_match:\n",
    "            row_index = int(row_match.group(1))\n",
    "            max_row = max(max_row, row_index)\n",
    "        \n",
    "        # Update max column index\n",
    "        if col_match:\n",
    "            col_index = int(col_match.group(1))\n",
    "            max_col = max(max_col, col_index)\n",
    "        elif '/TD/' in path and not col_match:  # Check for default first column\n",
    "            max_col = max(max_col, 1)  # Ensure first column is counted\n",
    "    return (max_row, max_col)    \n",
    "\n",
    "# Function to check if the API could not identify the index of the table\n",
    "def is_unidentified(table):\n",
    "    table_dim = get_table_dim(table)\n",
    "    min_headers = min(table_dim)\n",
    "    header_count = len([el for el in table if \"TH\" in el['path']])\n",
    "    if header_count < min_headers:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Function to transform the unidentified table by determining its index using LLM\n",
    "def transform_unidentified_table(table):\n",
    "    # Function to get the first 3 rows \n",
    "    def get_first_three_rows(table):\n",
    "            # Regular expression to extract row index\n",
    "            row_regex = re.compile(r'/TR\\[(\\d+)\\]')\n",
    "            \n",
    "            # List to hold the first three rows table\n",
    "            rows = []\n",
    "            \n",
    "            for entry in table:\n",
    "                path = entry['path']\n",
    "                # Find row index\n",
    "                row_match = row_regex.search(path)\n",
    "                \n",
    "                if row_match:\n",
    "                    row_index = int(row_match.group(1))\n",
    "                    # Check if the row index is within the first three rows\n",
    "                    if 1 <= row_index <= 3:\n",
    "                        rows.append(entry)\n",
    "                elif '/TR/' in path:  # Check for paths that imply the first row implicitly\n",
    "                    rows.append(entry)\n",
    "        \n",
    "            table_first_3_rows = [] # This will be a 2D list\n",
    "            table_rows = set([re.search(r'Table(?:\\[\\d+\\])?/TR(?:\\[\\d+\\])?', item['path']).group() for item in table if re.search(r'Table(?:\\[\\d+\\])?/TR(?:\\[\\d+\\])?', item['path'])])\n",
    "            uniq_rows = sorted([row_name.split(\"/\")[-1] for row_name in list(table_rows)[:3]])\n",
    "            uniq_rows[0] += \"/\"\n",
    "\n",
    "\n",
    "            for i in range(len(uniq_rows)):\n",
    "                row = [el for el in table if uniq_rows[i] in el['path']]\n",
    "                unique_tds = set()\n",
    "                for item in row:\n",
    "                            path_parts = item['path'].split('/')\n",
    "                            for part in path_parts:\n",
    "                                if 'TD' in part:\n",
    "                                    unique_tds.add(part)\n",
    "                # Convert the set to a list and sort it for consistent output\n",
    "                unique_tds_list = sorted(list(unique_tds))\n",
    "                        \n",
    "                sections_of_row = []\n",
    "                for td in unique_tds_list:\n",
    "                            td_section = \"\"\n",
    "                            for i in range(len(row)):\n",
    "                                if i ==0:\n",
    "                                    td += \"/\"\n",
    "                                if td in row[i]['path']:\n",
    "                                    td_section += row[i]['text'].strip()\n",
    "\n",
    "                            sections_of_row.append(td_section)\n",
    "                \n",
    "                # print(sections_of_row)\n",
    "                table_first_3_rows.append(sections_of_row)\n",
    "\n",
    "            # account for case where there are 2 index columns:\n",
    "            min_dim = max(list(map(lambda x: len(x), table_first_3_rows)))\n",
    "            if len(table_first_3_rows[0]) < min_dim:\n",
    "                table_first_3_rows[0].insert(0, \"empty\")\n",
    "            return table_first_3_rows\n",
    "\n",
    "    # Function to get the first 3 rows of the table into a string to parse to the LLM \n",
    "    def get_table_check_string(table_first_3_rows):\n",
    "        table_str = ''\n",
    "        for i in range(len(table_first_3_rows)):\n",
    "            table_str += f'''Row{i+1}: {\" | \".join(table_first_3_rows[i])}\\n'''\n",
    "        return table_str\n",
    "\n",
    "    # Function to clean the output if it decides to explain its choice.\n",
    "    def clean_llama_output_if_string(input_str):\n",
    "        \"\"\"\n",
    "        Extracts a dictionary-like substring from the given input string.\n",
    "        \n",
    "        Args:\n",
    "        input_str (str): The input string containing the dictionary-like substring.\n",
    "        \n",
    "        Returns:\n",
    "        dict: The extracted dictionary as a Python dictionary.\n",
    "        \"\"\"\n",
    "        # Use a regular expression to find the dictionary-like substring\n",
    "        match = re.search(r'\\{.*?\\}', input_str)\n",
    "        \n",
    "        if match:\n",
    "            dict_str = match.group()\n",
    "            try:\n",
    "                # Safely evaluate the dictionary string\n",
    "                extracted_dict = eval(dict_str)\n",
    "                if isinstance(extracted_dict, dict):\n",
    "                    return extracted_dict\n",
    "            except (SyntaxError, NameError):\n",
    "                pass\n",
    "        \n",
    "        return None\n",
    "\n",
    "    # Function to decide the indexing of the table\n",
    "    def eval_table_index_llama(table_str):\n",
    "        class Header(BaseModel):\n",
    "            index: int = Field(description=(\n",
    "                '''Index of the table indicating the type of indexing:\\n\\\n",
    "                    0 - No indexes\\n\\\n",
    "                    1 - Indexed by the first column\\n\\\n",
    "                    2 - Indexed by the first row\\n\\\n",
    "                    3 - Indexed by both the first row and the first column'''))\n",
    "\n",
    "        parser = JsonOutputParser(pydantic_object=Header)\n",
    "\n",
    "        chat = ChatGroq(temperature=0, model_name=\"llama3-8b-8192\")\n",
    "        \n",
    "        template = '''You will assist me in deciding, based on the first 3 entries of a table, whether the first row or the first colum should be the header, or if both the first column and first row are headers\n",
    "    Table:\n",
    "    {table}\n",
    "\n",
    "    Determine the indexing type of the table and output:\n",
    "    - 0 if the table is indexed by the first column.\n",
    "    - 1 if the table is indexed by the first row.\n",
    "    - 2 if the table is indexed by both the first row and the first column.   \n",
    "                \n",
    "    {format_instructions}'''\n",
    "        prompt = PromptTemplate(\n",
    "            template=template,\n",
    "            input_variables=[\"table\"],\n",
    "            partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "        )\n",
    "            \n",
    "        chain = prompt | chat | parser\n",
    "        return chain.invoke({\"table\": table_str})\n",
    "    \n",
    "    table_first_3_rows = get_first_three_rows(table)\n",
    "    table_str = get_table_check_string(table_first_3_rows)\n",
    "    header_output = eval_table_index_llama(table_str)\n",
    "    if isinstance(header_output, str):\n",
    "        header_output = clean_llama_output_if_string(header_output)\n",
    "    header_index = header_output['index']\n",
    "    # Now need diff cases of how to handle the table if the model outputs, 0,1, or 2\n",
    "        # where 0 is the first col, 1 is second col, 2 is for both cols are headers\n",
    "    pass\n",
    "\n",
    "if is_unidentified(table):\n",
    "    processed_unidentified_table = transform_unidentified_table(table)\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the extracted data from the extractor\n",
    "def get_extracted_data(extracted_data):\n",
    "    if 'structuredData.json' in extracted_data:\n",
    "        json_data = json.loads(extracted_data['structuredData.json'])\n",
    "    return json_data\n",
    "\n",
    "\n",
    "\n",
    "# This function converts tables to strings to be able to be processed by LLMs. \n",
    "def get_and_save_table_strings(extractor, table_output_directory):\n",
    "    \n",
    "    # Function to convert each row in a raw unprocessed table (ie index of table not decided) into a string\n",
    "    def get_raw_table_string(df):\n",
    "        table_str = \"\"\n",
    "        for i in range(2):\n",
    "            if i ==1:\n",
    "                table_str += f\"Row {i}: {df.iloc[i].values.tolist()}\"  \n",
    "            else:\n",
    "                table_str += f\"Row {i}: {df.iloc[i].values.tolist()}\\n\"\n",
    "        return table_str\n",
    "    \n",
    "    # Function to decide if header is first row or first column\n",
    "    def evaluate_table_index_llama(table_str):\n",
    "        class Header(BaseModel):\n",
    "            index: int = Field(description=\"Header of the table, 0 for first row as the header, 1 for first column as the header\")\n",
    "            \n",
    "        parser = JsonOutputParser(pydantic_object=Header)\n",
    "\n",
    "        chat = ChatGroq(temperature=0, model_name=\"llama3-8b-8192\")\n",
    "        \n",
    "        template = '''You will assist me in deciding, based on the first 2 entries of a table, whether the first row or the first colum should be the header. \n",
    "                You are to output an int, 0 or 1. Where 0 if the first row is header, and 1 if the first column is the header.\n",
    "                Follow the format instructions carefully.\n",
    "                Table:\n",
    "                {table}\n",
    "                \n",
    "                {format_instructions}\n",
    "                '''\n",
    "        prompt = PromptTemplate(\n",
    "            template=template,\n",
    "            input_variables=[\"table\"],\n",
    "            partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "        )\n",
    "        chain = prompt | chat | parser\n",
    "        return chain.invoke({\"table\": table_str})\n",
    "\n",
    "    # Tables need procecssing when extraced from BytesIO\n",
    "    def clean_table_values(x):\n",
    "        if isinstance(x, str):\n",
    "            return x.replace('_x000D_', '').strip()\n",
    "        return x\n",
    "    \n",
    "    # Code adapted from a medium blog on how to represent rows of tables in strings\n",
    "    def convert_table_to_string(df):\n",
    "        for _, row in df.iterrows():\n",
    "            row_str = \"\"\n",
    "            for col in df.columns:\n",
    "                sentences = re.split(r'(?<=\\.)\\s*', row[col])\n",
    "                row_sentence = \"\"\n",
    "                for i in range(len(sentences)):\n",
    "                    row_sentence += sentences[i] + \"\\n\"\n",
    "                row_str += f\"{col}: {row_sentence}, \"\n",
    "            formatted = row_str[:-2]\n",
    "        return formatted\n",
    "\n",
    "    os.makedirs(table_output_directory, exist_ok=True)\n",
    "    \n",
    "    # The literal extraction of the file itself\n",
    "    excel_files = {k: v for k, v in extractor.extracted_data.items() if k.endswith('.xlsx')}\n",
    "    \n",
    "    table_dataframes = {}\n",
    "\n",
    "    num_tables =0\n",
    "    for _, content in excel_files.items():\n",
    "        excel_stream = BytesIO(content)\n",
    "        df = pd.read_excel(excel_stream, header=None)\n",
    "        \n",
    "        df = df.applymap(clean_table_values)\n",
    "        # # Uncomment bottom code to ensure that Groq decides table header index \n",
    "        \n",
    "        # df_str = get_raw_table_string(df) \n",
    "        # dic = evaluate_table_index_llama(df_str)\n",
    "        # header_index = dic['index']\n",
    "        \n",
    "        # Uncomment this if u uncomment the code above\n",
    "        header_index = 1\n",
    "        \n",
    "        # If header_index is non zero\n",
    "        if header_index == 1:\n",
    "            df = pd.read_excel(excel_stream, header=None)\n",
    "            df = df.applymap(clean_table_values)\n",
    "            df = df.T\n",
    "            # Set the first row as the new header\n",
    "            new_header = df.iloc[0]  # Take the first row for the header\n",
    "            df = df[1:]  # Take the data less the header row\n",
    "            df.columns = new_header  # Set the header row as the df header\n",
    "            # Optionally, reset index if necessary\n",
    "            df.reset_index(drop=True, inplace=True)\n",
    "        else:\n",
    "            df = pd.read_excel(excel_stream, header=0)\n",
    "        \n",
    "        output_file_path = os.path.join(table_output_directory, f\"table_{num_tables}.csv\")\n",
    "\n",
    "        # Writing the table to the corresponding csv file. \n",
    "        df.to_csv(output_file_path, index=False)\n",
    "            \n",
    "        table_str = convert_table_to_string(df)\n",
    "        table_dataframes[num_tables] = table_str\n",
    "        num_tables += 1\n",
    "        \n",
    "    return table_dataframes\n",
    "\n",
    "\n",
    "\n",
    "# Function to initialise a flexible text splitter\n",
    "def initialise_text_splitter(chunk_size, chunk_overlap):\n",
    "    separator_list = [\"\\n\\n\", \"\\n\", \". \", \"!\", \"?\", \",\", \" \", \"\", \")\", \"(\"]\n",
    "    try:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=chunk_overlap,\n",
    "                length_function=len,\n",
    "                separators=separator_list)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to initialize text splitter: {e}\")\n",
    "        return None\n",
    "    return text_splitter\n",
    "\n",
    "\n",
    "\n",
    "# Function to obtain text chunks using the text splitter\n",
    "def get_text_chunks(file_path, json_data, text_splitter):\n",
    "    if 'elements' not in json_data:\n",
    "        logging.error(\"Missing 'elements' key in json_data\")\n",
    "        raise ValueError(\"Missing 'elements' key in json_data\")\n",
    "\n",
    "    if not text_splitter:\n",
    "        logging.error(\"Text splitter not initialised properly. \")\n",
    "        sys.exit()  \n",
    "        \n",
    "    file_name = file_path.split(\"/\")[-1]\n",
    "\n",
    "    # Chunks are split by pages here\n",
    "    page_text = \"\"\n",
    "    start_page = 0\n",
    "    all_chunks = []\n",
    "    \n",
    "    list_label = \"\"\n",
    "    for i in range(len(json_data['elements'])):\n",
    "        try:\n",
    "            current_page = json_data['elements'][i]['Page']\n",
    "        except KeyError:\n",
    "            logging.warning(f\"Missing 'Page' key in element at index {i}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            if current_page > start_page:\n",
    "                # Update the new page number\n",
    "                start_page = current_page               \n",
    "                # Generate the chunks for the previous page\n",
    "                separated_list = text_splitter.split_text(page_text)\n",
    "                for chunk in separated_list:\n",
    "                    if chunk not in [\". \", \".\"]:  # Simplified condition\n",
    "                        all_chunks.append({'ElementType': 'Text', 'file_name': file_name, 'Page': current_page, 'Text': chunk})\n",
    "                # Update the string of text \n",
    "                page_text = \"\"\n",
    "                list_label = \"\"\n",
    "            else:\n",
    "                if 'Text' in json_data['elements'][i]:  # Check if Text is not empty\n",
    "                    if json_data['elements'][i]['Path'].endswith(\"Lbl\") and not json_data['elements'][i]['Path'].startswith(\"//Document/Table\"):\n",
    "                        list_label = json_data['elements'][i]['Text']\n",
    "                    else:\n",
    "                        if list_label:\n",
    "                            page_text += list_label + json_data['elements'][i]['Text']\n",
    "                            list_label = \"\"  # Reset list label to empty string\n",
    "                        else:\n",
    "                            page_text += json_data['elements'][i]['Text'] + \"\\n\"\n",
    "        except KeyError as e:\n",
    "            logging.warning(f\"Key error in json_data['elements'][i] processing at index {i}: {e}\")\n",
    "    \n",
    "    \n",
    "    # Processing the last page of the text\n",
    "    if page_text:\n",
    "        separated_list = text_splitter.split_text(page_text)\n",
    "        for chunk in separated_list:\n",
    "            if chunk not in [\". \", \".\"]:\n",
    "                all_chunks.append({'ElementType': 'Text', 'file_name': file_name, 'Page': current_page, 'Text': chunk})\n",
    "\n",
    "    return all_chunks\n",
    "\n",
    "\n",
    "# Further enhancement to include the tables with the metadata so that it can be parsed to the \n",
    "    # function that takes in the tables + metadata for embeddings generation\n",
    "def get_table_strings_with_metadata(table_dataframes, json_data):\n",
    "    \n",
    "    # Function to obtain the page number of each table\n",
    "    def get_table_pages(json_data):\n",
    "        table_file_pages = {}\n",
    "        # Obtaining the table metadata\n",
    "        for i in range(len(json_data['elements'])):\n",
    "            try:\n",
    "                file_paths = json_data['elements'][i].get('filePaths')\n",
    "                if file_paths:\n",
    "                    page = json_data['elements'][i].get('Page', 'Unknown')\n",
    "                    match = re.search(r'\\d+', file_paths[0])\n",
    "                    table_index = match.group(0)\n",
    "                    table_file_pages[int(table_index)] = {\"Page\": page}\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing file paths at index {i}: {e}\")\n",
    "        return table_file_pages\n",
    "    \n",
    "    table_file_pages = get_table_pages(json_data)\n",
    "    meta_table_batch = []\n",
    "    table_dfs = []\n",
    "    for table_index,_ in table_dataframes.items():\n",
    "        dic = {}\n",
    "        dic['ElementType'] = 'Table'\n",
    "        dic['Page'] = table_file_pages[table_index]['Page']\n",
    "        dic['Table'] = table_dataframes[table_index]\n",
    "        table_dfs.append(dic)\n",
    "\n",
    "        #  Obtain metadata for sparse embeddings\n",
    "        meta_table_batch.append(f\"ElementType 'Table', Page {table_file_pages[table_index]['Page']}, {table_dataframes[table_index]}\")\n",
    "    return table_dfs, meta_table_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-02 18:38:10,933 - INFO - Started uploading asset\n",
      "2024-07-02 18:38:15,314 - INFO - Finished uploading asset\n",
      "2024-07-02 18:38:15,316 - INFO - Started submitting EXTRACT_PDF job\n",
      "2024-07-02 18:38:16,553 - INFO - Started getting job result\n",
      "2024-07-02 18:38:27,172 - INFO - Finished polling for status\n",
      "2024-07-02 18:38:27,176 - INFO - Finished getting job result\n",
      "2024-07-02 18:38:27,178 - INFO - Started getting content\n",
      "2024-07-02 18:38:27,486 - INFO - Finished getting content\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique ID: 0e4dcde5-fe65-4391-ade1-d1389e41d635\n"
     ]
    }
   ],
   "source": [
    "file_path = '../PDF/HSI1000-chapter1.pdf'\n",
    "extractor, unique_id = ExtractTextTableInfoFromPDF.create_with_unique_id(file_path)\n",
    "print(\"\\nUnique ID:\", unique_id)\n",
    "table_output_directory = f\"../data/{unique_id}\"\n",
    "extracted_data = extractor.extracted_data\n",
    "pdf_data = get_extracted_data(extracted_data)\n",
    "table_dataframes = get_and_save_table_strings(extractor, table_output_directory)\n",
    "\n",
    "# Use some form of evaluator to decide chunk size?\n",
    "text_splitter = initialise_text_splitter(300, 50)\n",
    "\n",
    "# Get out important information\n",
    "text_chunks = get_text_chunks(file_path, pdf_data, text_splitter)\n",
    "# table_dfs, meta_table_batch= get_table_strings_with_metadata(table_dataframes, pdf_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
